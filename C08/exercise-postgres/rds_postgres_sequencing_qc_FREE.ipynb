{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS RDS PostgreSQL (FREE TIER) - Sequencing QC Database\n",
    "\n",
    "This notebook demonstrates setting up an AWS RDS PostgreSQL database for clinical genetic testing sequencing QC operations.\n",
    "\n",
    "## üéâ FREE TIER VERSION üéâ\n",
    "This version uses RDS Free Tier (db.t3.micro) which is **100% FREE** for the first 12 months!\n",
    "\n",
    "## Features\n",
    "- Create RDS PostgreSQL instance (db.t3.micro - FREE TIER)\n",
    "- Create tables for sequencing QC metrics\n",
    "- Generate and populate faux QC data\n",
    "- Visualize QC metrics\n",
    "- Clean up all AWS resources\n",
    "\n",
    "## Free Tier Benefits\n",
    "- ‚úÖ 750 hours/month of db.t3.micro (enough for 24/7 operation)\n",
    "- ‚úÖ 20 GB of storage\n",
    "- ‚úÖ 20 GB of backup storage\n",
    "- ‚úÖ Free for 12 months\n",
    "- ‚úÖ Full PostgreSQL 16 compatibility\n",
    "\n",
    "## Prerequisites\n",
    "- AWS account eligible for free tier (new account or within first 12 months)\n",
    "- AWS credentials configured (via AWS CLI or environment variables)\n",
    "- Appropriate IAM permissions for RDS and EC2\n",
    "- Python packages: boto3, psycopg2-binary, pandas, matplotlib, seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "import psycopg2\n",
    "import psycopg2.extensions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from psycopg2.extras import execute_values\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Register numpy type adapters for psycopg2 (prevents numpy type errors)\n",
    "psycopg2.extensions.register_adapter(np.int64, lambda x: int(x))\n",
    "psycopg2.extensions.register_adapter(np.int32, lambda x: int(x))\n",
    "psycopg2.extensions.register_adapter(np.float64, lambda x: float(x))\n",
    "psycopg2.extensions.register_adapter(np.float32, lambda x: float(x))\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(\"‚úì Numpy adapters registered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "AWS_REGION = 'us-east-1'\n",
    "DB_INSTANCE_IDENTIFIER = 'sequencing-qc-db-free'\n",
    "DB_NAME = 'sequencing_qc_db'\n",
    "MASTER_USERNAME = 'qcadmin'\n",
    "MASTER_PASSWORD = 'QC_Secure_Pass_2024!'  # In production, use Secrets Manager\n",
    "\n",
    "# Initialize AWS clients\n",
    "rds_client = boto3.client('rds', region_name=AWS_REGION)\n",
    "ec2_client = boto3.client('ec2', region_name=AWS_REGION)\n",
    "\n",
    "print(f\"Configuration set for region: {AWS_REGION}\")\n",
    "print(f\"Database instance identifier: {DB_INSTANCE_IDENTIFIER}\")\n",
    "print(f\"\\nüéâ Using FREE TIER: db.t3.micro (100% free for 12 months!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RDS Free Tier PostgreSQL Instance Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rds_free_tier_instance():\n",
    "    \"\"\"\n",
    "    Create an RDS PostgreSQL instance using FREE TIER (db.t3.micro).\n",
    "    Returns the instance endpoint information.\n",
    "    \n",
    "    FREE TIER SPECS:\n",
    "    - Instance: db.t3.micro (2 vCPU, 1 GB RAM)\n",
    "    - Storage: 20 GB (max free tier)\n",
    "    - Cost: $0.00 for first 12 months (750 hours/month)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Creating RDS PostgreSQL instance (FREE TIER)...\")\n",
    "        print(\"  Instance class: db.t3.micro\")\n",
    "        print(\"  Storage: 20 GB (free tier max)\")\n",
    "        print(\"  Engine: PostgreSQL 16.4\")\n",
    "        print(\"\")\n",
    "        \n",
    "        # Create the RDS instance\n",
    "        response = rds_client.create_db_instance(\n",
    "            DBInstanceIdentifier=DB_INSTANCE_IDENTIFIER,\n",
    "            DBInstanceClass='db.t3.micro',          # FREE TIER!\n",
    "            Engine='postgres',\n",
    "            EngineVersion='16.4',\n",
    "            MasterUsername=MASTER_USERNAME,\n",
    "            MasterUserPassword=MASTER_PASSWORD,\n",
    "            DBName=DB_NAME,\n",
    "            AllocatedStorage=20,                    # FREE TIER: 20 GB max\n",
    "            StorageType='gp2',                      # General purpose SSD\n",
    "            PubliclyAccessible=True,                # For demo; use VPC in production\n",
    "            BackupRetentionPeriod=1,                # Minimal backups\n",
    "            StorageEncrypted=True,\n",
    "            EnableCloudwatchLogsExports=['postgresql'],\n",
    "            Tags=[\n",
    "                {'Key': 'Purpose', 'Value': 'SequencingQC'},\n",
    "                {'Key': 'FreeTier', 'Value': 'Yes'},\n",
    "                {'Key': 'Environment', 'Value': 'Learning'}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úì Instance creation initiated: {DB_INSTANCE_IDENTIFIER}\")\n",
    "        print(\"\")\n",
    "        \n",
    "        # Wait for instance to be available\n",
    "        print(\"Waiting for instance to become available (this may take 5-10 minutes)...\")\n",
    "        print(\"‚òï Grab a coffee while AWS provisions your FREE database!\")\n",
    "        \n",
    "        waiter = rds_client.get_waiter('db_instance_available')\n",
    "        waiter.wait(\n",
    "            DBInstanceIdentifier=DB_INSTANCE_IDENTIFIER,\n",
    "            WaiterConfig={'Delay': 30, 'MaxAttempts': 40}\n",
    "        )\n",
    "        \n",
    "        # Get instance details\n",
    "        instance_info = rds_client.describe_db_instances(\n",
    "            DBInstanceIdentifier=DB_INSTANCE_IDENTIFIER\n",
    "        )['DBInstances'][0]\n",
    "        \n",
    "        endpoint = instance_info['Endpoint']['Address']\n",
    "        port = instance_info['Endpoint']['Port']\n",
    "        \n",
    "        print(f\"\\n\" + \"=\"*70)\n",
    "        print(\"‚úì RDS Instance is available!\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"  Endpoint: {endpoint}\")\n",
    "        print(f\"  Port: {port}\")\n",
    "        print(f\"  Database: {DB_NAME}\")\n",
    "        print(f\"  Instance class: {instance_info['DBInstanceClass']}\")\n",
    "        print(f\"  Storage: {instance_info['AllocatedStorage']} GB\")\n",
    "        print(f\"  Engine: {instance_info['Engine']} {instance_info['EngineVersion']}\")\n",
    "        print(f\"\\nüéâ FREE TIER: This costs $0.00 (within 750 hours/month limit)!\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        return {\n",
    "            'endpoint': endpoint,\n",
    "            'port': port,\n",
    "            'database': DB_NAME,\n",
    "            'username': MASTER_USERNAME,\n",
    "            'password': MASTER_PASSWORD\n",
    "        }\n",
    "        \n",
    "    except rds_client.exceptions.DBInstanceAlreadyExistsFault:\n",
    "        print(f\"Instance {DB_INSTANCE_IDENTIFIER} already exists. Retrieving details...\")\n",
    "        instance_info = rds_client.describe_db_instances(\n",
    "            DBInstanceIdentifier=DB_INSTANCE_IDENTIFIER\n",
    "        )['DBInstances'][0]\n",
    "        \n",
    "        return {\n",
    "            'endpoint': instance_info['Endpoint']['Address'],\n",
    "            'port': instance_info['Endpoint']['Port'],\n",
    "            'database': DB_NAME,\n",
    "            'username': MASTER_USERNAME,\n",
    "            'password': MASTER_PASSWORD\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating instance: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_security_group(db_instance_identifier):\n",
    "    \"\"\"\n",
    "    Configure security group to allow PostgreSQL connections.\n",
    "    This is required for connecting to the database from external clients.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get instance details\n",
    "        instance_info = rds_client.describe_db_instances(\n",
    "            DBInstanceIdentifier=db_instance_identifier\n",
    "        )['DBInstances'][0]\n",
    "        \n",
    "        vpc_security_groups = instance_info.get('VpcSecurityGroups', [])\n",
    "        \n",
    "        if not vpc_security_groups:\n",
    "            print(\"WARNING: No VPC security groups found!\")\n",
    "            return\n",
    "        \n",
    "        for sg in vpc_security_groups:\n",
    "            sg_id = sg['VpcSecurityGroupId']\n",
    "            \n",
    "            # Check if rule already exists\n",
    "            sg_details = ec2_client.describe_security_groups(\n",
    "                GroupIds=[sg_id]\n",
    "            )['SecurityGroups'][0]\n",
    "            \n",
    "            has_postgres_rule = False\n",
    "            for rule in sg_details.get('IpPermissions', []):\n",
    "                if rule.get('FromPort') == 5432 and rule.get('ToPort') == 5432:\n",
    "                    has_postgres_rule = True\n",
    "                    break\n",
    "            \n",
    "            if not has_postgres_rule:\n",
    "                print(f\"Adding PostgreSQL inbound rule to security group {sg_id}...\")\n",
    "                ec2_client.authorize_security_group_ingress(\n",
    "                    GroupId=sg_id,\n",
    "                    IpPermissions=[\n",
    "                        {\n",
    "                            'IpProtocol': 'tcp',\n",
    "                            'FromPort': 5432,\n",
    "                            'ToPort': 5432,\n",
    "                            'IpRanges': [\n",
    "                                {\n",
    "                                    'CidrIp': '0.0.0.0/0',\n",
    "                                    'Description': 'PostgreSQL access (demo only)'\n",
    "                                }\n",
    "                            ]\n",
    "                        }\n",
    "                    ]\n",
    "                )\n",
    "                print(\"  ‚úì Security group configured\")\n",
    "            else:\n",
    "                print(\"  ‚úì Security group already configured\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not configure security group: {e}\")\n",
    "        print(\"You may need to manually add inbound rule for port 5432\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the RDS FREE TIER instance\n",
    "db_config = create_rds_free_tier_instance()\n",
    "\n",
    "# Configure security group\n",
    "print(\"\\nConfiguring security group...\")\n",
    "configure_security_group(DB_INSTANCE_IDENTIFIER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Database Connection Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_db_connection():\n",
    "    \"\"\"\n",
    "    Create and return a database connection.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            host=db_config['endpoint'],\n",
    "            port=db_config['port'],\n",
    "            database=db_config['database'],\n",
    "            user=db_config['username'],\n",
    "            password=db_config['password'],\n",
    "            connect_timeout=10\n",
    "        )\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to database: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    test_conn = get_db_connection()\n",
    "    print(\"‚úì Database connection successful!\")\n",
    "    test_conn.close()\n",
    "except Exception as e:\n",
    "    print(f\"Connection failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Database Schema\n",
    "\n",
    "We'll create 7 tables for clinical sequencing QC:\n",
    "1. **sequencing_runs** - Sequencing run metadata\n",
    "2. **samples** - Patient samples\n",
    "3. **qc_metrics** - Overall QC metrics\n",
    "4. **coverage_metrics** - Coverage statistics\n",
    "5. **variant_calls** - Variant calling statistics\n",
    "6. **contamination_checks** - Sample contamination detection\n",
    "7. **alignment_stats** - Read alignment statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tables():\n",
    "    \"\"\"\n",
    "    Create all tables for sequencing QC database.\n",
    "    \"\"\"\n",
    "    conn = get_db_connection()\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Drop existing tables if they exist\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS alignment_stats CASCADE;\")\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS contamination_checks CASCADE;\")\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS variant_calls CASCADE;\")\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS coverage_metrics CASCADE;\")\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS qc_metrics CASCADE;\")\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS samples CASCADE;\")\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS sequencing_runs CASCADE;\")\n",
    "    \n",
    "    # Table 1: Sequencing Runs\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE sequencing_runs (\n",
    "            run_id SERIAL PRIMARY KEY,\n",
    "            run_name VARCHAR(100) UNIQUE NOT NULL,\n",
    "            sequencer_id VARCHAR(50) NOT NULL,\n",
    "            platform VARCHAR(50) NOT NULL,\n",
    "            run_date DATE NOT NULL,\n",
    "            flow_cell_id VARCHAR(50),\n",
    "            num_cycles INTEGER,\n",
    "            operator VARCHAR(100),\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        );\n",
    "    \"\"\")\n",
    "    print(\"‚úì Created table: sequencing_runs\")\n",
    "    \n",
    "    # Table 2: Samples\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE samples (\n",
    "            sample_id SERIAL PRIMARY KEY,\n",
    "            sample_name VARCHAR(100) UNIQUE NOT NULL,\n",
    "            patient_id VARCHAR(50) NOT NULL,\n",
    "            sample_type VARCHAR(50) NOT NULL,\n",
    "            collection_date DATE,\n",
    "            run_id INTEGER REFERENCES sequencing_runs(run_id),\n",
    "            assay_type VARCHAR(100),\n",
    "            clinical_indication TEXT,\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        );\n",
    "    \"\"\")\n",
    "    print(\"‚úì Created table: samples\")\n",
    "    \n",
    "    # Table 3: QC Metrics\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE qc_metrics (\n",
    "            qc_id SERIAL PRIMARY KEY,\n",
    "            sample_id INTEGER REFERENCES samples(sample_id),\n",
    "            total_reads BIGINT NOT NULL,\n",
    "            passed_filter_reads BIGINT NOT NULL,\n",
    "            q30_bases_pct DECIMAL(5,2),\n",
    "            mean_quality_score DECIMAL(5,2),\n",
    "            gc_content_pct DECIMAL(5,2),\n",
    "            duplicate_rate_pct DECIMAL(5,2),\n",
    "            insert_size_mean DECIMAL(8,2),\n",
    "            qc_status VARCHAR(20),\n",
    "            qc_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        );\n",
    "    \"\"\")\n",
    "    print(\"‚úì Created table: qc_metrics\")\n",
    "    \n",
    "    # Table 4: Coverage Metrics\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE coverage_metrics (\n",
    "            coverage_id SERIAL PRIMARY KEY,\n",
    "            sample_id INTEGER REFERENCES samples(sample_id),\n",
    "            mean_coverage DECIMAL(10,2) NOT NULL,\n",
    "            median_coverage DECIMAL(10,2),\n",
    "            coverage_10x_pct DECIMAL(5,2),\n",
    "            coverage_20x_pct DECIMAL(5,2),\n",
    "            coverage_30x_pct DECIMAL(5,2),\n",
    "            coverage_100x_pct DECIMAL(5,2),\n",
    "            uniformity_pct DECIMAL(5,2),\n",
    "            on_target_rate_pct DECIMAL(5,2),\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        );\n",
    "    \"\"\")\n",
    "    print(\"‚úì Created table: coverage_metrics\")\n",
    "    \n",
    "    # Table 5: Variant Calls\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE variant_calls (\n",
    "            variant_id SERIAL PRIMARY KEY,\n",
    "            sample_id INTEGER REFERENCES samples(sample_id),\n",
    "            total_variants INTEGER NOT NULL,\n",
    "            snvs INTEGER,\n",
    "            indels INTEGER,\n",
    "            het_hom_ratio DECIMAL(5,2),\n",
    "            ti_tv_ratio DECIMAL(5,2),\n",
    "            pathogenic_variants INTEGER,\n",
    "            vus_variants INTEGER,\n",
    "            benign_variants INTEGER,\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        );\n",
    "    \"\"\")\n",
    "    print(\"‚úì Created table: variant_calls\")\n",
    "    \n",
    "    # Table 6: Contamination Checks\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE contamination_checks (\n",
    "            contamination_id SERIAL PRIMARY KEY,\n",
    "            sample_id INTEGER REFERENCES samples(sample_id),\n",
    "            contamination_estimate_pct DECIMAL(5,2) NOT NULL,\n",
    "            contamination_status VARCHAR(20),\n",
    "            method VARCHAR(50),\n",
    "            num_snps_analyzed INTEGER,\n",
    "            confidence_score DECIMAL(5,2),\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        );\n",
    "    \"\"\")\n",
    "    print(\"‚úì Created table: contamination_checks\")\n",
    "    \n",
    "    # Table 7: Alignment Stats\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE alignment_stats (\n",
    "            alignment_id SERIAL PRIMARY KEY,\n",
    "            sample_id INTEGER REFERENCES samples(sample_id),\n",
    "            total_reads BIGINT NOT NULL,\n",
    "            mapped_reads BIGINT NOT NULL,\n",
    "            properly_paired_reads BIGINT,\n",
    "            mapping_quality_mean DECIMAL(5,2),\n",
    "            error_rate DECIMAL(6,4),\n",
    "            mismatch_rate DECIMAL(6,4),\n",
    "            alignment_rate_pct DECIMAL(5,2),\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        );\n",
    "    \"\"\")\n",
    "    print(\"‚úì Created table: alignment_stats\")\n",
    "    \n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    print(\"\\n‚úì All tables created successfully!\")\n",
    "\n",
    "# Create the tables\n",
    "create_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate and Populate Faux Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequencing_runs(num_runs=10):\n",
    "    \"\"\"\n",
    "    Generate faux sequencing run data.\n",
    "    \"\"\"\n",
    "    platforms = ['NovaSeq 6000', 'NextSeq 2000', 'MiSeq']\n",
    "    operators = ['Alice Chen', 'Bob Martinez', 'Carol Johnson', 'David Kim']\n",
    "    \n",
    "    runs = []\n",
    "    start_date = datetime.now() - timedelta(days=90)\n",
    "    \n",
    "    for i in range(num_runs):\n",
    "        run_date = start_date + timedelta(days=i*9)\n",
    "        runs.append((\n",
    "            f\"RUN_{run_date.strftime('%Y%m%d')}_{i+1:03d}\",\n",
    "            f\"SEQ_{np.random.choice(['NS', 'NV', 'MS'])}{int(np.random.randint(1000, 9999))}\",\n",
    "            str(np.random.choice(platforms)),\n",
    "            run_date.date(),\n",
    "            f\"FC_{int(np.random.randint(100000, 999999))}\",\n",
    "            int(np.random.choice([150, 250, 300])),\n",
    "            str(np.random.choice(operators))\n",
    "        ))\n",
    "    \n",
    "    return runs\n",
    "\n",
    "def generate_samples(num_samples=50):\n",
    "    \"\"\"\n",
    "    Generate faux sample data.\n",
    "    \"\"\"\n",
    "    sample_types = ['Blood', 'Saliva', 'Tissue', 'Buccal Swab']\n",
    "    assay_types = ['Whole Exome Sequencing', 'Targeted Gene Panel', 'Whole Genome Sequencing', 'Cancer Panel']\n",
    "    indications = ['Hereditary Cancer', 'Cardiovascular Disease', 'Rare Disease', 'Pharmacogenomics']\n",
    "    \n",
    "    samples = []\n",
    "    start_date = datetime.now() - timedelta(days=90)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        collection_date = start_date + timedelta(days=int(np.random.randint(0, 85)))\n",
    "        run_id = (i // 5) + 1  # 5 samples per run\n",
    "        \n",
    "        samples.append((\n",
    "            f\"SAMP_{i+1:05d}\",\n",
    "            f\"PT_{i+1:04d}\",\n",
    "            str(np.random.choice(sample_types)),\n",
    "            collection_date.date(),\n",
    "            run_id,\n",
    "            str(np.random.choice(assay_types)),\n",
    "            str(np.random.choice(indications))\n",
    "        ))\n",
    "    \n",
    "    return samples\n",
    "\n",
    "def generate_qc_metrics(num_samples=50):\n",
    "    \"\"\"\n",
    "    Generate faux QC metrics data.\n",
    "    \"\"\"\n",
    "    metrics = []\n",
    "    \n",
    "    for sample_id in range(1, num_samples + 1):\n",
    "        total_reads = int(np.random.randint(40_000_000, 150_000_000))\n",
    "        passed_filter = int(total_reads * np.random.uniform(0.85, 0.98))\n",
    "        \n",
    "        # Introduce some failing samples\n",
    "        if np.random.random() < 0.1:  # 10% fail rate\n",
    "            q30_bases = np.random.uniform(70, 84)\n",
    "            quality_score = np.random.uniform(28, 32)\n",
    "            status = 'FAIL'\n",
    "        else:\n",
    "            q30_bases = np.random.uniform(85, 95)\n",
    "            quality_score = np.random.uniform(33, 38)\n",
    "            status = 'PASS'\n",
    "        \n",
    "        metrics.append((\n",
    "            sample_id,\n",
    "            total_reads,\n",
    "            passed_filter,\n",
    "            round(float(q30_bases), 2),\n",
    "            round(float(quality_score), 2),\n",
    "            round(float(np.random.uniform(40, 60)), 2),\n",
    "            round(float(np.random.uniform(5, 25)), 2),\n",
    "            round(float(np.random.uniform(150, 400)), 2),\n",
    "            status\n",
    "        ))\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def generate_coverage_metrics(num_samples=50):\n",
    "    \"\"\"\n",
    "    Generate faux coverage metrics.\n",
    "    \"\"\"\n",
    "    metrics = []\n",
    "    \n",
    "    for sample_id in range(1, num_samples + 1):\n",
    "        mean_cov = np.random.uniform(80, 250)\n",
    "        median_cov = mean_cov * np.random.uniform(0.9, 1.1)\n",
    "        \n",
    "        metrics.append((\n",
    "            sample_id,\n",
    "            round(float(mean_cov), 2),\n",
    "            round(float(median_cov), 2),\n",
    "            round(float(np.random.uniform(95, 99.9)), 2),\n",
    "            round(float(np.random.uniform(90, 99.5)), 2),\n",
    "            round(float(np.random.uniform(85, 99)), 2),\n",
    "            round(float(np.random.uniform(70, 95)), 2),\n",
    "            round(float(np.random.uniform(75, 95)), 2),\n",
    "            round(float(np.random.uniform(65, 90)), 2)\n",
    "        ))\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def generate_variant_calls(num_samples=50):\n",
    "    \"\"\"\n",
    "    Generate faux variant calling data.\n",
    "    \"\"\"\n",
    "    variants = []\n",
    "    \n",
    "    for sample_id in range(1, num_samples + 1):\n",
    "        total = int(np.random.randint(20000, 80000))\n",
    "        snvs = int(total * np.random.uniform(0.85, 0.92))\n",
    "        indels = total - snvs\n",
    "        \n",
    "        pathogenic = int(np.random.randint(0, 15))\n",
    "        vus = int(np.random.randint(10, 100))\n",
    "        benign = total - pathogenic - vus\n",
    "        \n",
    "        variants.append((\n",
    "            sample_id,\n",
    "            total,\n",
    "            snvs,\n",
    "            indels,\n",
    "            round(float(np.random.uniform(1.2, 2.5)), 2),\n",
    "            round(float(np.random.uniform(2.0, 2.2)), 2),\n",
    "            pathogenic,\n",
    "            vus,\n",
    "            benign\n",
    "        ))\n",
    "    \n",
    "    return variants\n",
    "\n",
    "def generate_contamination_checks(num_samples=50):\n",
    "    \"\"\"\n",
    "    Generate faux contamination check data.\n",
    "    \"\"\"\n",
    "    checks = []\n",
    "    methods = ['VerifyBAMID', 'ContEst', 'FREEMIX']\n",
    "    \n",
    "    for sample_id in range(1, num_samples + 1):\n",
    "        # Most samples have low contamination\n",
    "        if np.random.random() < 0.9:\n",
    "            contam = np.random.uniform(0, 2)\n",
    "            status = 'PASS'\n",
    "        else:\n",
    "            contam = np.random.uniform(2, 10)\n",
    "            status = 'FAIL'\n",
    "        \n",
    "        checks.append((\n",
    "            sample_id,\n",
    "            round(float(contam), 2),\n",
    "            status,\n",
    "            str(np.random.choice(methods)),\n",
    "            int(np.random.randint(5000, 50000)),\n",
    "            round(float(np.random.uniform(0.8, 1.0)), 2)\n",
    "        ))\n",
    "    \n",
    "    return checks\n",
    "\n",
    "def generate_alignment_stats(num_samples=50):\n",
    "    \"\"\"\n",
    "    Generate faux alignment statistics.\n",
    "    \"\"\"\n",
    "    stats = []\n",
    "    \n",
    "    for sample_id in range(1, num_samples + 1):\n",
    "        total = int(np.random.randint(40_000_000, 150_000_000))\n",
    "        mapped = int(total * np.random.uniform(0.92, 0.99))\n",
    "        properly_paired = int(mapped * np.random.uniform(0.85, 0.95))\n",
    "        \n",
    "        stats.append((\n",
    "            sample_id,\n",
    "            total,\n",
    "            mapped,\n",
    "            properly_paired,\n",
    "            round(float(np.random.uniform(50, 60)), 2),\n",
    "            round(float(np.random.uniform(0.001, 0.01)), 4),\n",
    "            round(float(np.random.uniform(0.002, 0.015)), 4),\n",
    "            round(float((mapped / total) * 100), 2)\n",
    "        ))\n",
    "    \n",
    "    return stats\n",
    "\n",
    "print(\"Data generation functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_database():\n",
    "    \"\"\"\n",
    "    Populate all tables with generated data.\n",
    "    \"\"\"\n",
    "    conn = get_db_connection()\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Generate data\n",
    "    print(\"Generating data...\")\n",
    "    runs = generate_sequencing_runs(10)\n",
    "    samples = generate_samples(50)\n",
    "    qc_metrics = generate_qc_metrics(50)\n",
    "    coverage = generate_coverage_metrics(50)\n",
    "    variants = generate_variant_calls(50)\n",
    "    contamination = generate_contamination_checks(50)\n",
    "    alignment = generate_alignment_stats(50)\n",
    "    \n",
    "    # Insert sequencing runs\n",
    "    print(\"Inserting sequencing runs...\")\n",
    "    execute_values(cursor, \"\"\"\n",
    "        INSERT INTO sequencing_runs (run_name, sequencer_id, platform, run_date, \n",
    "                                     flow_cell_id, num_cycles, operator)\n",
    "        VALUES %s\n",
    "    \"\"\", runs)\n",
    "    \n",
    "    # Insert samples\n",
    "    print(\"Inserting samples...\")\n",
    "    execute_values(cursor, \"\"\"\n",
    "        INSERT INTO samples (sample_name, patient_id, sample_type, collection_date,\n",
    "                            run_id, assay_type, clinical_indication)\n",
    "        VALUES %s\n",
    "    \"\"\", samples)\n",
    "    \n",
    "    # Insert QC metrics\n",
    "    print(\"Inserting QC metrics...\")\n",
    "    execute_values(cursor, \"\"\"\n",
    "        INSERT INTO qc_metrics (sample_id, total_reads, passed_filter_reads, q30_bases_pct,\n",
    "                               mean_quality_score, gc_content_pct, duplicate_rate_pct,\n",
    "                               insert_size_mean, qc_status)\n",
    "        VALUES %s\n",
    "    \"\"\", qc_metrics)\n",
    "    \n",
    "    # Insert coverage metrics\n",
    "    print(\"Inserting coverage metrics...\")\n",
    "    execute_values(cursor, \"\"\"\n",
    "        INSERT INTO coverage_metrics (sample_id, mean_coverage, median_coverage,\n",
    "                                     coverage_10x_pct, coverage_20x_pct, coverage_30x_pct,\n",
    "                                     coverage_100x_pct, uniformity_pct, on_target_rate_pct)\n",
    "        VALUES %s\n",
    "    \"\"\", coverage)\n",
    "    \n",
    "    # Insert variant calls\n",
    "    print(\"Inserting variant calls...\")\n",
    "    execute_values(cursor, \"\"\"\n",
    "        INSERT INTO variant_calls (sample_id, total_variants, snvs, indels,\n",
    "                                  het_hom_ratio, ti_tv_ratio, pathogenic_variants,\n",
    "                                  vus_variants, benign_variants)\n",
    "        VALUES %s\n",
    "    \"\"\", variants)\n",
    "    \n",
    "    # Insert contamination checks\n",
    "    print(\"Inserting contamination checks...\")\n",
    "    execute_values(cursor, \"\"\"\n",
    "        INSERT INTO contamination_checks (sample_id, contamination_estimate_pct,\n",
    "                                         contamination_status, method, num_snps_analyzed,\n",
    "                                         confidence_score)\n",
    "        VALUES %s\n",
    "    \"\"\", contamination)\n",
    "    \n",
    "    # Insert alignment stats\n",
    "    print(\"Inserting alignment statistics...\")\n",
    "    execute_values(cursor, \"\"\"\n",
    "        INSERT INTO alignment_stats (sample_id, total_reads, mapped_reads,\n",
    "                                    properly_paired_reads, mapping_quality_mean,\n",
    "                                    error_rate, mismatch_rate, alignment_rate_pct)\n",
    "        VALUES %s\n",
    "    \"\"\", alignment)\n",
    "    \n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    \n",
    "    print(\"\\n‚úì Database populated successfully!\")\n",
    "    print(f\"  - {len(runs)} sequencing runs\")\n",
    "    print(f\"  - {len(samples)} samples\")\n",
    "    print(f\"  - {len(qc_metrics)} QC metric records\")\n",
    "    print(f\"  - {len(coverage)} coverage metric records\")\n",
    "    print(f\"  - {len(variants)} variant call records\")\n",
    "    print(f\"  - {len(contamination)} contamination check records\")\n",
    "    print(f\"  - {len(alignment)} alignment stat records\")\n",
    "\n",
    "# Populate the database\n",
    "populate_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Visualizations\n",
    "\n",
    "**Note**: The visualization cells are identical to the Aurora Serverless version.\n",
    "I'll include just the first one here for brevity - copy the rest from the original notebook if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 1: QC Status Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query QC status data\n",
    "conn = get_db_connection()\n",
    "qc_status_df = pd.read_sql_query(\"\"\"\n",
    "    SELECT qc_status, COUNT(*) as count\n",
    "    FROM qc_metrics\n",
    "    GROUP BY qc_status\n",
    "\"\"\", conn)\n",
    "\n",
    "# Create pie chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "plt.pie(qc_status_df['count'], labels=qc_status_df['qc_status'], autopct='%1.1f%%',\n",
    "        colors=colors, startangle=90)\n",
    "plt.title('Sample QC Status Distribution', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 2: Mean Coverage Distribution¬∂"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query QC metrics\n",
    "conn = get_db_connection()\n",
    "qc_df = pd.read_sql_query(\"\"\"\n",
    "    SELECT q30_bases_pct, mean_quality_score, qc_status\n",
    "    FROM qc_metrics\n",
    "\"\"\", conn)\n",
    "\n",
    "# Create scatter plot\n",
    "plt.figure(figsize=(12, 7))\n",
    "colors = {'PASS': '#2ecc71', 'FAIL': '#e74c3c'}\n",
    "for status in qc_df['qc_status'].unique():\n",
    "    subset = qc_df[qc_df['qc_status'] == status]\n",
    "    plt.scatter(subset['q30_bases_pct'], subset['mean_quality_score'], \n",
    "               c=colors[status], label=status, s=100, alpha=0.6, edgecolors='black')\n",
    "\n",
    "plt.xlabel('Q30 Bases (%)', fontsize=12)\n",
    "plt.ylabel('Mean Quality Score', fontsize=12)\n",
    "plt.title('Quality Metrics: Q30 Bases vs Mean Quality Score', fontsize=16, fontweight='bold')\n",
    "plt.legend(title='QC Status', fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 3: Q30 Score vs Quality Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query QC metrics\n",
    "conn = get_db_connection()\n",
    "qc_df = pd.read_sql_query(\"\"\"\n",
    "    SELECT q30_bases_pct, mean_quality_score, qc_status\n",
    "    FROM qc_metrics\n",
    "\"\"\", conn)\n",
    "\n",
    "# Create scatter plot\n",
    "plt.figure(figsize=(12, 7))\n",
    "colors = {'PASS': '#2ecc71', 'FAIL': '#e74c3c'}\n",
    "for status in qc_df['qc_status'].unique():\n",
    "    subset = qc_df[qc_df['qc_status'] == status]\n",
    "    plt.scatter(subset['q30_bases_pct'], subset['mean_quality_score'], \n",
    "               c=colors[status], label=status, s=100, alpha=0.6, edgecolors='black')\n",
    "\n",
    "plt.xlabel('Q30 Bases (%)', fontsize=12)\n",
    "plt.ylabel('Mean Quality Score', fontsize=12)\n",
    "plt.title('Quality Metrics: Q30 Bases vs Mean Quality Score', fontsize=16, fontweight='bold')\n",
    "plt.legend(title='QC Status', fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 4: Variant Classification Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query variant data\n",
    "conn = get_db_connection()\n",
    "variant_df = pd.read_sql_query(\"\"\"\n",
    "    SELECT \n",
    "        SUM(pathogenic_variants) as pathogenic,\n",
    "        SUM(vus_variants) as vus,\n",
    "        SUM(benign_variants) as benign\n",
    "    FROM variant_calls\n",
    "\"\"\", conn)\n",
    "\n",
    "# Prepare data for bar chart\n",
    "categories = ['Pathogenic', 'VUS', 'Benign']\n",
    "values = [variant_df['pathogenic'][0], variant_df['vus'][0], variant_df['benign'][0]]\n",
    "colors_bar = ['#e74c3c', '#f39c12', '#2ecc71']\n",
    "\n",
    "# Create bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(categories, values, color=colors_bar, edgecolor='black', alpha=0.8)\n",
    "plt.ylabel('Total Number of Variants', fontsize=12)\n",
    "plt.title('Variant Classification Summary Across All Samples', fontsize=16, fontweight='bold')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{int(height):,}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 5: Contamination Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query contamination data\n",
    "conn = get_db_connection()\n",
    "contam_df = pd.read_sql_query(\"\"\"\n",
    "    SELECT contamination_estimate_pct, contamination_status\n",
    "    FROM contamination_checks\n",
    "    ORDER BY contamination_estimate_pct\n",
    "\"\"\", conn)\n",
    "\n",
    "# Create box plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "box_colors = {'PASS': '#2ecc71', 'FAIL': '#e74c3c'}\n",
    "bp = plt.boxplot([contam_df[contam_df['contamination_status'] == 'PASS']['contamination_estimate_pct'],\n",
    "                   contam_df[contam_df['contamination_status'] == 'FAIL']['contamination_estimate_pct']],\n",
    "                  labels=['PASS', 'FAIL'],\n",
    "                  patch_artist=True,\n",
    "                  widths=0.6)\n",
    "\n",
    "for patch, status in zip(bp['boxes'], ['PASS', 'FAIL']):\n",
    "    patch.set_facecolor(box_colors[status])\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "plt.axhline(y=2.0, color='red', linestyle='--', linewidth=2, label='Threshold (2%)')\n",
    "plt.ylabel('Contamination Estimate (%)', fontsize=12)\n",
    "plt.xlabel('Contamination Status', fontsize=12)\n",
    "plt.title('Sample Contamination Estimates by Status', fontsize=16, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 6: Coverage Uniformity Across Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Query coverage uniformity data\n",
    "conn = get_db_connection()\n",
    "uniformity_df = pd.read_sql_query(\"\"\"\n",
    "    SELECT \n",
    "        s.sample_name,\n",
    "        c.coverage_10x_pct,\n",
    "        c.coverage_20x_pct,\n",
    "        c.coverage_30x_pct,\n",
    "        c.coverage_100x_pct\n",
    "    FROM coverage_metrics c\n",
    "    JOIN samples s ON c.sample_id = s.sample_id\n",
    "    ORDER BY s.sample_id\n",
    "    LIMIT 20\n",
    "\"\"\", conn)\n",
    "\n",
    "# Create line plot\n",
    "plt.figure(figsize=(14, 7))\n",
    "x = range(len(uniformity_df))\n",
    "plt.plot(x, uniformity_df['coverage_10x_pct'], marker='o', label='10x Coverage', linewidth=2)\n",
    "plt.plot(x, uniformity_df['coverage_20x_pct'], marker='s', label='20x Coverage', linewidth=2)\n",
    "plt.plot(x, uniformity_df['coverage_30x_pct'], marker='^', label='30x Coverage', linewidth=2)\n",
    "plt.plot(x, uniformity_df['coverage_100x_pct'], marker='d', label='100x Coverage', linewidth=2)\n",
    "\n",
    "plt.xlabel('Sample Index', fontsize=12)\n",
    "plt.ylabel('Percentage of Bases Covered (%)', fontsize=12)\n",
    "plt.title('Coverage Uniformity Across Different Depth Thresholds (First 20 Samples)', \n",
    "          fontsize=16, fontweight='bold')\n",
    "plt.legend(loc='lower left', fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(60, 100)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 7: Alignment Rate by Assay Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query alignment data by assay type\n",
    "conn = get_db_connection()\n",
    "alignment_df = pd.read_sql_query(\"\"\"\n",
    "    SELECT \n",
    "        s.assay_type,\n",
    "        AVG(a.alignment_rate_pct) as avg_alignment_rate,\n",
    "        STDDEV(a.alignment_rate_pct) as std_alignment_rate,\n",
    "        COUNT(*) as num_samples\n",
    "    FROM alignment_stats a\n",
    "    JOIN samples s ON a.sample_id = s.sample_id\n",
    "    GROUP BY s.assay_type\n",
    "    ORDER BY avg_alignment_rate DESC\n",
    "\"\"\", conn)\n",
    "\n",
    "# Create bar chart with error bars\n",
    "plt.figure(figsize=(12, 7))\n",
    "x_pos = range(len(alignment_df))\n",
    "bars = plt.bar(x_pos, alignment_df['avg_alignment_rate'], \n",
    "               yerr=alignment_df['std_alignment_rate'],\n",
    "               color='#9b59b6', alpha=0.8, edgecolor='black', capsize=5)\n",
    "\n",
    "plt.xticks(x_pos, alignment_df['assay_type'], rotation=15, ha='right')\n",
    "plt.ylabel('Average Alignment Rate (%)', fontsize=12)\n",
    "plt.xlabel('Assay Type', fontsize=12)\n",
    "plt.title('Average Alignment Rate by Assay Type', fontsize=16, fontweight='bold')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.ylim(90, 100)\n",
    "\n",
    "# Add sample count labels\n",
    "for i, (bar, count) in enumerate(zip(bars, alignment_df['num_samples'])):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.3,\n",
    "            f'n={count}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(\"\\n‚úì All visualizations generated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Database Cleanup and Resource Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_rds_free_tier_instance():\n",
    "    \"\"\"\n",
    "    Delete RDS instance and all associated resources.\n",
    "    Optimized for FAST deletion - skips all snapshots and backups.\n",
    "    WARNING: This will permanently delete all data with NO RECOVERY option!\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üóëÔ∏è  RDS FREE TIER CLEANUP\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"WARNING: Fast deletion mode - NO SNAPSHOTS will be created!\")\n",
    "    print(\"This will permanently delete the RDS instance and ALL data!\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Delete DB instance (FAST mode - no snapshots)\n",
    "        print(f\"Deleting RDS instance: {DB_INSTANCE_IDENTIFIER} (fast mode)...\")\n",
    "        \n",
    "        try:\n",
    "            rds_client.delete_db_instance(\n",
    "                DBInstanceIdentifier=DB_INSTANCE_IDENTIFIER,\n",
    "                SkipFinalSnapshot=True,           # NO final snapshot - FAST\n",
    "                DeleteAutomatedBackups=True       # Delete all backups\n",
    "            )\n",
    "            print(f\"  ‚úì Instance deletion initiated (no snapshot)\")\n",
    "            \n",
    "            # Wait for instance to be deleted\n",
    "            print(\"  Waiting for instance deletion (2-5 minutes)...\")\n",
    "            waiter = rds_client.get_waiter('db_instance_deleted')\n",
    "            waiter.wait(\n",
    "                DBInstanceIdentifier=DB_INSTANCE_IDENTIFIER,\n",
    "                WaiterConfig={'Delay': 20, 'MaxAttempts': 40}\n",
    "            )\n",
    "            print(\"  ‚úì Instance deleted successfully\")\n",
    "            \n",
    "        except rds_client.exceptions.DBInstanceNotFoundFault:\n",
    "            print(\"  ‚úì Instance not found (already deleted)\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Instance deletion issue: {e}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"‚úì All RDS resources cleaned up successfully!\")\n",
    "        print(\"‚úì No snapshots created - deletion was FAST\")\n",
    "        print(\"‚úì FREE TIER hours are now available for other resources\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"\\nIMPORTANT: Verify cleanup in AWS Console:\")\n",
    "        print(\"  1. Go to: RDS > Databases\")\n",
    "        print(f\"  2. Confirm no instance named '{DB_INSTANCE_IDENTIFIER}'\")\n",
    "        print(\"  3. Check: RDS > Snapshots (should be none for this instance)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error during cleanup: {str(e)}\")\n",
    "        print(\"\\nManual cleanup steps:\")\n",
    "        print(\"  1. AWS Console > RDS > Databases\")\n",
    "        print(f\"  2. Select instance: {DB_INSTANCE_IDENTIFIER}\")\n",
    "        print(\"  3. Actions > Delete\")\n",
    "        print(\"  4. UNCHECK 'Create final snapshot'\")\n",
    "        print(\"  5. CHECK 'Delete automated backups'\")\n",
    "        print(\"  6. Type instance name and confirm\")\n",
    "        raise\n",
    "\n",
    "print(\"Fast cleanup function defined.\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"To clean up all resources (FAST mode - NO SNAPSHOTS):\")\n",
    "print(\"  >>> cleanup_rds_free_tier_instance()\")\n",
    "print(\"\\nWARNING: This will permanently delete all data with NO recovery option!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT THE LINE BELOW TO RUN CLEANUP\n",
    "cleanup_rds_free_tier_instance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Database Setup**: Created an RDS PostgreSQL instance using FREE TIER (db.t3.micro)\n",
    "2. **Schema Design**: Designed 7 tables for clinical sequencing QC operations:\n",
    "   - `sequencing_runs` - Run metadata\n",
    "   - `samples` - Patient samples\n",
    "   - `qc_metrics` - Quality control metrics\n",
    "   - `coverage_metrics` - Coverage statistics\n",
    "   - `variant_calls` - Variant calling results\n",
    "   - `contamination_checks` - Contamination detection\n",
    "   - `alignment_stats` - Alignment quality\n",
    "\n",
    "3. **Data Generation**: Created realistic faux QC metrics for 50 samples across 10 sequencing runs\n",
    "\n",
    "4. **Visualizations**: Generated 7 informative plots (same as Aurora version)\n",
    "\n",
    "5. **Resource Cleanup**: Provided function to safely remove all AWS resources\n",
    "\n",
    "## FREE TIER Benefits\n",
    "\n",
    "### Cost Comparison:\n",
    "| Version | Instance Type | Monthly Cost | Free Tier? |\n",
    "|---------|---------------|--------------|------------|\n",
    "| **This Version** | db.t3.micro | **$0.00** | ‚úÖ Yes (12 months) |\n",
    "| Aurora Serverless v2 | 0.5-1.0 ACU | $2-20 | ‚ùå No |\n",
    "\n",
    "### Free Tier Limits:\n",
    "- ‚úÖ 750 hours/month (= 24/7 operation)\n",
    "- ‚úÖ 20 GB storage\n",
    "- ‚úÖ 20 GB backup storage  \n",
    "- ‚úÖ Valid for 12 months from AWS account creation\n",
    "\n",
    "### Important Notes:\n",
    "- This is a FREE TIER setup - $0 cost if within free tier limits\n",
    "- Same PostgreSQL functionality as Aurora Serverless\n",
    "- Publicly accessible database (use VPC in production)\n",
    "- Passwords are hardcoded (use AWS Secrets Manager in production)\n",
    "- Remember to clean up to free resources for other exercises!\n",
    "- Always verify cleanup in AWS Console to ensure no charges\n",
    "\n",
    "## After Free Tier Expires\n",
    "\n",
    "After 12 months, db.t3.micro costs:\n",
    "- **$0.017/hour** = ~$12.41/month if running 24/7\n",
    "- For learning exercises (4 hours): ~$0.07 (7 cents)\n",
    "- Still much cheaper than Aurora Serverless v2!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
