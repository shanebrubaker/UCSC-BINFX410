{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive NGS Quality Control Analysis for Genetic Testing\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates a complete quality control workflow for Illumina NGS sequencing data in clinical genetic testing, including batch normalization. The workflow includes:\n",
    "\n",
    "1. **Simulated Data Generation**: Reference genome, targeted panel, and FASTQ reads\n",
    "2. **Q-score Analysis**: Base quality distribution and per-base quality metrics\n",
    "3. **Adapter Trimming**: Removal of Illumina adapter sequences\n",
    "4. **Quality Trimming**: Trimming low-quality bases\n",
    "5. **FastQC/MultiQC Analysis**: Comprehensive quality metrics\n",
    "6. **Alignment Metrics**: Coverage, evenness, and mapping quality\n",
    "7. **PCR Duplicate Marking**: Detection and marking of duplicates\n",
    "8. **Contamination Detection**: Sample purity assessment\n",
    "9. **Batch Normalization**: Correction for batch effects\n",
    "10. **Final Report**: Pass/fail recommendation based on clinical QC criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before starting, set up your Conda environment (from Terminal):\n",
    "# conda create -n ngs_qc python=3.10\n",
    "# conda activate ngs_qc\n",
    "# conda install -c bioconda -c conda-forge multiqc fastqc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify\n",
    "# multiqc --version\n",
    "# fastqc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic: Check Python environment\n",
    "import sys\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"\\nPython path:\")\n",
    "for path in sys.path:\n",
    "    print(f\"  {path}\")\n",
    "\n",
    "# Install required packages\n",
    "! pip install numpy matplotlib seaborn pandas pysam biopython scipy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix: Install pysam to the exact Python that Jupyter is using\n",
    "import sys\n",
    "print(f\"Installing to: {sys.executable}\")\n",
    "!{sys.executable} -m pip install pysam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "import os\n",
    "import gzip\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "import pysam\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set visualization parameters\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"✓ Environment setup complete!\")\n",
    "print(f\"  NumPy version: {np.__version__}\")\n",
    "print(f\"  Pandas version: {pd.__version__}\")\n",
    "print(f\"  Matplotlib version: {plt.matplotlib.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Generate Simulated Reference Genome and Targeted Panel\n",
    "\n",
    "We'll simulate a targeted panel for cancer genetic testing with 10 clinically relevant regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define targeted panel regions (10 regions for clinical genetic testing)\n",
    "# Simulating genes commonly tested in cancer panels\n",
    "PANEL_REGIONS = [\n",
    "    {'name': 'BRCA1_exon2', 'chr': 'chr17', 'start': 10000, 'end': 10500, 'gene': 'BRCA1'},\n",
    "    {'name': 'BRCA1_exon11', 'chr': 'chr17', 'start': 25000, 'end': 25800, 'gene': 'BRCA1'},\n",
    "    {'name': 'TP53_exon5', 'chr': 'chr17', 'start': 50000, 'end': 50400, 'gene': 'TP53'},\n",
    "    {'name': 'TP53_exon7', 'chr': 'chr17', 'start': 60000, 'end': 60350, 'gene': 'TP53'},\n",
    "    {'name': 'EGFR_exon19', 'chr': 'chr7', 'start': 15000, 'end': 15500, 'gene': 'EGFR'},\n",
    "    {'name': 'EGFR_exon21', 'chr': 'chr7', 'start': 30000, 'end': 30450, 'gene': 'EGFR'},\n",
    "    {'name': 'KRAS_exon2', 'chr': 'chr12', 'start': 8000, 'end': 8400, 'gene': 'KRAS'},\n",
    "    {'name': 'KRAS_exon3', 'chr': 'chr12', 'start': 12000, 'end': 12350, 'gene': 'KRAS'},\n",
    "    {'name': 'PIK3CA_exon9', 'chr': 'chr3', 'start': 20000, 'end': 20500, 'gene': 'PIK3CA'},\n",
    "    {'name': 'PIK3CA_exon20', 'chr': 'chr3', 'start': 35000, 'end': 35550, 'gene': 'PIK3CA'},\n",
    "]\n",
    "\n",
    "def generate_reference_genome(panel_regions, output_file='reference.fasta'):\n",
    "    \"\"\"\n",
    "    Generate a simulated reference genome with chromosomes containing targeted regions.\n",
    "    \"\"\"\n",
    "    chromosomes = {}\n",
    "    \n",
    "    # Determine max position per chromosome\n",
    "    chr_sizes = {}\n",
    "    for region in panel_regions:\n",
    "        chr_name = region['chr']\n",
    "        if chr_name not in chr_sizes:\n",
    "            chr_sizes[chr_name] = region['end'] + 10000\n",
    "        else:\n",
    "            chr_sizes[chr_name] = max(chr_sizes[chr_name], region['end'] + 10000)\n",
    "    \n",
    "    # Generate sequences for each chromosome\n",
    "    for chr_name, size in chr_sizes.items():\n",
    "        bases = ['A', 'C', 'G', 'T']\n",
    "        # GC content around 40% (typical for human genome)\n",
    "        sequence = ''.join(random.choices(bases, weights=[0.3, 0.2, 0.2, 0.3], k=size))\n",
    "        chromosomes[chr_name] = sequence\n",
    "    \n",
    "    # Write to FASTA file\n",
    "    with open(output_file, 'w') as f:\n",
    "        for chr_name, seq in sorted(chromosomes.items()):\n",
    "            f.write(f'>{chr_name}\\n')\n",
    "            for i in range(0, len(seq), 80):\n",
    "                f.write(seq[i:i+80] + '\\n')\n",
    "    \n",
    "    print(f\"✓ Generated reference genome: {output_file}\")\n",
    "    for chr_name, size in sorted(chr_sizes.items()):\n",
    "        print(f\"  {chr_name}: {size:,} bp\")\n",
    "    \n",
    "    return output_file, chromosomes\n",
    "\n",
    "# Generate reference\n",
    "ref_file, chromosomes = generate_reference_genome(PANEL_REGIONS)\n",
    "\n",
    "# Display panel information\n",
    "print(f\"\\n✓ Targeted Panel: 10 regions across {len(set(r['chr'] for r in PANEL_REGIONS))} chromosomes\")\n",
    "print(\"\\nPanel Regions:\")\n",
    "print(\"=\" * 90)\n",
    "df_panel = pd.DataFrame(PANEL_REGIONS)\n",
    "df_panel['size'] = df_panel['end'] - df_panel['start']\n",
    "print(df_panel.to_string(index=False))\n",
    "print(\"=\" * 90)\n",
    "print(f\"Total target size: {df_panel['size'].sum():,} bp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Simulate FASTQ Reads with Realistic Quality Profiles\n",
    "\n",
    "We'll simulate reads with:\n",
    "- Illumina-like quality score degradation\n",
    "- Adapter contamination\n",
    "- PCR duplicates\n",
    "- Off-target reads\n",
    "- Multiple batches for batch normalization demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illumina adapter sequences\n",
    "ILLUMINA_ADAPTER = \"AGATCGGAAGAGC\"\n",
    "ADAPTER_R2 = \"AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT\"\n",
    "\n",
    "def simulate_quality_scores(length, mean_quality=35, degradation_rate=0.002, batch_effect=0):\n",
    "    \"\"\"\n",
    "    Simulate Illumina-like quality scores with degradation and batch effects.\n",
    "    \"\"\"\n",
    "    qualities = []\n",
    "    for i in range(length):\n",
    "        position_factor = np.exp(-degradation_rate * i)\n",
    "        mean_q = mean_quality * position_factor + batch_effect\n",
    "        q = int(np.random.normal(mean_q, 3))\n",
    "        q = max(5, min(40, q))\n",
    "        qualities.append(q)\n",
    "    return qualities\n",
    "\n",
    "def introduce_sequencing_errors(sequence, quality_scores):\n",
    "    \"\"\"\n",
    "    Introduce sequencing errors based on quality scores.\n",
    "    \"\"\"\n",
    "    seq_list = list(sequence)\n",
    "    for i, q in enumerate(quality_scores):\n",
    "        error_prob = 10 ** (-q / 10)\n",
    "        if random.random() < error_prob:\n",
    "            bases = ['A', 'C', 'G', 'T']\n",
    "            bases.remove(seq_list[i])\n",
    "            seq_list[i] = random.choice(bases)\n",
    "    return ''.join(seq_list)\n",
    "\n",
    "def simulate_paired_end_reads(chromosomes, panel_regions, num_reads=10000, \n",
    "                             read_length=150, insert_size=300, \n",
    "                             adapter_contamination=0.05,\n",
    "                             pcr_duplicate_rate=0.15,\n",
    "                             off_target_rate=0.02,\n",
    "                             batch_id=1,\n",
    "                             batch_effect=0):\n",
    "    \"\"\"\n",
    "    Simulate paired-end Illumina reads for targeted sequencing.\n",
    "    \"\"\"\n",
    "    reads_r1 = []\n",
    "    reads_r2 = []\n",
    "    read_metadata = []\n",
    "    \n",
    "    target_sizes = [(r['end'] - r['start']) for r in panel_regions]\n",
    "    total_target = sum(target_sizes)\n",
    "    weights = [s / total_target for s in target_sizes]\n",
    "    \n",
    "    duplicate_positions = set()\n",
    "    \n",
    "    for i in range(num_reads):\n",
    "        is_off_target = random.random() < off_target_rate\n",
    "        \n",
    "        if is_off_target:\n",
    "            chr_name = random.choice(list(chromosomes.keys()))\n",
    "            chr_seq = chromosomes[chr_name]\n",
    "            max_start = len(chr_seq) - insert_size - 100\n",
    "            if max_start > 0:\n",
    "                pos = random.randint(0, max_start)\n",
    "                region_name = 'OFF_TARGET'\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            region = random.choices(panel_regions, weights=weights)[0]\n",
    "            chr_name = region['chr']\n",
    "            region_name = region['name']\n",
    "            chr_seq = chromosomes[chr_name]\n",
    "            \n",
    "            region_size = region['end'] - region['start']\n",
    "            if region_size < insert_size:\n",
    "                pos = region['start']\n",
    "            else:\n",
    "                pos = random.randint(region['start'], region['end'] - insert_size)\n",
    "        \n",
    "        # Check for PCR duplicates\n",
    "        is_duplicate = False\n",
    "        pos_key = (chr_name, pos)\n",
    "        if pos_key in duplicate_positions and random.random() < pcr_duplicate_rate:\n",
    "            is_duplicate = True\n",
    "        duplicate_positions.add(pos_key)\n",
    "        \n",
    "        # Extract fragment\n",
    "        fragment = chr_seq[pos:pos + insert_size]\n",
    "        \n",
    "        # R1: forward read\n",
    "        r1_seq = fragment[:read_length]\n",
    "        r1_qual = simulate_quality_scores(len(r1_seq), batch_effect=batch_effect)\n",
    "        r1_seq = introduce_sequencing_errors(r1_seq, r1_qual)\n",
    "        \n",
    "        # Add adapter contamination\n",
    "        if random.random() < adapter_contamination:\n",
    "            adapter_start = random.randint(int(read_length * 0.7), read_length - 10)\n",
    "            r1_seq = r1_seq[:adapter_start] + ILLUMINA_ADAPTER[:read_length - adapter_start]\n",
    "            for j in range(adapter_start, read_length):\n",
    "                if j < len(r1_qual):\n",
    "                    r1_qual[j] = min(r1_qual[j], 25)\n",
    "        \n",
    "        # R2: reverse complement read\n",
    "        r2_start = max(0, insert_size - read_length)\n",
    "        r2_seq = str(Seq(fragment[r2_start:r2_start + read_length]).reverse_complement())\n",
    "        r2_qual = simulate_quality_scores(len(r2_seq), batch_effect=batch_effect)\n",
    "        r2_seq = introduce_sequencing_errors(r2_seq, r2_qual)\n",
    "        \n",
    "        # Quality strings (Phred+33 encoding)\n",
    "        r1_qual_str = ''.join([chr(q + 33) for q in r1_qual])\n",
    "        r2_qual_str = ''.join([chr(q + 33) for q in r2_qual])\n",
    "        \n",
    "        # Create FASTQ records\n",
    "        read_id = f\"BATCH{batch_id}_{i+1:06d}_{chr_name}_{pos}_{region_name}\"\n",
    "        \n",
    "        reads_r1.append({\n",
    "            'id': f\"@{read_id}/1\",\n",
    "            'seq': r1_seq,\n",
    "            'qual': r1_qual_str\n",
    "        })\n",
    "        \n",
    "        reads_r2.append({\n",
    "            'id': f\"@{read_id}/2\",\n",
    "            'seq': r2_seq,\n",
    "            'qual': r2_qual_str\n",
    "        })\n",
    "        \n",
    "        read_metadata.append({\n",
    "            'read_id': read_id,\n",
    "            'chr': chr_name,\n",
    "            'pos': pos,\n",
    "            'region': region_name,\n",
    "            'is_duplicate': is_duplicate,\n",
    "            'is_off_target': is_off_target,\n",
    "            'batch': batch_id\n",
    "        })\n",
    "    \n",
    "    return reads_r1, reads_r2, read_metadata\n",
    "\n",
    "# Simulate reads from 3 different batches (for batch normalization demo)\n",
    "print(\"Simulating paired-end reads from 3 batches...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "all_reads_r1 = []\n",
    "all_reads_r2 = []\n",
    "all_metadata = []\n",
    "\n",
    "# Batch 1: Normal quality\n",
    "reads_r1_b1, reads_r2_b1, meta_b1 = simulate_paired_end_reads(\n",
    "    chromosomes, PANEL_REGIONS, num_reads=3000, batch_id=1, batch_effect=0\n",
    ")\n",
    "print(f\"Batch 1: {len(reads_r1_b1):,} reads (normal quality)\")\n",
    "\n",
    "# Batch 2: Slightly lower quality\n",
    "reads_r1_b2, reads_r2_b2, meta_b2 = simulate_paired_end_reads(\n",
    "    chromosomes, PANEL_REGIONS, num_reads=3000, batch_id=2, batch_effect=-2\n",
    ")\n",
    "print(f\"Batch 2: {len(reads_r1_b2):,} reads (slightly lower quality)\")\n",
    "\n",
    "# Batch 3: Slightly higher quality\n",
    "reads_r1_b3, reads_r2_b3, meta_b3 = simulate_paired_end_reads(\n",
    "    chromosomes, PANEL_REGIONS, num_reads=4000, batch_id=3, batch_effect=1\n",
    ")\n",
    "print(f\"Batch 3: {len(reads_r1_b3):,} reads (slightly higher quality)\")\n",
    "\n",
    "# Combine all batches\n",
    "all_reads_r1 = reads_r1_b1 + reads_r1_b2 + reads_r1_b3\n",
    "all_reads_r2 = reads_r2_b1 + reads_r2_b2 + reads_r2_b3\n",
    "all_metadata = meta_b1 + meta_b2 + meta_b3\n",
    "\n",
    "print(f\"\\n✓ Total reads: {len(all_reads_r1):,}\")\n",
    "df_meta = pd.DataFrame(all_metadata)\n",
    "print(f\"  PCR duplicates: {df_meta['is_duplicate'].sum():,} ({df_meta['is_duplicate'].mean()*100:.1f}%)\")\n",
    "print(f\"  Off-target reads: {df_meta['is_off_target'].sum():,} ({df_meta['is_off_target'].mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_fastq(reads, filename):\n",
    "    \"\"\"\n",
    "    Write reads to FASTQ file (gzipped).\n",
    "    \"\"\"\n",
    "    with gzip.open(filename, 'wt') as f:\n",
    "        for read in reads:\n",
    "            f.write(f\"{read['id']}\\n\")\n",
    "            f.write(f\"{read['seq']}\\n\")\n",
    "            f.write(\"+\\n\")\n",
    "            f.write(f\"{read['qual']}\\n\")\n",
    "    print(f\"✓ Wrote {len(reads):,} reads to {filename}\")\n",
    "\n",
    "# Write FASTQ files\n",
    "FASTQ_R1 = 'sample_R1.fastq.gz'\n",
    "FASTQ_R2 = 'sample_R2.fastq.gz'\n",
    "\n",
    "write_fastq(all_reads_r1, FASTQ_R1)\n",
    "write_fastq(all_reads_r2, FASTQ_R2)\n",
    "\n",
    "print(f\"\\nFile sizes:\")\n",
    "print(f\"  R1: {os.path.getsize(FASTQ_R1)/1024/1024:.2f} MB\")\n",
    "print(f\"  R2: {os.path.getsize(FASTQ_R2)/1024/1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastQC Analysis on Raw Reads\n",
    "\n",
    "Run FastQC on the raw FASTQ files to generate standard quality reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directories\n",
    "!mkdir -p fastqc_raw fastqc_trimmed\n",
    "\n",
    "# Run FastQC on raw FASTQ files\n",
    "print(\"Running FastQC on raw FASTQ files...\")\n",
    "!fastqc {FASTQ_R1} {FASTQ_R2} -o fastqc_raw/ -t 2 --quiet\n",
    "\n",
    "print(\"\\n✓ FastQC analysis complete for raw reads\")\n",
    "print(f\"  Output directory: fastqc_raw/\")\n",
    "print(f\"  Reports generated:\")\n",
    "print(f\"    - sample_R1_fastqc.html\")\n",
    "print(f\"    - sample_R2_fastqc.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Q-Score Analysis\n",
    "\n",
    "Analyze base quality scores across all reads and identify batch effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_quality_scores(fastq_file, sample_size=None):\n",
    "    \"\"\"\n",
    "    Analyze quality scores from FASTQ file.\n",
    "    \"\"\"\n",
    "    qualities_per_position = defaultdict(list)\n",
    "    read_mean_qualities = []\n",
    "    read_lengths = []\n",
    "    batch_qualities = defaultdict(list)\n",
    "    \n",
    "    count = 0\n",
    "    with gzip.open(fastq_file, 'rt') as f:\n",
    "        while True:\n",
    "            header = f.readline().strip()\n",
    "            if not header:\n",
    "                break\n",
    "            seq = f.readline().strip()\n",
    "            plus = f.readline().strip()\n",
    "            qual = f.readline().strip()\n",
    "            \n",
    "            # Extract batch ID from header\n",
    "            batch_id = header.split('_')[0].replace('@BATCH', '')\n",
    "            \n",
    "            qual_scores = [ord(c) - 33 for c in qual]\n",
    "            \n",
    "            for pos, q in enumerate(qual_scores):\n",
    "                qualities_per_position[pos].append(q)\n",
    "            \n",
    "            mean_q = np.mean(qual_scores)\n",
    "            read_mean_qualities.append(mean_q)\n",
    "            batch_qualities[batch_id].append(mean_q)\n",
    "            read_lengths.append(len(seq))\n",
    "            \n",
    "            count += 1\n",
    "            if sample_size and count >= sample_size:\n",
    "                break\n",
    "    \n",
    "    stats = {\n",
    "        'num_reads': count,\n",
    "        'mean_read_length': np.mean(read_lengths),\n",
    "        'mean_quality': np.mean(read_mean_qualities),\n",
    "        'median_quality': np.median(read_mean_qualities),\n",
    "        'qualities_per_position': qualities_per_position,\n",
    "        'read_mean_qualities': read_mean_qualities,\n",
    "        'batch_qualities': batch_qualities\n",
    "    }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "print(\"Analyzing quality scores...\")\n",
    "qc_r1 = analyze_quality_scores(FASTQ_R1)\n",
    "qc_r2 = analyze_quality_scores(FASTQ_R2)\n",
    "\n",
    "print(\"\\n✓ Quality Score Summary:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"R1: Mean Q = {qc_r1['mean_quality']:.2f}, Median Q = {qc_r1['median_quality']:.2f}\")\n",
    "print(f\"R2: Mean Q = {qc_r2['mean_quality']:.2f}, Median Q = {qc_r2['median_quality']:.2f}\")\n",
    "\n",
    "print(\"\\nQuality by Batch (R1):\")\n",
    "for batch_id in sorted(qc_r1['batch_qualities'].keys()):\n",
    "    batch_mean = np.mean(qc_r1['batch_qualities'][batch_id])\n",
    "    print(f\"  Batch {batch_id}: Mean Q = {batch_mean:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 1: Per-Base Quality Scores and Batch Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Per-base quality\n",
    "positions = sorted(qc_r1['qualities_per_position'].keys())\n",
    "means = [np.mean(qc_r1['qualities_per_position'][pos]) for pos in positions]\n",
    "q25 = [np.percentile(qc_r1['qualities_per_position'][pos], 25) for pos in positions]\n",
    "q75 = [np.percentile(qc_r1['qualities_per_position'][pos], 75) for pos in positions]\n",
    "\n",
    "axes[0].axhspan(28, 40, alpha=0.2, color='green', label='Good quality (Q≥28)')\n",
    "axes[0].axhspan(20, 28, alpha=0.2, color='yellow', label='Moderate (20≤Q<28)')\n",
    "axes[0].axhspan(0, 20, alpha=0.2, color='red', label='Poor (Q<20)')\n",
    "axes[0].fill_between(positions, q25, q75, alpha=0.5, color='blue', label='IQR')\n",
    "axes[0].plot(positions, means, 'b-', linewidth=2, label='Mean quality')\n",
    "axes[0].set_xlabel('Position in read (bp)', fontsize=12)\n",
    "axes[0].set_ylabel('Quality Score', fontsize=12)\n",
    "axes[0].set_title('Per-Base Quality Scores (R1)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylim(0, 42)\n",
    "axes[0].legend(loc='lower left')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Batch effect distribution\n",
    "batch_data = [qc_r1['batch_qualities'][bid] for bid in sorted(qc_r1['batch_qualities'].keys())]\n",
    "batch_labels = [f'Batch {bid}' for bid in sorted(qc_r1['batch_qualities'].keys())]\n",
    "bp = axes[1].boxplot(batch_data, labels=batch_labels, patch_artist=True, showmeans=True)\n",
    "for patch, color in zip(bp['boxes'], ['lightblue', 'lightcoral', 'lightgreen']):\n",
    "    patch.set_facecolor(color)\n",
    "axes[1].axhline(30, color='red', linestyle='--', linewidth=2, label='Q30 threshold')\n",
    "axes[1].set_ylabel('Mean Read Quality', fontsize=12)\n",
    "axes[1].set_title('Quality Distribution by Batch (Before Normalization)', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Q30 rates\n",
    "q30_r1 = sum(1 for q in qc_r1['read_mean_qualities'] if q >= 30) / len(qc_r1['read_mean_qualities']) * 100\n",
    "q30_r2 = sum(1 for q in qc_r2['read_mean_qualities'] if q >= 30) / len(qc_r2['read_mean_qualities']) * 100\n",
    "print(f\"\\nQ30 Rates: R1={q30_r1:.1f}%, R2={q30_r2:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Adapter Detection and Trimming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_adapters(fastq_file, adapter_seq, sample_size=1000):\n",
    "    \"\"\"Detect adapter contamination.\"\"\"\n",
    "    adapter_counts = 0\n",
    "    total_reads = 0\n",
    "    adapter_short = adapter_seq[:10]\n",
    "    \n",
    "    with gzip.open(fastq_file, 'rt') as f:\n",
    "        while True:\n",
    "            header = f.readline().strip()\n",
    "            if not header:\n",
    "                break\n",
    "            seq = f.readline().strip()\n",
    "            plus = f.readline().strip()\n",
    "            qual = f.readline().strip()\n",
    "            \n",
    "            if adapter_short in seq:\n",
    "                adapter_counts += 1\n",
    "            \n",
    "            total_reads += 1\n",
    "            if total_reads >= sample_size:\n",
    "                break\n",
    "    \n",
    "    return adapter_counts / total_reads * 100 if total_reads > 0 else 0\n",
    "\n",
    "print(\"Detecting adapter contamination...\")\n",
    "adapter_rate_r1 = detect_adapters(FASTQ_R1, ILLUMINA_ADAPTER, sample_size=10000)\n",
    "adapter_rate_r2 = detect_adapters(FASTQ_R2, ADAPTER_R2, sample_size=10000)\n",
    "print(f\"✓ Adapter contamination: R1={adapter_rate_r1:.2f}%, R2={adapter_rate_r2:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_adapters_and_quality(reads, adapter_seq, min_quality=20, min_length=50):\n",
    "    \"\"\"Trim adapters and low-quality bases.\"\"\"\n",
    "    trimmed_reads = []\n",
    "    trim_stats = {'total': len(reads), 'adapter_trimmed': 0, 'quality_trimmed': 0, 'kept': 0}\n",
    "    \n",
    "    adapter_short = adapter_seq[:10]\n",
    "    \n",
    "    for read in reads:\n",
    "        seq = read['seq']\n",
    "        qual = read['qual']\n",
    "        qual_scores = [ord(c) - 33 for c in qual]\n",
    "        \n",
    "        # Adapter trimming\n",
    "        if adapter_short in seq:\n",
    "            adapter_pos = seq.index(adapter_short)\n",
    "            seq = seq[:adapter_pos]\n",
    "            qual_scores = qual_scores[:adapter_pos]\n",
    "            trim_stats['adapter_trimmed'] += 1\n",
    "        \n",
    "        # Quality trimming from 3' end\n",
    "        original_len = len(qual_scores)\n",
    "        while len(qual_scores) > 0 and qual_scores[-1] < min_quality:\n",
    "            seq = seq[:-1]\n",
    "            qual_scores = qual_scores[:-1]\n",
    "        \n",
    "        if len(qual_scores) < original_len:\n",
    "            trim_stats['quality_trimmed'] += 1\n",
    "        \n",
    "        if len(seq) < min_length:\n",
    "            continue\n",
    "        \n",
    "        qual_str = ''.join([chr(q + 33) for q in qual_scores])\n",
    "        trimmed_reads.append({'id': read['id'], 'seq': seq, 'qual': qual_str})\n",
    "        trim_stats['kept'] += 1\n",
    "    \n",
    "    return trimmed_reads, trim_stats\n",
    "\n",
    "print(\"Trimming adapters and low-quality bases...\")\n",
    "trimmed_r1, trim_stats_r1 = trim_adapters_and_quality(all_reads_r1, ILLUMINA_ADAPTER)\n",
    "trimmed_r2, trim_stats_r2 = trim_adapters_and_quality(all_reads_r2, ADAPTER_R2)\n",
    "\n",
    "print(f\"\\n✓ Trimming Results:\")\n",
    "print(f\"  R1: {trim_stats_r1['kept']:,}/{trim_stats_r1['total']:,} kept ({trim_stats_r1['kept']/trim_stats_r1['total']*100:.1f}%)\")\n",
    "print(f\"  R2: {trim_stats_r2['kept']:,}/{trim_stats_r2['total']:,} kept ({trim_stats_r2['kept']/trim_stats_r2['total']*100:.1f}%)\")\n",
    "\n",
    "# Write trimmed files\n",
    "write_fastq(trimmed_r1, 'sample_trimmed_R1.fastq.gz')\n",
    "write_fastq(trimmed_r2, 'sample_trimmed_R2.fastq.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastQC Analysis on Trimmed Reads\n",
    "\n",
    "Run FastQC on the trimmed FASTQ files to assess quality improvement after trimming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run FastQC on trimmed FASTQ files\n",
    "print(\"Running FastQC on trimmed FASTQ files...\")\n",
    "!fastqc sample_trimmed_R1.fastq.gz sample_trimmed_R2.fastq.gz -o fastqc_trimmed/ -t 2 --quiet\n",
    "\n",
    "print(\"\\n✓ FastQC analysis complete for trimmed reads\")\n",
    "print(f\"  Output directory: fastqc_trimmed/\")\n",
    "print(f\"  Reports generated:\")\n",
    "print(f\"    - sample_trimmed_R1_fastqc.html\")\n",
    "print(f\"    - sample_trimmed_R2_fastqc.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiQC Report Generation\n",
    "\n",
    "Aggregate all FastQC reports using MultiQC to create a comprehensive quality control summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory for MultiQC\n",
    "!mkdir -p multiqc_report\n",
    "\n",
    "# Run MultiQC to aggregate all FastQC reports\n",
    "print(\"Running MultiQC to aggregate all FastQC reports...\")\n",
    "!multiqc fastqc_raw/ fastqc_trimmed/ -o multiqc_report/ --force --quiet\n",
    "\n",
    "# Verify the report was actually generated\n",
    "import os\n",
    "if os.path.exists(\"multiqc_report/multiqc_report.html\"):\n",
    "    print(\"\\n✓ MultiQC report generated successfully\")\n",
    "    print(f\"  Output directory: multiqc_report/\")\n",
    "    print(f\"  Report file: multiqc_report/multiqc_report.html\")\n",
    "    print(f\"\\nMultiQC Summary:\")\n",
    "    print(f\"  • Aggregated FastQC results from raw and trimmed reads\")\n",
    "    print(f\"  • Includes quality metrics, adapter content, sequence duplication levels\")\n",
    "    print(f\"  • Provides comparative analysis across all samples\")\n",
    "else:\n",
    "    print(\"\\n✗ ERROR: MultiQC report was NOT generated!\")\n",
    "    print(\"  Check that MultiQC is installed: pip install multiqc\")\n",
    "    print(\"  You may also need: pip install \\\"pydantic>=2.0\\\"\")\n",
    "    raise RuntimeError(\"MultiQC failed to generate report\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open MultiQC Report in Browser\n",
    "\n",
    "Open the comprehensive MultiQC HTML report in your default web browser for interactive exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open MultiQC report in browser\n",
    "import webbrowser\n",
    "import os\n",
    "\n",
    "multiqc_report_path = os.path.abspath('multiqc_report/multiqc_report.html')\n",
    "print(f\"Opening MultiQC report in browser...\")\n",
    "print(f\"Report location: {multiqc_report_path}\")\n",
    "\n",
    "# Open in default browser\n",
    "webbrowser.open(f'file://{multiqc_report_path}')\n",
    "\n",
    "print(\"\\n✓ MultiQC report opened in browser\")\n",
    "print(\"\\nThe report includes:\")\n",
    "print(\"  • General Statistics - Overview of all QC metrics\")\n",
    "print(\"  • FastQC Results - Detailed quality metrics for each FASTQ file\")\n",
    "print(\"  • Sequence Quality - Per-base and per-sequence quality scores\")\n",
    "print(\"  • Adapter Content - Adapter contamination levels\")\n",
    "print(\"  • Sequence Duplication - Duplication levels across reads\")\n",
    "print(\"  • Overrepresented Sequences - Identification of contamination\")\n",
    "print(\"\\nYou can now interact with the MultiQC report in your browser!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Simulate Alignment and Calculate Coverage Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index reference\n",
    "!samtools faidx {ref_file}\n",
    "print(f\"✓ Indexed reference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_alignment(trimmed_r1, trimmed_r2, chromosomes, output_bam='aligned_sorted.bam'):\n",
    "    \"\"\"Simulate alignment (in real workflow use BWA).\"\"\"\n",
    "    header = {'HD': {'VN': '1.6', 'SO': 'coordinate'}}\n",
    "    sq_entries = [{'SN': chr_name, 'LN': len(chromosomes[chr_name])} for chr_name in sorted(chromosomes.keys())]\n",
    "    header['SQ'] = sq_entries\n",
    "    header['PG'] = [{'ID': 'sim_aligner', 'PN': 'sim_align', 'VN': '1.0'}]\n",
    "    \n",
    "    temp_bam = 'temp.bam'\n",
    "    bamfile = pysam.AlignmentFile(temp_bam, 'wb', header=header)\n",
    "    \n",
    "    chr_to_tid = {chr_name: i for i, chr_name in enumerate(sorted(chromosomes.keys()))}\n",
    "    mapping_qualities = []\n",
    "    \n",
    "    for r1, r2 in zip(trimmed_r1, trimmed_r2):\n",
    "        read_id = r1['id'].strip('@').rstrip('/1')\n",
    "        parts = read_id.split('_')\n",
    "        \n",
    "        if len(parts) < 4:\n",
    "            continue\n",
    "        \n",
    "        chr_name = parts[2]\n",
    "        try:\n",
    "            pos = int(parts[3])\n",
    "        except ValueError:\n",
    "            continue\n",
    "        \n",
    "        if chr_name not in chr_to_tid:\n",
    "            continue\n",
    "        \n",
    "        r1_qual_scores = [ord(c) - 33 for c in r1['qual']]\n",
    "        mapq = min(60, int(np.mean(r1_qual_scores) * 1.5))\n",
    "        mapping_qualities.append(mapq)\n",
    "        \n",
    "        # Create alignments\n",
    "        a1 = pysam.AlignedSegment()\n",
    "        a1.query_name = read_id\n",
    "        a1.query_sequence = r1['seq']\n",
    "        a1.flag = 99\n",
    "        a1.reference_id = chr_to_tid[chr_name]\n",
    "        a1.reference_start = pos\n",
    "        a1.mapping_quality = mapq\n",
    "        a1.cigar = [(0, len(r1['seq']))]\n",
    "        a1.query_qualities = pysam.qualitystring_to_array(r1['qual'])\n",
    "        a1.next_reference_id = chr_to_tid[chr_name]\n",
    "        a1.next_reference_start = pos + 200\n",
    "        a1.template_length = 300\n",
    "        a1.set_tag('NM', 0)\n",
    "        \n",
    "        bamfile.write(a1)\n",
    "    \n",
    "    bamfile.close()\n",
    "    \n",
    "    pysam.sort('-o', output_bam, temp_bam)\n",
    "    pysam.index(output_bam)\n",
    "    os.remove(temp_bam)\n",
    "    \n",
    "    return output_bam, mapping_qualities\n",
    "\n",
    "print(\"Simulating alignment...\")\n",
    "bam_file, mapping_qualities = simulate_alignment(trimmed_r1, trimmed_r2, chromosomes)\n",
    "print(f\"✓ Created BAM: {bam_file}\")\n",
    "print(f\"  Mean MAPQ: {np.mean(mapping_qualities):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_coverage_metrics(bam_file, panel_regions):\n",
    "    \"\"\"Calculate coverage metrics for targeted regions.\"\"\"\n",
    "    bamfile = pysam.AlignmentFile(bam_file, 'rb')\n",
    "    coverage_data = []\n",
    "    \n",
    "    for region in panel_regions:\n",
    "        chr_name = region['chr']\n",
    "        start = region['start']\n",
    "        end = region['end']\n",
    "        region_name = region['name']\n",
    "        gene = region['gene']\n",
    "        \n",
    "        # Get depth at each position\n",
    "        depths = []\n",
    "        for pileupcolumn in bamfile.pileup(chr_name, start, end, truncate=True):\n",
    "            if start <= pileupcolumn.pos < end:\n",
    "                depths.append(pileupcolumn.n)\n",
    "        \n",
    "        # Fill in zeros for positions with no coverage\n",
    "        depth_dict = defaultdict(int)\n",
    "        for pileupcolumn in bamfile.pileup(chr_name, start, end, truncate=True):\n",
    "            if start <= pileupcolumn.pos < end:\n",
    "                depth_dict[pileupcolumn.pos] = pileupcolumn.n\n",
    "        \n",
    "        depths = [depth_dict.get(pos, 0) for pos in range(start, end)]\n",
    "        \n",
    "        if len(depths) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mean_cov = np.mean(depths)\n",
    "        median_cov = np.median(depths)\n",
    "        min_cov = np.min(depths)\n",
    "        max_cov = np.max(depths)\n",
    "        \n",
    "        # Coverage uniformity (coefficient of variation)\n",
    "        cv = (np.std(depths) / mean_cov * 100) if mean_cov > 0 else 0\n",
    "        \n",
    "        # Percentage of bases at various thresholds\n",
    "        pct_20x = sum(1 for d in depths if d >= 20) / len(depths) * 100\n",
    "        pct_100x = sum(1 for d in depths if d >= 100) / len(depths) * 100\n",
    "        pct_500x = sum(1 for d in depths if d >= 500) / len(depths) * 100\n",
    "        \n",
    "        coverage_data.append({\n",
    "            'region': region_name,\n",
    "            'gene': gene,\n",
    "            'chr': chr_name,\n",
    "            'start': start,\n",
    "            'end': end,\n",
    "            'size': end - start,\n",
    "            'mean_coverage': mean_cov,\n",
    "            'median_coverage': median_cov,\n",
    "            'min_coverage': min_cov,\n",
    "            'max_coverage': max_cov,\n",
    "            'evenness_cv': cv,\n",
    "            'pct_20x': pct_20x,\n",
    "            'pct_100x': pct_100x,\n",
    "            'pct_500x': pct_500x,\n",
    "            'depths': depths\n",
    "        })\n",
    "    \n",
    "    bamfile.close()\n",
    "    \n",
    "    return pd.DataFrame(coverage_data)\n",
    "\n",
    "print(\"Calculating coverage metrics...\")\n",
    "df_coverage = calculate_coverage_metrics(bam_file, PANEL_REGIONS)\n",
    "\n",
    "print(\"\\n✓ Coverage Metrics:\")\n",
    "print(\"=\" * 90)\n",
    "print(df_coverage[['region', 'gene', 'mean_coverage', 'evenness_cv', 'pct_20x', 'pct_100x']].to_string(index=False))\n",
    "print(\"=\" * 90)\n",
    "print(f\"\\nOverall Statistics:\")\n",
    "print(f\"  Mean coverage: {df_coverage['mean_coverage'].mean():.1f}x\")\n",
    "print(f\"  Mean CV: {df_coverage['evenness_cv'].mean():.1f}%\")\n",
    "print(f\"  % bases ≥20x: {df_coverage['pct_20x'].mean():.1f}%\")\n",
    "print(f\"  % bases ≥100x: {df_coverage['pct_100x'].mean():.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 2: Coverage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# 1. Coverage by region\n",
    "ax1 = axes[0, 0]\n",
    "regions_short = [r.split('_')[0][:10] for r in df_coverage['region']]\n",
    "colors_regions = plt.cm.tab10(np.linspace(0, 1, len(df_coverage)))\n",
    "ax1.bar(range(len(df_coverage)), df_coverage['mean_coverage'], color=colors_regions, edgecolor='black')\n",
    "ax1.axhline(100, color='green', linestyle='--', linewidth=2, label='Target: 100x')\n",
    "ax1.axhline(20, color='orange', linestyle='--', linewidth=2, label='Minimum: 20x')\n",
    "ax1.set_xticks(range(len(df_coverage)))\n",
    "ax1.set_xticklabels(regions_short, rotation=45, ha='right', fontsize=9)\n",
    "ax1.set_ylabel('Mean Coverage (x)', fontsize=11)\n",
    "ax1.set_title('Mean Coverage by Region', fontweight='bold', fontsize=12)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 2. Coverage uniformity (CV)\n",
    "ax2 = axes[0, 1]\n",
    "ax2.bar(range(len(df_coverage)), df_coverage['evenness_cv'], color='skyblue', edgecolor='black')\n",
    "ax2.axhline(35, color='red', linestyle='--', linewidth=2, label='Max: 35%')\n",
    "ax2.set_xticks(range(len(df_coverage)))\n",
    "ax2.set_xticklabels(regions_short, rotation=45, ha='right', fontsize=9)\n",
    "ax2.set_ylabel('Coefficient of Variation (%)', fontsize=11)\n",
    "ax2.set_title('Coverage Uniformity (Lower is Better)', fontweight='bold', fontsize=12)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 3. Coverage distribution for a sample region\n",
    "ax3 = axes[1, 0]\n",
    "sample_region_idx = 0\n",
    "sample_region = df_coverage.iloc[sample_region_idx]\n",
    "positions = np.arange(len(sample_region['depths']))\n",
    "ax3.fill_between(positions, sample_region['depths'], alpha=0.6, color='blue', label='Coverage')\n",
    "ax3.axhline(sample_region['mean_coverage'], color='red', linestyle='--', \n",
    "            linewidth=2, label=f'Mean: {sample_region[\"mean_coverage\"]:.1f}x')\n",
    "ax3.axhline(20, color='orange', linestyle='--', linewidth=2, label='20x threshold')\n",
    "ax3.set_xlabel('Position in region', fontsize=11)\n",
    "ax3.set_ylabel('Coverage depth', fontsize=11)\n",
    "ax3.set_title(f'Coverage Profile: {sample_region[\"region\"]}', fontweight='bold', fontsize=12)\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Percentage of bases meeting coverage thresholds\n",
    "ax4 = axes[1, 1]\n",
    "thresholds = ['≥20x', '≥100x', '≥500x']\n",
    "mean_pcts = [df_coverage['pct_20x'].mean(), \n",
    "             df_coverage['pct_100x'].mean(), \n",
    "             df_coverage['pct_500x'].mean()]\n",
    "colors_thresh = ['green', 'orange', 'red']\n",
    "bars = ax4.bar(thresholds, mean_pcts, color=colors_thresh, edgecolor='black', alpha=0.7)\n",
    "ax4.axhline(95, color='blue', linestyle='--', linewidth=2, label='95% target')\n",
    "ax4.set_ylabel('% of Bases', fontsize=11)\n",
    "ax4.set_title('Coverage Threshold Achievement', fontweight='bold', fontsize=12)\n",
    "ax4.set_ylim(0, 105)\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add values on bars\n",
    "for bar, val in zip(bars, mean_pcts):\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{val:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCoverage Summary:\")\n",
    "print(f\"  Best covered region: {df_coverage.loc[df_coverage['mean_coverage'].idxmax(), 'region']} ({df_coverage['mean_coverage'].max():.1f}x)\")\n",
    "print(f\"  Worst covered region: {df_coverage.loc[df_coverage['mean_coverage'].idxmin(), 'region']} ({df_coverage['mean_coverage'].min():.1f}x)\")\n",
    "print(f\"  Most uniform region (lowest CV): {df_coverage.loc[df_coverage['evenness_cv'].idxmin(), 'region']} ({df_coverage['evenness_cv'].min():.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: PCR Duplicate Marking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_pcr_duplicates(bam_file):\n",
    "    \"\"\"\n",
    "    Mark PCR duplicates in BAM file.\n",
    "    In a real workflow, use Picard MarkDuplicates.\n",
    "    This simulates duplicate detection based on alignment position.\n",
    "    \"\"\"\n",
    "    bamfile = pysam.AlignmentFile(bam_file, 'rb')\n",
    "    \n",
    "    # Track reads by position\n",
    "    position_reads = defaultdict(list)\n",
    "    total_reads = 0\n",
    "    \n",
    "    for read in bamfile:\n",
    "        if read.is_unmapped:\n",
    "            continue\n",
    "        \n",
    "        total_reads += 1\n",
    "        key = (read.reference_name, read.reference_start, read.is_reverse)\n",
    "        position_reads[key].append(read)\n",
    "    \n",
    "    # Count duplicates\n",
    "    duplicates = 0\n",
    "    for key, reads in position_reads.items():\n",
    "        if len(reads) > 1:\n",
    "            # Keep the read with highest quality, mark others as duplicates\n",
    "            duplicates += len(reads) - 1\n",
    "    \n",
    "    bamfile.close()\n",
    "    \n",
    "    duplicate_rate = (duplicates / total_reads * 100) if total_reads > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'total_reads': total_reads,\n",
    "        'duplicates': duplicates,\n",
    "        'unique_reads': total_reads - duplicates,\n",
    "        'duplicate_rate': duplicate_rate\n",
    "    }\n",
    "\n",
    "print(\"Marking PCR duplicates...\")\n",
    "dup_stats = mark_pcr_duplicates(bam_file)\n",
    "\n",
    "print(f\"\\n✓ PCR Duplicate Analysis:\")\n",
    "print(f\"  Total reads: {dup_stats['total_reads']:,}\")\n",
    "print(f\"  Unique reads: {dup_stats['unique_reads']:,}\")\n",
    "print(f\"  Duplicates: {dup_stats['duplicates']:,}\")\n",
    "print(f\"  Duplicate rate: {dup_stats['duplicate_rate']:.1f}%\")\n",
    "print(f\"  Status: {'PASS' if dup_stats['duplicate_rate'] < 30.0 else 'FAIL'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Contamination Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_contamination(bam_file, min_alt_freq=0.02, min_coverage=100):\n",
    "    \"\"\"Detect contamination by looking for unexpected minor alleles.\"\"\"\n",
    "    bamfile = pysam.AlignmentFile(bam_file, 'rb')\n",
    "    suspicious_sites = []\n",
    "    total_sites = 0\n",
    "    \n",
    "    for chr_name in bamfile.references:\n",
    "        chr_length = bamfile.get_reference_length(chr_name)\n",
    "        \n",
    "        for _ in range(100):\n",
    "            pos = random.randint(0, chr_length - 1)\n",
    "            bases = defaultdict(int)\n",
    "            \n",
    "            for pileupcolumn in bamfile.pileup(chr_name, pos, pos + 1, truncate=True):\n",
    "                if pileupcolumn.pos == pos:\n",
    "                    for pileupread in pileupcolumn.pileups:\n",
    "                        if not pileupread.is_del and not pileupread.is_refskip:\n",
    "                            base = pileupread.alignment.query_sequence[pileupread.query_position]\n",
    "                            bases[base] += 1\n",
    "            \n",
    "            if sum(bases.values()) < min_coverage:\n",
    "                continue\n",
    "            \n",
    "            total_sites += 1\n",
    "            \n",
    "            if len(bases) > 1:\n",
    "                sorted_bases = sorted(bases.items(), key=lambda x: x[1], reverse=True)\n",
    "                minor_freq = sorted_bases[1][1] / sum(bases.values())\n",
    "                \n",
    "                if min_alt_freq < minor_freq < 0.40:\n",
    "                    suspicious_sites.append(minor_freq)\n",
    "    \n",
    "    bamfile.close()\n",
    "    \n",
    "    estimated_contamination = np.median(suspicious_sites) * 100 if suspicious_sites else 0\n",
    "    \n",
    "    return {\n",
    "        'total_sites_checked': total_sites,\n",
    "        'suspicious_sites': len(suspicious_sites),\n",
    "        'estimated_contamination': estimated_contamination\n",
    "    }\n",
    "\n",
    "print(\"Detecting contamination...\")\n",
    "contam_stats = detect_contamination(bam_file)\n",
    "print(f\"\\n✓ Contamination Detection:\")\n",
    "print(f\"  Sites checked: {contam_stats['total_sites_checked']:,}\")\n",
    "print(f\"  Estimated contamination: {contam_stats['estimated_contamination']:.2f}%\")\n",
    "print(f\"  Status: {'PASS' if contam_stats['estimated_contamination'] < 2.0 else 'FAIL'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Batch Normalization\n",
    "\n",
    "Apply batch normalization to correct for systematic differences between sequencing batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_batch_normalization(df_metadata, qc_stats):\n",
    "    \"\"\"\n",
    "    Perform batch normalization on quality scores and coverage metrics.\n",
    "    \"\"\"\n",
    "    # Extract batch-specific metrics\n",
    "    batch_metrics = []\n",
    "    \n",
    "    for batch_id in sorted(qc_stats['batch_qualities'].keys()):\n",
    "        batch_quals = qc_stats['batch_qualities'][batch_id]\n",
    "        batch_metrics.append({\n",
    "            'batch': int(batch_id),\n",
    "            'mean_quality': np.mean(batch_quals),\n",
    "            'median_quality': np.median(batch_quals),\n",
    "            'std_quality': np.std(batch_quals),\n",
    "            'num_reads': len(batch_quals)\n",
    "        })\n",
    "    \n",
    "    df_batches = pd.DataFrame(batch_metrics)\n",
    "    \n",
    "    # Calculate normalization factors using Z-score normalization\n",
    "    overall_mean = df_batches['mean_quality'].mean()\n",
    "    overall_std = df_batches['mean_quality'].std()\n",
    "    \n",
    "    df_batches['z_score'] = (df_batches['mean_quality'] - overall_mean) / overall_std\n",
    "    df_batches['normalization_factor'] = overall_mean - df_batches['mean_quality']\n",
    "    \n",
    "    # Apply normalization to quality scores\n",
    "    normalized_batch_qualities = {}\n",
    "    \n",
    "    for batch_id in sorted(qc_stats['batch_qualities'].keys()):\n",
    "        batch_idx = int(batch_id) - 1\n",
    "        norm_factor = df_batches.loc[batch_idx, 'normalization_factor']\n",
    "        \n",
    "        original_quals = qc_stats['batch_qualities'][batch_id]\n",
    "        normalized_quals = [q + norm_factor for q in original_quals]\n",
    "        normalized_batch_qualities[batch_id] = normalized_quals\n",
    "    \n",
    "    # Calculate metrics before and after normalization\n",
    "    before_variance = df_batches['mean_quality'].var()\n",
    "    \n",
    "    after_means = [np.mean(normalized_batch_qualities[bid]) for bid in sorted(normalized_batch_qualities.keys())]\n",
    "    after_variance = np.var(after_means)\n",
    "    \n",
    "    normalization_results = {\n",
    "        'batch_summary': df_batches,\n",
    "        'normalized_qualities': normalized_batch_qualities,\n",
    "        'variance_reduction': (before_variance - after_variance) / before_variance * 100,\n",
    "        'before_variance': before_variance,\n",
    "        'after_variance': after_variance\n",
    "    }\n",
    "    \n",
    "    return normalization_results\n",
    "\n",
    "print(\"Performing batch normalization...\")\n",
    "norm_results = perform_batch_normalization(df_meta, qc_r1)\n",
    "\n",
    "print(\"\\n✓ Batch Normalization Results:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nBatch Summary (Before Normalization):\")\n",
    "print(norm_results['batch_summary'][['batch', 'mean_quality', 'std_quality', 'z_score', 'normalization_factor']].to_string(index=False))\n",
    "print(\"\\nNormalization Performance:\")\n",
    "print(f\"  Variance before: {norm_results['before_variance']:.4f}\")\n",
    "print(f\"  Variance after: {norm_results['after_variance']:.4f}\")\n",
    "print(f\"  Variance reduction: {norm_results['variance_reduction']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 3: Batch Normalization Effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Before normalization\n",
    "batch_data_before = [qc_r1['batch_qualities'][bid] for bid in sorted(qc_r1['batch_qualities'].keys())]\n",
    "batch_labels = [f'Batch {bid}' for bid in sorted(qc_r1['batch_qualities'].keys())]\n",
    "bp1 = axes[0].boxplot(batch_data_before, labels=batch_labels, patch_artist=True, showmeans=True)\n",
    "for patch, color in zip(bp1['boxes'], ['lightblue', 'lightcoral', 'lightgreen']):\n",
    "    patch.set_facecolor(color)\n",
    "axes[0].axhline(qc_r1['mean_quality'], color='red', linestyle='--', linewidth=2, label='Overall mean')\n",
    "axes[0].set_ylabel('Mean Read Quality', fontsize=12)\n",
    "axes[0].set_title('Before Batch Normalization', fontsize=14, fontweight='bold')\n",
    "# Auto-adjust ylim to show all data including the mean line\n",
    "all_data_before = [item for sublist in batch_data_before for item in sublist]\n",
    "ymin = min(all_data_before) - 2\n",
    "ymax = max(all_data_before) + 2\n",
    "axes[0].set_ylim(ymin, ymax)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# After normalization\n",
    "batch_data_after = [norm_results['normalized_qualities'][bid] for bid in sorted(norm_results['normalized_qualities'].keys())]\n",
    "bp2 = axes[1].boxplot(batch_data_after, labels=batch_labels, patch_artist=True, showmeans=True)\n",
    "for patch, color in zip(bp2['boxes'], ['lightblue', 'lightcoral', 'lightgreen']):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "all_normalized = []\n",
    "for vals in batch_data_after:\n",
    "    all_normalized.extend(vals)\n",
    "axes[1].axhline(np.mean(all_normalized), color='red', linestyle='--', linewidth=2, label='Overall mean')\n",
    "axes[1].set_ylabel('Mean Read Quality', fontsize=12)\n",
    "axes[1].set_title('After Batch Normalization', fontsize=14, fontweight='bold')\n",
    "# Auto-adjust ylim for after normalization as well\n",
    "all_data_after = [item for sublist in batch_data_after for item in sublist]\n",
    "ymin2 = min(all_data_after) - 2\n",
    "ymax2 = max(all_data_after) + 2\n",
    "axes[1].set_ylim(ymin2, ymax2)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBatch means after normalization:\")\n",
    "for bid in sorted(norm_results['normalized_qualities'].keys()):\n",
    "    mean_after = np.mean(norm_results['normalized_qualities'][bid])\n",
    "    print(f\"  Batch {bid}: {mean_after:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 4: Mapping Quality and Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# MAPQ distribution\n",
    "axes[0].hist(mapping_qualities, bins=60, edgecolor='black', alpha=0.7, color='purple')\n",
    "axes[0].axvline(np.mean(mapping_qualities), color='red', linestyle='--', linewidth=2, \n",
    "                label=f'Mean: {np.mean(mapping_qualities):.1f}')\n",
    "axes[0].axvline(20, color='orange', linestyle='--', linewidth=2, label='Q20 threshold')\n",
    "axes[0].set_xlabel('Mapping Quality (MAPQ)', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title('Mapping Quality Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Duplicate rate pie chart\n",
    "labels = ['Unique Reads', 'PCR Duplicates']\n",
    "sizes = [dup_stats['total_reads'] - dup_stats['duplicates'], dup_stats['duplicates']]\n",
    "colors_dup = ['lightgreen', 'salmon']\n",
    "explode = (0.05, 0.05)\n",
    "\n",
    "axes[1].pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90, \n",
    "            colors=colors_dup, explode=explode, shadow=True,\n",
    "            textprops={'fontsize': 11, 'fontweight': 'bold'})\n",
    "axes[1].set_title('PCR Duplicate Rate', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 5: Comprehensive QC Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.35, wspace=0.3)\n",
    "\n",
    "# 1. Read Quality\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax1.bar(['R1', 'R2'], [qc_r1['mean_quality'], qc_r2['mean_quality']], \n",
    "        color=['skyblue', 'lightcoral'], edgecolor='black', linewidth=2)\n",
    "ax1.axhline(30, color='green', linestyle='--', linewidth=2, label='Q30')\n",
    "ax1.set_ylabel('Mean Quality', fontsize=11)\n",
    "ax1.set_title('Read Quality', fontweight='bold', fontsize=12)\n",
    "ax1.set_ylim(0, 40)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 2. Adapter Contamination\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax2.bar(['R1', 'R2'], [adapter_rate_r1, adapter_rate_r2], \n",
    "        color=['skyblue', 'lightcoral'], edgecolor='black', linewidth=2)\n",
    "ax2.axhline(10, color='red', linestyle='--', linewidth=2, label='10% threshold')\n",
    "ax2.set_ylabel('Contamination %', fontsize=11)\n",
    "ax2.set_title('Adapter Contamination', fontweight='bold', fontsize=12)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 3. Trimming Efficiency\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "ax3.bar(['R1', 'R2'], \n",
    "        [trim_stats_r1['kept']/trim_stats_r1['total']*100, \n",
    "         trim_stats_r2['kept']/trim_stats_r2['total']*100], \n",
    "        color=['skyblue', 'lightcoral'], edgecolor='black', linewidth=2)\n",
    "ax3.axhline(90, color='green', linestyle='--', linewidth=2, label='90% target')\n",
    "ax3.set_ylabel('% Reads Retained', fontsize=11)\n",
    "ax3.set_title('Post-Trimming Retention', fontweight='bold', fontsize=12)\n",
    "ax3.set_ylim(0, 105)\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 4. Coverage by Gene\n",
    "ax4 = fig.add_subplot(gs[1, :])\n",
    "gene_coverage = df_coverage.groupby('gene')['mean_coverage'].mean()\n",
    "colors_gene = plt.cm.Set2(np.linspace(0, 1, len(gene_coverage)))\n",
    "bars = ax4.bar(range(len(gene_coverage)), gene_coverage.values, color=colors_gene, edgecolor='black', linewidth=1.5)\n",
    "ax4.axhline(100, color='green', linestyle='--', linewidth=2, label='Target: 100x')\n",
    "ax4.axhline(20, color='orange', linestyle='--', linewidth=2, label='Min: 20x')\n",
    "ax4.set_xticks(range(len(gene_coverage)))\n",
    "ax4.set_xticklabels(gene_coverage.index, fontsize=11)\n",
    "ax4.set_ylabel('Mean Coverage (x)', fontsize=12)\n",
    "ax4.set_title('Mean Coverage by Gene', fontweight='bold', fontsize=13)\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 5. Batch Normalization Effect\n",
    "ax5 = fig.add_subplot(gs[2, 0])\n",
    "batch_means_before = [np.mean(qc_r1['batch_qualities'][bid]) for bid in sorted(qc_r1['batch_qualities'].keys())]\n",
    "batch_means_after = [np.mean(norm_results['normalized_qualities'][bid]) for bid in sorted(norm_results['normalized_qualities'].keys())]\n",
    "x = np.arange(3)\n",
    "width = 0.35\n",
    "ax5.bar(x - width/2, batch_means_before, width, label='Before', color='lightcoral', edgecolor='black')\n",
    "ax5.bar(x + width/2, batch_means_after, width, label='After', color='lightgreen', edgecolor='black')\n",
    "ax5.set_ylabel('Mean Quality', fontsize=11)\n",
    "ax5.set_title('Batch Normalization', fontweight='bold', fontsize=12)\n",
    "ax5.set_xticks(x)\n",
    "ax5.set_xticklabels(['B1', 'B2', 'B3'])\n",
    "ax5.legend()\n",
    "ax5.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 6. Coverage Thresholds\n",
    "ax6 = fig.add_subplot(gs[2, 1])\n",
    "thresholds = ['≥20x', '≥100x', '≥500x']\n",
    "pct_values = [df_coverage['pct_20x'].mean(), df_coverage['pct_100x'].mean(), df_coverage['pct_500x'].mean()]\n",
    "colors_thresh = ['green', 'orange', 'red']\n",
    "ax6.bar(thresholds, pct_values, color=colors_thresh, edgecolor='black', alpha=0.8, linewidth=1.5)\n",
    "ax6.axhline(95, color='blue', linestyle='--', linewidth=2, label='95% target')\n",
    "ax6.set_ylabel('% of Bases', fontsize=11)\n",
    "ax6.set_title('Coverage Thresholds', fontweight='bold', fontsize=12)\n",
    "ax6.set_ylim(0, 105)\n",
    "ax6.legend()\n",
    "ax6.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 7. Contamination\n",
    "ax7 = fig.add_subplot(gs[2, 2])\n",
    "contam_value = contam_stats['estimated_contamination']\n",
    "contam_color = 'green' if contam_value < 2.0 else 'red'\n",
    "ax7.bar(['Sample'], [contam_value], color=contam_color, edgecolor='black', alpha=0.8, linewidth=2)\n",
    "ax7.axhline(2.0, color='red', linestyle='--', linewidth=2, label='2% threshold')\n",
    "ax7.set_ylabel('Contamination %', fontsize=11)\n",
    "ax7.set_title('Contamination', fontweight='bold', fontsize=12)\n",
    "ax7.set_ylim(0, max(5, contam_value * 1.5))\n",
    "ax7.legend()\n",
    "ax7.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "fig.suptitle('NGS Quality Control Dashboard', fontsize=18, fontweight='bold', y=0.995)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Generate Final QC Report with Pass/Fail Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QC thresholds for clinical genetic testing\n",
    "QC_THRESHOLDS = {\n",
    "    'min_q30_rate': 80.0,\n",
    "    'min_mean_quality': 30.0,\n",
    "    'max_adapter_contamination': 10.0,\n",
    "    'min_trimmed_retention': 85.0,\n",
    "    'min_mean_coverage': 100.0,\n",
    "    'min_pct_20x': 95.0,\n",
    "    'min_pct_100x': 90.0,\n",
    "    'max_coverage_cv': 35.0,\n",
    "    'max_duplicate_rate': 30.0,\n",
    "    'max_contamination': 2.0,\n",
    "    'min_mapq': 20.0,\n",
    "    'max_batch_variance': 5.0,  # Maximum acceptable variance between batches\n",
    "}\n",
    "\n",
    "def evaluate_qc_metrics():\n",
    "    \"\"\"Evaluate all QC metrics against thresholds.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # 1. Read Quality\n",
    "    q30_r1 = sum(1 for q in qc_r1['read_mean_qualities'] if q >= 30) / len(qc_r1['read_mean_qualities']) * 100\n",
    "    q30_r2 = sum(1 for q in qc_r2['read_mean_qualities'] if q >= 30) / len(qc_r2['read_mean_qualities']) * 100\n",
    "    mean_q30 = (q30_r1 + q30_r2) / 2\n",
    "    \n",
    "    results.append({\n",
    "        'Category': 'Read Quality',\n",
    "        'Metric': 'Q30 Rate',\n",
    "        'Value': f'{mean_q30:.1f}%',\n",
    "        'Threshold': f'≥{QC_THRESHOLDS[\"min_q30_rate\"]}%',\n",
    "        'Status': 'PASS' if mean_q30 >= QC_THRESHOLDS['min_q30_rate'] else 'FAIL'\n",
    "    })\n",
    "    \n",
    "    mean_quality = (qc_r1['mean_quality'] + qc_r2['mean_quality']) / 2\n",
    "    results.append({\n",
    "        'Category': 'Read Quality',\n",
    "        'Metric': 'Mean Base Quality',\n",
    "        'Value': f'{mean_quality:.1f}',\n",
    "        'Threshold': f'≥{QC_THRESHOLDS[\"min_mean_quality\"]}',\n",
    "        'Status': 'PASS' if mean_quality >= QC_THRESHOLDS['min_mean_quality'] else 'FAIL'\n",
    "    })\n",
    "    \n",
    "    # 2. Adapter Contamination\n",
    "    max_adapter = max(adapter_rate_r1, adapter_rate_r2)\n",
    "    results.append({\n",
    "        'Category': 'Pre-processing',\n",
    "        'Metric': 'Adapter Contamination',\n",
    "        'Value': f'{max_adapter:.1f}%',\n",
    "        'Threshold': f'≤{QC_THRESHOLDS[\"max_adapter_contamination\"]}%',\n",
    "        'Status': 'PASS' if max_adapter <= QC_THRESHOLDS['max_adapter_contamination'] else 'FAIL'\n",
    "    })\n",
    "    \n",
    "    # 3. Trimming Efficiency\n",
    "    trim_retention = (trim_stats_r1['kept'] + trim_stats_r2['kept']) / (trim_stats_r1['total'] + trim_stats_r2['total']) * 100\n",
    "    results.append({\n",
    "        'Category': 'Pre-processing',\n",
    "        'Metric': 'Post-Trim Retention',\n",
    "        'Value': f'{trim_retention:.1f}%',\n",
    "        'Threshold': f'≥{QC_THRESHOLDS[\"min_trimmed_retention\"]}%',\n",
    "        'Status': 'PASS' if trim_retention >= QC_THRESHOLDS['min_trimmed_retention'] else 'FAIL'\n",
    "    })\n",
    "    \n",
    "    # 4. Mapping Quality\n",
    "    mean_mapq = np.mean(mapping_qualities)\n",
    "    results.append({\n",
    "        'Category': 'Alignment',\n",
    "        'Metric': 'Mean MAPQ',\n",
    "        'Value': f'{mean_mapq:.1f}',\n",
    "        'Threshold': f'≥{QC_THRESHOLDS[\"min_mapq\"]}',\n",
    "        'Status': 'PASS' if mean_mapq >= QC_THRESHOLDS['min_mapq'] else 'FAIL'\n",
    "    })\n",
    "    \n",
    "    # 5. Coverage Metrics\n",
    "    mean_cov = df_coverage['mean_coverage'].mean()\n",
    "    results.append({\n",
    "        'Category': 'Coverage',\n",
    "        'Metric': 'Mean Target Coverage',\n",
    "        'Value': f'{mean_cov:.1f}x',\n",
    "        'Threshold': f'≥{QC_THRESHOLDS[\"min_mean_coverage\"]}x',\n",
    "        'Status': 'PASS' if mean_cov >= QC_THRESHOLDS['min_mean_coverage'] else 'FAIL'\n",
    "    })\n",
    "    \n",
    "    pct_20x = df_coverage['pct_20x'].mean()\n",
    "    results.append({\n",
    "        'Category': 'Coverage',\n",
    "        'Metric': 'Bases ≥20x',\n",
    "        'Value': f'{pct_20x:.1f}%',\n",
    "        'Threshold': f'≥{QC_THRESHOLDS[\"min_pct_20x\"]}%',\n",
    "        'Status': 'PASS' if pct_20x >= QC_THRESHOLDS['min_pct_20x'] else 'FAIL'\n",
    "    })\n",
    "    \n",
    "    pct_100x = df_coverage['pct_100x'].mean()\n",
    "    results.append({\n",
    "        'Category': 'Coverage',\n",
    "        'Metric': 'Bases ≥100x',\n",
    "        'Value': f'{pct_100x:.1f}%',\n",
    "        'Threshold': f'≥{QC_THRESHOLDS[\"min_pct_100x\"]}%',\n",
    "        'Status': 'PASS' if pct_100x >= QC_THRESHOLDS['min_pct_100x'] else 'FAIL'\n",
    "    })\n",
    "    \n",
    "    mean_cv = df_coverage['evenness_cv'].mean()\n",
    "    results.append({\n",
    "        'Category': 'Coverage',\n",
    "        'Metric': 'Coverage Uniformity (CV)',\n",
    "        'Value': f'{mean_cv:.1f}%',\n",
    "        'Threshold': f'≤{QC_THRESHOLDS[\"max_coverage_cv\"]}%',\n",
    "        'Status': 'PASS' if mean_cv <= QC_THRESHOLDS['max_coverage_cv'] else 'FAIL'\n",
    "    })\n",
    "    \n",
    "    # 6. PCR Duplicates\n",
    "    dup_rate = dup_stats['duplicate_rate']\n",
    "    results.append({\n",
    "        'Category': 'Library Quality',\n",
    "        'Metric': 'PCR Duplicate Rate',\n",
    "        'Value': f'{dup_rate:.1f}%',\n",
    "        'Threshold': f'≤{QC_THRESHOLDS[\"max_duplicate_rate\"]}%',\n",
    "        'Status': 'PASS' if dup_rate <= QC_THRESHOLDS['max_duplicate_rate'] else 'FAIL'\n",
    "    })\n",
    "    \n",
    "    # 7. Contamination\n",
    "    contam = contam_stats['estimated_contamination']\n",
    "    results.append({\n",
    "        'Category': 'Sample Quality',\n",
    "        'Metric': 'Contamination',\n",
    "        'Value': f'{contam:.2f}%',\n",
    "        'Threshold': f'≤{QC_THRESHOLDS[\"max_contamination\"]}%',\n",
    "        'Status': 'PASS' if contam <= QC_THRESHOLDS['max_contamination'] else 'FAIL'\n",
    "    })\n",
    "    \n",
    "    # 8. Batch Effect\n",
    "    batch_variance = norm_results['before_variance']\n",
    "    results.append({\n",
    "        'Category': 'Batch Normalization',\n",
    "        'Metric': 'Batch Variance (pre-norm)',\n",
    "        'Value': f'{batch_variance:.2f}',\n",
    "        'Threshold': f'≤{QC_THRESHOLDS[\"max_batch_variance\"]}',\n",
    "        'Status': 'PASS' if batch_variance <= QC_THRESHOLDS['max_batch_variance'] else 'WARNING'\n",
    "    })\n",
    "    \n",
    "    variance_reduction = norm_results['variance_reduction']\n",
    "    results.append({\n",
    "        'Category': 'Batch Normalization',\n",
    "        'Metric': 'Variance Reduction',\n",
    "        'Value': f'{variance_reduction:.1f}%',\n",
    "        'Threshold': '≥50%',\n",
    "        'Status': 'PASS' if variance_reduction >= 50 else 'INFO'\n",
    "    })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Evaluate metrics\n",
    "qc_results = evaluate_qc_metrics()\n",
    "\n",
    "# Overall pass/fail\n",
    "num_fail = (qc_results['Status'] == 'FAIL').sum()\n",
    "num_pass = (qc_results['Status'] == 'PASS').sum()\n",
    "overall_status = 'PASS' if num_fail == 0 else 'FAIL'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final QC Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate trim retention for use in report\n",
    "trim_retention = (trim_stats_r1['kept'] + trim_stats_r2['kept']) / (trim_stats_r1['total'] + trim_stats_r2['total']) * 100\n",
    "\n",
    "# Generate comprehensive report\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\" \" * 30 + \"NGS QUALITY CONTROL REPORT\")\n",
    "print(\" \" * 25 + \"Clinical Genetic Testing Pipeline\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"\\nSample ID: SAMPLE_001\")\n",
    "print(f\"Panel: Cancer Hotspot Panel (10 regions, 5 genes)\")\n",
    "print(f\"Sequencing Platform: Illumina NextSeq\")\n",
    "print(f\"Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Batches: 3 (Batch normalization applied)\")\n",
    "print(f\"\\n\" + \"=\" * 100)\n",
    "print(\"QC METRICS SUMMARY\")\n",
    "print(\"=\" * 100)\n",
    "print(qc_results.to_string(index=False))\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(f\"\\nQC EVALUATION:\")\n",
    "print(f\"  Total metrics: {len(qc_results)}\")\n",
    "print(f\"  PASS: {num_pass}\")\n",
    "print(f\"  FAIL: {num_fail}\")\n",
    "print(f\"  WARNING/INFO: {len(qc_results) - num_pass - num_fail}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 100)\n",
    "if overall_status == 'PASS':\n",
    "    print(f\"  ✓✓✓ OVERALL STATUS: PASS ✓✓✓\")\n",
    "    print(f\"\\n  RECOMMENDATION:\")\n",
    "    print(f\"  • Sample meets all QC criteria for clinical genetic testing\")\n",
    "    print(f\"  • Suitable for variant calling and clinical reporting\")\n",
    "    print(f\"  • Batch effects successfully normalized (variance reduction: {norm_results['variance_reduction']:.1f}%)\")\n",
    "    print(f\"  • All target regions meet minimum coverage requirements\")\n",
    "    print(f\"  • No significant contamination detected\")\n",
    "else:\n",
    "    print(f\"  ✗✗✗ OVERALL STATUS: FAIL ✗✗✗\")\n",
    "    print(f\"\\n  FAILED METRICS:\")\n",
    "    failed_metrics = qc_results[qc_results['Status'] == 'FAIL']\n",
    "    for idx, row in failed_metrics.iterrows():\n",
    "        print(f\"    • {row['Metric']}: {row['Value']} (threshold: {row['Threshold']})\")\n",
    "    print(f\"\\n  RECOMMENDATION:\")\n",
    "    print(f\"    • Sample FAILS QC criteria\")\n",
    "    print(f\"    • RECOMMENDED ACTION: Re-sequence sample or review library preparation\")\n",
    "    print(f\"    • DO NOT proceed with variant calling until issues are resolved\")\n",
    "\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(f\"\\nKEY FINDINGS:\")\n",
    "print(f\"  • Mean coverage across targets: {df_coverage['mean_coverage'].mean():.1f}x\")\n",
    "print(f\"  • Coverage uniformity (mean CV): {df_coverage['evenness_cv'].mean():.1f}%\")\n",
    "print(f\"  • PCR duplicate rate: {dup_stats['duplicate_rate']:.1f}%\")\n",
    "print(f\"  • Estimated contamination: {contam_stats['estimated_contamination']:.2f}%\")\n",
    "print(f\"  • Batch variance reduction after normalization: {norm_results['variance_reduction']:.1f}%\")\n",
    "print(f\"  • Read retention after QC: {trim_retention:.1f}%\")\n",
    "\n",
    "print(f\"\\nNEXT STEPS:\")\n",
    "if overall_status == 'PASS':\n",
    "    print(f\"  1. Proceed with variant calling using GATK or similar tools\")\n",
    "    print(f\"  2. Apply batch-normalized quality scores for improved accuracy\")\n",
    "    print(f\"  3. Generate clinical variant report\")\n",
    "    print(f\"  4. Perform variant interpretation and classification\")\n",
    "else:\n",
    "    print(f\"  1. Review failed metrics and identify root causes\")\n",
    "    print(f\"  2. Consider re-extraction, library prep, or re-sequencing\")\n",
    "    print(f\"  3. Contact lab team for troubleshooting\")\n",
    "    print(f\"  4. DO NOT proceed with clinical reporting\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"Report generated by NGS QC Pipeline v1.0\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 6: Final QC Pass/Fail Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Overall pass/fail\n",
    "status_counts = qc_results['Status'].value_counts()\n",
    "colors_map = {'PASS': 'lightgreen', 'FAIL': 'salmon', 'WARNING': 'yellow', 'INFO': 'lightblue'}\n",
    "colors_status = [colors_map.get(status, 'gray') for status in status_counts.index]\n",
    "\n",
    "axes[0].pie(status_counts.values, labels=status_counts.index, autopct='%1.1f%%',\n",
    "            startangle=90, colors=colors_status, shadow=True, \n",
    "            textprops={'fontsize': 12, 'fontweight': 'bold'})\n",
    "axes[0].set_title(f'Overall QC Status: {overall_status}', \n",
    "                  fontsize=16, fontweight='bold',\n",
    "                  color='green' if overall_status == 'PASS' else 'red')\n",
    "\n",
    "# Category breakdown\n",
    "category_status = qc_results.groupby(['Category', 'Status']).size().unstack(fill_value=0)\n",
    "category_status.plot(kind='barh', stacked=True, ax=axes[1], \n",
    "                     color=[colors_map.get(col, 'gray') for col in category_status.columns],\n",
    "                     edgecolor='black', linewidth=1.5)\n",
    "axes[1].set_xlabel('Number of Metrics', fontsize=12)\n",
    "axes[1].set_ylabel('Category', fontsize=12)\n",
    "axes[1].set_title('QC Metrics by Category', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(title='Status', loc='lower right', framealpha=0.9)\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save reports\n",
    "qc_results.to_csv('qc_report.csv', index=False)\n",
    "print(\"✓ QC report saved to: qc_report.csv\")\n",
    "\n",
    "df_coverage_export = df_coverage.drop(columns=['depths'])\n",
    "df_coverage_export.to_csv('coverage_stats.csv', index=False)\n",
    "print(\"✓ Coverage statistics saved to: coverage_stats.csv\")\n",
    "\n",
    "norm_results['batch_summary'].to_csv('batch_normalization_report.csv', index=False)\n",
    "print(\"✓ Batch normalization report saved to: batch_normalization_report.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓✓✓ All QC analyses complete! ✓✓✓\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## End of Notebook ##"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
