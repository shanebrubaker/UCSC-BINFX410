{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23eeb6b3",
   "metadata": {},
   "source": [
    "# Big Data Bioinformatics Exercises\n",
    "## Processing Large-Scale Sequencing Data with Python\n",
    "\n",
    "**Duration:** 4-5 hour workshop (can be split across 2-3 sessions)\n",
    "\n",
    "**Session Plan:**\n",
    "- **Session 1 (~2 hours):** Parts 0-5 — Foundations\n",
    "  - Parts 0-1: Setup + Memory Efficiency (30 min)\n",
    "  - Parts 2-3: Chunking + Parallel Processing (40 min)\n",
    "  - Parts 4-5: Indexing + Streaming Pipelines (40 min)\n",
    "- **Session 2 (~2 hours):** Parts 6-9 — Advanced Patterns\n",
    "  - Parts 6-7: MapReduce + Tool Integration (50 min)\n",
    "  - Parts 8-9: Profiling + Paired-End (40 min)\n",
    "- **Session 3 / Homework:** Part 10 — Final Project (60+ min)\n",
    "\n",
    "**What you'll learn:** Memory-efficient processing, chunking, parallel execution, indexing, streaming pipelines, MapReduce, tool integration, profiling, and paired-end data handling — all applied to real bioinformatics data formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f87355-9aad-4beb-8b9d-57f3af956a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note ##\n",
    "# Review the README.md file for setup instructions before running #\n",
    "# Install seqkit and fastp #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604cafd7",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 0: Setup and Data Generation\n",
    "\n",
    "Before we can process big data, we need data to process. In this section we'll:\n",
    "1. Set up our environment and global constants\n",
    "2. Learn the FASTQ format (the standard for sequencing data)\n",
    "3. Generate realistic test datasets\n",
    "\n",
    "### The FASTQ Format\n",
    "\n",
    "Each sequencing read is stored as 4 lines:\n",
    "```\n",
    "@READ_ID          <- Header (starts with @)\n",
    "ACGTACGTACGT      <- DNA sequence\n",
    "+                 <- Separator (starts with +)\n",
    "IIIIIIIIIII       <- Quality scores (ASCII-encoded Phred scores)\n",
    "```\n",
    "\n",
    "Quality scores use ASCII encoding: each character maps to a Phred quality score.\n",
    "- `!` (ASCII 33) = Phred 0 (worst)\n",
    "- `I` (ASCII 73) = Phred 40 (best for Illumina)\n",
    "- Higher score = higher confidence in the base call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd6bcaa",
   "metadata": {},
   "source": [
    "### 0.1 Global Constants and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a562a0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import hashlib\n",
    "import shutil\n",
    "import subprocess\n",
    "from io import StringIO\n",
    "from typing import Generator, Tuple, List, Dict, Any, Optional\n",
    "from collections import defaultdict\n",
    "import multiprocessing\n",
    "# Use 'fork' context so worker functions defined in the notebook can be pickled\n",
    "# (default 'spawn' on macOS cannot pickle interactively-defined functions)\n",
    "import platform\n",
    "if platform.system() == 'Darwin':\n",
    "    _mp_context = multiprocessing.get_context('fork')\n",
    "else:\n",
    "    _mp_context = multiprocessing.get_context()\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Data directory\n",
    "DATA_DIR = 'data'\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"Data directory: {os.path.abspath(DATA_DIR)}\")\n",
    "print(f\"CPU cores available: {multiprocessing.cpu_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e978dcc7",
   "metadata": {},
   "source": [
    "### 0.2 Solution Display Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8fd293",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "display(HTML('''<style>\n",
    "/* Solution cells are hidden by default via Jupyter's source_hidden metadata */\n",
    "/* If your environment doesn't support source_hidden, solutions are still\n",
    "   clearly marked with banners */\n",
    "</style>\n",
    "<p><b>Setup complete.</b> Solution cells are hidden by default.\n",
    "Click the \"...\" or expand arrow next to collapsed cells to reveal solutions.</p>\n",
    "'''))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12219fe8",
   "metadata": {},
   "source": [
    "### 0.3 Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32670640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_tool(tool_name: str) -> bool:\n",
    "    \"\"\"Check if a command-line tool is available on PATH.\"\"\"\n",
    "    return shutil.which(tool_name) is not None\n",
    "\n",
    "def file_md5(filepath: str) -> str:\n",
    "    \"\"\"Compute MD5 hash of a file for verification.\"\"\"\n",
    "    h = hashlib.md5()\n",
    "    with open(filepath, 'rb') as f:\n",
    "        for chunk in iter(lambda: f.read(8192), b''):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def file_line_count(filepath: str) -> int:\n",
    "    \"\"\"Count lines in a file efficiently.\"\"\"\n",
    "    count = 0\n",
    "    with open(filepath, 'rb') as f:\n",
    "        for _ in f:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "# Check for optional bioinformatics tools\n",
    "for tool in ['seqkit', 'fastp']:\n",
    "    status = \"FOUND\" if check_tool(tool) else \"not found (optional)\"\n",
    "    print(f\"  {tool}: {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834956e8",
   "metadata": {},
   "source": [
    "### 0.4 FASTQ Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36201fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_fastq(filename: str,\n",
    "                     num_reads: int = 10000,\n",
    "                     read_length: int = 150,\n",
    "                     quality_profile: str = 'high',\n",
    "                     n_rate: float = 0.01,\n",
    "                     seed: int = RANDOM_SEED) -> None:\n",
    "    \"\"\"Generate a realistic test FASTQ file.\n",
    "\n",
    "    Args:\n",
    "        filename: Output file path\n",
    "        num_reads: Number of reads to generate\n",
    "        read_length: Length of each read in bases\n",
    "        quality_profile: 'high', 'medium', or 'low' — controls overall quality\n",
    "        n_rate: Probability of inserting N at low-quality positions\n",
    "        seed: Random seed for reproducibility\n",
    "    \"\"\"\n",
    "    rng = random.Random(seed)\n",
    "    bases = 'ACGT'\n",
    "\n",
    "    with open(filename, 'w') as f:\n",
    "        for i in range(num_reads):\n",
    "            # Generate header\n",
    "            header = f\"@READ_{i+1:07d} length={read_length}\"\n",
    "\n",
    "            # Generate sequence and quality together (position-dependent)\n",
    "            sequence = []\n",
    "            quality = []\n",
    "\n",
    "            for pos in range(read_length):\n",
    "                # Illumina-like decay: high quality plateau then drop at end\n",
    "                base_qual = max(2, int(40 - (pos / read_length) ** 2 * 30))\n",
    "\n",
    "                # Add noise\n",
    "                base_qual = max(2, base_qual + rng.randint(-5, 3))\n",
    "\n",
    "                # Apply quality profile modifier\n",
    "                if quality_profile == 'low':\n",
    "                    base_qual = max(2, base_qual - 15)\n",
    "                elif quality_profile == 'medium':\n",
    "                    base_qual = max(2, base_qual - 7)\n",
    "\n",
    "                quality.append(chr(base_qual + 33))\n",
    "\n",
    "                # Insert N at low-quality positions\n",
    "                if base_qual < 10 and rng.random() < n_rate:\n",
    "                    sequence.append('N')\n",
    "                else:\n",
    "                    sequence.append(rng.choice(bases))\n",
    "\n",
    "            f.write(f\"{header}\\n\")\n",
    "            f.write(''.join(sequence) + '\\n')\n",
    "            f.write('+\\n')\n",
    "            f.write(''.join(quality) + '\\n')\n",
    "\n",
    "    size_mb = os.path.getsize(filename) / (1024 * 1024)\n",
    "    print(f\"Created {filename}: {num_reads:,} reads, {size_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6d30b9",
   "metadata": {},
   "source": [
    "### 0.5 Paired-End Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d9d2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_paired_end_fastq(prefix: str,\n",
    "                           num_reads: int = 10000,\n",
    "                           read_length: int = 150,\n",
    "                           insert_size: int = 300,\n",
    "                           seed: int = RANDOM_SEED) -> Tuple[str, str]:\n",
    "    \"\"\"Generate paired-end FASTQ files with realistic insert sizes.\n",
    "\n",
    "    Simulates Illumina paired-end sequencing:\n",
    "    - Generate a fragment of length ~insert_size\n",
    "    - R1 = first read_length bases (forward)\n",
    "    - R2 = last read_length bases (reverse complement)\n",
    "\n",
    "    Args:\n",
    "        prefix: Output file prefix (creates prefix_R1.fastq, prefix_R2.fastq)\n",
    "        num_reads: Number of read pairs\n",
    "        read_length: Length of each read\n",
    "        insert_size: Mean fragment size (actual size varies +/- 30)\n",
    "        seed: Random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (R1 filename, R2 filename)\n",
    "    \"\"\"\n",
    "    complement = {'A': 'T', 'T': 'A', 'C': 'G', 'G': 'C', 'N': 'N'}\n",
    "    rng = random.Random(seed)\n",
    "    bases = 'ACGT'\n",
    "\n",
    "    r1_file = f\"{prefix}_R1.fastq\"\n",
    "    r2_file = f\"{prefix}_R2.fastq\"\n",
    "\n",
    "    with open(r1_file, 'w') as f1, open(r2_file, 'w') as f2:\n",
    "        for i in range(num_reads):\n",
    "            # Vary insert size around the mean\n",
    "            actual_insert = max(read_length + 10,\n",
    "                               insert_size + rng.randint(-30, 30))\n",
    "\n",
    "            # Generate full fragment\n",
    "            fragment = [rng.choice(bases) for _ in range(actual_insert)]\n",
    "\n",
    "            # R1: forward read from start\n",
    "            r1_seq = fragment[:read_length]\n",
    "\n",
    "            # R2: reverse complement from end\n",
    "            r2_seq = [complement[b] for b in reversed(fragment[-read_length:])]\n",
    "\n",
    "            # Generate quality scores (position-dependent, same model)\n",
    "            r1_qual = []\n",
    "            r2_qual = []\n",
    "            for pos in range(read_length):\n",
    "                base_qual = max(2, int(40 - (pos / read_length) ** 2 * 30))\n",
    "                q1 = max(2, base_qual + rng.randint(-5, 3))\n",
    "                q2 = max(2, base_qual + rng.randint(-5, 3))\n",
    "                r1_qual.append(chr(q1 + 33))\n",
    "                r2_qual.append(chr(q2 + 33))\n",
    "\n",
    "                # Insert Ns at low quality positions\n",
    "                if q1 < 10 and rng.random() < 0.01:\n",
    "                    r1_seq[pos] = 'N'\n",
    "                if q2 < 10 and rng.random() < 0.01:\n",
    "                    r2_seq[pos] = 'N'\n",
    "\n",
    "            # Write R1\n",
    "            f1.write(f\"@READ_{i+1:07d}/1 length={read_length}\\n\")\n",
    "            f1.write(''.join(r1_seq) + '\\n')\n",
    "            f1.write('+\\n')\n",
    "            f1.write(''.join(r1_qual) + '\\n')\n",
    "\n",
    "            # Write R2\n",
    "            f2.write(f\"@READ_{i+1:07d}/2 length={read_length}\\n\")\n",
    "            f2.write(''.join(r2_seq) + '\\n')\n",
    "            f2.write('+\\n')\n",
    "            f2.write(''.join(r2_qual) + '\\n')\n",
    "\n",
    "    for fn in (r1_file, r2_file):\n",
    "        size_mb = os.path.getsize(fn) / (1024 * 1024)\n",
    "        print(f\"Created {fn}: {num_reads:,} reads, {size_mb:.1f} MB\")\n",
    "\n",
    "    return r1_file, r2_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9580a181",
   "metadata": {},
   "source": [
    "### 0.6 Generate All Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9084472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test datasets\n",
    "print(\"Generating test data...\\n\")\n",
    "\n",
    "# Standard quality dataset\n",
    "create_test_fastq(os.path.join(DATA_DIR, 'sample.fastq'),\n",
    "                  num_reads=10000, quality_profile='high', seed=RANDOM_SEED)\n",
    "\n",
    "# Larger dataset for benchmarking\n",
    "create_test_fastq(os.path.join(DATA_DIR, 'large_sample.fastq'),\n",
    "                  num_reads=100000, quality_profile='high', seed=RANDOM_SEED + 1)\n",
    "\n",
    "# Mixed quality dataset\n",
    "create_test_fastq(os.path.join(DATA_DIR, 'mixed_quality.fastq'),\n",
    "                  num_reads=10000, quality_profile='medium', n_rate=0.02, seed=RANDOM_SEED + 2)\n",
    "\n",
    "# Paired-end dataset\n",
    "create_paired_end_fastq(os.path.join(DATA_DIR, 'sample'),\n",
    "                        num_reads=10000, seed=RANDOM_SEED + 3)\n",
    "\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdcd126",
   "metadata": {},
   "source": [
    "### 0.7 Verify Your Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bc3f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify all files were created correctly\n",
    "print(\"Verification:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "expected_files = [\n",
    "    'sample.fastq',\n",
    "    'large_sample.fastq',\n",
    "    'mixed_quality.fastq',\n",
    "    'sample_R1.fastq',\n",
    "    'sample_R2.fastq',\n",
    "]\n",
    "\n",
    "all_ok = True\n",
    "for fname in expected_files:\n",
    "    fpath = os.path.join(DATA_DIR, fname)\n",
    "    if os.path.exists(fpath):\n",
    "        size = os.path.getsize(fpath)\n",
    "        lines = file_line_count(fpath)\n",
    "        reads = lines // 4\n",
    "        md5 = file_md5(fpath)\n",
    "        print(f\"  {fname:25s} {size/1024:8.1f} KB  {reads:>7,} reads  MD5: {md5[:8]}...\")\n",
    "    else:\n",
    "        print(f\"  {fname:25s} MISSING!\")\n",
    "        all_ok = False\n",
    "\n",
    "if all_ok:\n",
    "    print(\"\\nAll files created successfully!\")\n",
    "else:\n",
    "    print(\"\\nWARNING: Some files are missing. Re-run the generation cells above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d505b3",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Memory Efficiency with Generators\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "A typical sequencing run produces millions of reads. Loading them all into memory at once would require gigabytes of RAM. **Generators** let us process one read at a time, using constant memory regardless of file size.\n",
    "\n",
    "### Key Concept: Lists vs. Generators\n",
    "\n",
    "| | List | Generator |\n",
    "|---|---|---|\n",
    "| Memory | Stores ALL items | Stores ONE item at a time |\n",
    "| Access | Random (any index) | Sequential (forward only) |\n",
    "| Reuse | Multiple passes | Single pass |\n",
    "| Speed | Fast access, slow creation | Lazy evaluation |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561502f7",
   "metadata": {},
   "source": [
    "### 1.1 Reading FASTQ: List Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b1fe6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fastq_list(filename: str) -> list:\n",
    "    \"\"\"Read all FASTQ records into a list.\n",
    "\n",
    "    Returns a list of tuples: (header, sequence, quality)\n",
    "    WARNING: Loads entire file into memory!\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    with open(filename, 'r') as f:\n",
    "        while True:\n",
    "            header = f.readline().strip()\n",
    "            if not header:\n",
    "                break\n",
    "            sequence = f.readline().strip()\n",
    "            f.readline()  # skip '+' line\n",
    "            quality = f.readline().strip()\n",
    "            records.append((header, sequence, quality))\n",
    "    return records\n",
    "\n",
    "# Measure memory usage\n",
    "import sys\n",
    "\n",
    "sample_file = os.path.join(DATA_DIR, 'sample.fastq')\n",
    "records = read_fastq_list(sample_file)\n",
    "list_size = sys.getsizeof(records)\n",
    "# Note: getsizeof only measures the list container, not the strings inside\n",
    "total_size = list_size + sum(\n",
    "    sys.getsizeof(r) + sys.getsizeof(r[0]) + sys.getsizeof(r[1]) + sys.getsizeof(r[2])\n",
    "    for r in records\n",
    ")\n",
    "print(f\"Number of records: {len(records):,}\")\n",
    "print(f\"List container size: {list_size:,} bytes\")\n",
    "print(f\"Estimated total memory: {total_size / 1024 / 1024:.1f} MB\")\n",
    "print(f\"First record header: {records[0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0753f7f9",
   "metadata": {},
   "source": [
    "### 1.2 Reading FASTQ: Generator Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d70d9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fastq_generator(filename: str) -> Generator[Tuple[str, str, str], None, None]:\n",
    "    \"\"\"Read FASTQ records one at a time using a generator.\n",
    "\n",
    "    Yields tuples of (header, sequence, quality).\n",
    "    Memory usage is constant regardless of file size.\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        while True:\n",
    "            header = f.readline().strip()\n",
    "            if not header:\n",
    "                break\n",
    "            sequence = f.readline().strip()\n",
    "            f.readline()  # skip '+' line\n",
    "            quality = f.readline().strip()\n",
    "            yield (header, sequence, quality)\n",
    "\n",
    "# Measure generator memory\n",
    "gen = read_fastq_generator(sample_file)\n",
    "gen_size = sys.getsizeof(gen)\n",
    "print(f\"Generator object size: {gen_size} bytes\")\n",
    "print(f\"Memory ratio (list/generator): {total_size / gen_size:.0f}x\")\n",
    "\n",
    "# We can still iterate over it\n",
    "count = 0\n",
    "for record in gen:\n",
    "    count += 1\n",
    "print(f\"Records yielded: {count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea9cb91",
   "metadata": {},
   "source": [
    "### Exercise 1.1: GC Content Calculator\n",
    "\n",
    "GC content is the percentage of bases that are G or C. It's a fundamental quality metric in genomics.\n",
    "\n",
    "**Task:** Implement `gc_content_generator()` that calculates GC content for each read using a generator, then compute the average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6958ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gc_content_generator(filename: str) -> Generator[float, None, None]:\n",
    "    \"\"\"Yield GC content (0.0-1.0) for each read in the FASTQ file.\n",
    "\n",
    "    GC content = (count of G + count of C) / total bases\n",
    "\n",
    "    TODO: Implement this function\n",
    "    - Use read_fastq_generator() to iterate over records\n",
    "    - For each record, calculate GC content from the sequence\n",
    "    - Yield the GC content as a float\n",
    "    \"\"\"\n",
    "    # TODO: Your implementation here\n",
    "    pass\n",
    "\n",
    "# Test your implementation:\n",
    "# gc_values = list(gc_content_generator(sample_file))\n",
    "# avg_gc = sum(gc_values) / len(gc_values)\n",
    "# print(f\"Average GC content: {avg_gc:.4f}\")\n",
    "# You should see approximately 0.50 (since our data uses random bases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91877f45",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# SOLUTION - Exercise 1.1 (click to expand)\n",
    "# =============================================\n",
    "\n",
    "def gc_content_generator(filename: str) -> Generator[float, None, None]:\n",
    "    \"\"\"Yield GC content (0.0-1.0) for each read in the FASTQ file.\"\"\"\n",
    "    for header, sequence, quality in read_fastq_generator(filename):\n",
    "        gc_count = sequence.upper().count('G') + sequence.upper().count('C')\n",
    "        yield gc_count / len(sequence) if len(sequence) > 0 else 0.0\n",
    "\n",
    "# Test\n",
    "gc_values = list(gc_content_generator(sample_file))\n",
    "avg_gc = sum(gc_values) / len(gc_values)\n",
    "print(f\"Average GC content: {avg_gc:.4f}\")\n",
    "print(f\"Min GC: {min(gc_values):.4f}, Max GC: {max(gc_values):.4f}\")\n",
    "print(f\"Number of reads analyzed: {len(gc_values):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806e549e",
   "metadata": {},
   "source": [
    "### Exercise 1.2: Memory Comparison\n",
    "\n",
    "**Task:** Write a function that measures the peak memory used by the list approach vs. the generator approach for computing average quality scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a05801",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_quality_list(filename: str) -> float:\n",
    "    \"\"\"Calculate average quality using the list approach.\n",
    "\n",
    "    TODO: Implement this function\n",
    "    - Load all records into a list using read_fastq_list()\n",
    "    - Calculate the average Phred quality across ALL bases in ALL reads\n",
    "    - Phred score = ord(char) - 33 for each quality character\n",
    "    \"\"\"\n",
    "    # TODO: Your implementation here\n",
    "    pass\n",
    "\n",
    "def avg_quality_generator(filename: str) -> float:\n",
    "    \"\"\"Calculate average quality using the generator approach.\n",
    "\n",
    "    TODO: Implement this function\n",
    "    - Use read_fastq_generator() to iterate\n",
    "    - Keep a running sum and count\n",
    "    - Return the average Phred quality\n",
    "    \"\"\"\n",
    "    # TODO: Your implementation here\n",
    "    pass\n",
    "\n",
    "# Test your implementations:\n",
    "# q_list = avg_quality_list(sample_file)\n",
    "# q_gen = avg_quality_generator(sample_file)\n",
    "# print(f\"List approach avg quality:      {q_list:.2f}\")\n",
    "# print(f\"Generator approach avg quality:  {q_gen:.2f}\")\n",
    "# Both should return the same value (around 28-32 for high-quality data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11891a33",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# SOLUTION - Exercise 1.2 (click to expand)\n",
    "# =============================================\n",
    "\n",
    "def avg_quality_list(filename: str) -> float:\n",
    "    \"\"\"Calculate average quality using the list approach.\"\"\"\n",
    "    records = read_fastq_list(filename)\n",
    "    total_qual = 0\n",
    "    total_bases = 0\n",
    "    for header, sequence, quality in records:\n",
    "        for char in quality:\n",
    "            total_qual += ord(char) - 33\n",
    "            total_bases += 1\n",
    "    return total_qual / total_bases if total_bases > 0 else 0.0\n",
    "\n",
    "def avg_quality_generator(filename: str) -> float:\n",
    "    \"\"\"Calculate average quality using the generator approach.\"\"\"\n",
    "    total_qual = 0\n",
    "    total_bases = 0\n",
    "    for header, sequence, quality in read_fastq_generator(filename):\n",
    "        for char in quality:\n",
    "            total_qual += ord(char) - 33\n",
    "            total_bases += 1\n",
    "    return total_qual / total_bases if total_bases > 0 else 0.0\n",
    "\n",
    "# Test\n",
    "q_list = avg_quality_list(sample_file)\n",
    "q_gen = avg_quality_generator(sample_file)\n",
    "print(f\"List approach avg quality:      {q_list:.2f}\")\n",
    "print(f\"Generator approach avg quality:  {q_gen:.2f}\")\n",
    "print(f\"Results match: {abs(q_list - q_gen) < 0.001}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76565f04",
   "metadata": {},
   "source": [
    "**Discussion**: At what file size does the generator approach become necessary? Consider that a typical laptop has 8-16 GB of RAM, and a single sequencing run can produce 100+ GB of FASTQ data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bb0483",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Chunked Processing\n",
    "\n",
    "### Why Chunks?\n",
    "\n",
    "While generators process one record at a time, sometimes we want to process records in **batches** (chunks). This gives us:\n",
    "- Better I/O efficiency (fewer system calls)\n",
    "- Natural units for parallel processing\n",
    "- Progress reporting at chunk boundaries\n",
    "- Ability to do batch operations (e.g., batch database inserts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eef001",
   "metadata": {},
   "source": [
    "### 2.1 The Chunking Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715b4937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fastq_chunks(filename: str,\n",
    "                     chunk_size: int = 1000) -> Generator[list, None, None]:\n",
    "    \"\"\"Read FASTQ records in chunks of fixed size.\n",
    "\n",
    "    Yields lists of (header, sequence, quality) tuples.\n",
    "    Each yielded list has at most chunk_size records.\n",
    "    \"\"\"\n",
    "    chunk = []\n",
    "    for record in read_fastq_generator(filename):\n",
    "        chunk.append(record)\n",
    "        if len(chunk) >= chunk_size:\n",
    "            yield chunk\n",
    "            chunk = []\n",
    "    if chunk:  # Don't forget the last partial chunk!\n",
    "        yield chunk\n",
    "\n",
    "# Demonstrate chunked reading\n",
    "print(\"Reading in chunks of 2500:\")\n",
    "for i, chunk in enumerate(read_fastq_chunks(sample_file, chunk_size=2500)):\n",
    "    print(f\"  Chunk {i}: {len(chunk)} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39902c3",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Quality Filter with Progress\n",
    "\n",
    "**Task:** Implement a chunked quality filter that reports progress as it processes each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3fe6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_quality_chunked(input_file: str,\n",
    "                             output_file: str,\n",
    "                             min_avg_quality: float = 20.0,\n",
    "                             chunk_size: int = 1000) -> Dict[str, int]:\n",
    "    \"\"\"Filter reads by average quality score, processing in chunks.\n",
    "\n",
    "    TODO: Implement this function\n",
    "    - Process the input file in chunks using read_fastq_chunks()\n",
    "    - For each read, calculate its average Phred quality\n",
    "    - Write reads that meet the minimum quality threshold to output_file\n",
    "    - Print progress after each chunk\n",
    "    - Return a dict with 'total', 'passed', and 'failed' counts\n",
    "    \"\"\"\n",
    "    # TODO: Your implementation here\n",
    "    pass\n",
    "\n",
    "# Test:\n",
    "# stats = filter_by_quality_chunked(\n",
    "#     sample_file,\n",
    "#     os.path.join(DATA_DIR, 'filtered.fastq'),\n",
    "#     min_avg_quality=25.0\n",
    "# )\n",
    "# print(f\"\\nResults: {stats}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3b52e4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# SOLUTION - Exercise 2.1 (click to expand)\n",
    "# =============================================\n",
    "\n",
    "def filter_by_quality_chunked(input_file: str,\n",
    "                             output_file: str,\n",
    "                             min_avg_quality: float = 20.0,\n",
    "                             chunk_size: int = 1000) -> Dict[str, int]:\n",
    "    \"\"\"Filter reads by average quality score, processing in chunks.\"\"\"\n",
    "    stats = {'total': 0, 'passed': 0, 'failed': 0}\n",
    "\n",
    "    with open(output_file, 'w') as out:\n",
    "        for i, chunk in enumerate(read_fastq_chunks(input_file, chunk_size)):\n",
    "            chunk_passed = 0\n",
    "            for header, sequence, quality in chunk:\n",
    "                stats['total'] += 1\n",
    "                avg_qual = sum(ord(c) - 33 for c in quality) / len(quality)\n",
    "\n",
    "                if avg_qual >= min_avg_quality:\n",
    "                    stats['passed'] += 1\n",
    "                    chunk_passed += 1\n",
    "                    out.write(f\"{header}\\n{sequence}\\n+\\n{quality}\\n\")\n",
    "                else:\n",
    "                    stats['failed'] += 1\n",
    "\n",
    "            print(f\"  Chunk {i}: {chunk_passed}/{len(chunk)} passed \"\n",
    "                  f\"(running total: {stats['passed']:,}/{stats['total']:,})\")\n",
    "\n",
    "    return stats\n",
    "\n",
    "# Test\n",
    "stats = filter_by_quality_chunked(\n",
    "    sample_file,\n",
    "    os.path.join(DATA_DIR, 'filtered.fastq'),\n",
    "    min_avg_quality=25.0\n",
    ")\n",
    "print(f\"\\nFinal: {stats['passed']:,} of {stats['total']:,} reads passed \"\n",
    "      f\"({100*stats['passed']/stats['total']:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4543c32",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Chunk Size Experiment\n",
    "\n",
    "**Task:** Measure processing time for different chunk sizes to find the sweet spot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ff3a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_chunk_sizes(filename: str,\n",
    "                         chunk_sizes: list) -> Dict[int, float]:\n",
    "    \"\"\"Benchmark processing time for different chunk sizes.\n",
    "\n",
    "    TODO: Implement this function\n",
    "    - For each chunk_size in chunk_sizes, time how long it takes to:\n",
    "      read all chunks and compute average quality per chunk\n",
    "    - Return a dict mapping chunk_size -> elapsed_time_seconds\n",
    "    \"\"\"\n",
    "    # TODO: Your implementation here\n",
    "    pass\n",
    "\n",
    "# Test:\n",
    "# sizes = [100, 500, 1000, 2500, 5000]\n",
    "# results = benchmark_chunk_sizes(sample_file, sizes)\n",
    "# for size, elapsed in results.items():\n",
    "#     print(f\"  Chunk size {size:5d}: {elapsed:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6127460",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# SOLUTION - Exercise 2.2 (click to expand)\n",
    "# =============================================\n",
    "\n",
    "def benchmark_chunk_sizes(filename: str,\n",
    "                         chunk_sizes: list) -> Dict[int, float]:\n",
    "    \"\"\"Benchmark processing time for different chunk sizes.\"\"\"\n",
    "    results = {}\n",
    "    for size in chunk_sizes:\n",
    "        start = time.perf_counter()\n",
    "        for chunk in read_fastq_chunks(filename, chunk_size=size):\n",
    "            # Simulate work: compute average quality per chunk\n",
    "            for header, sequence, quality in chunk:\n",
    "                _ = sum(ord(c) - 33 for c in quality) / len(quality)\n",
    "        elapsed = time.perf_counter() - start\n",
    "        results[size] = elapsed\n",
    "    return results\n",
    "\n",
    "sizes = [100, 500, 1000, 2500, 5000]\n",
    "results = benchmark_chunk_sizes(sample_file, sizes)\n",
    "print(\"Chunk size benchmarks:\")\n",
    "for size, elapsed in results.items():\n",
    "    print(f\"  Chunk size {size:5d}: {elapsed:.3f}s\")\n",
    "print(\"\\nNote: Very small chunks add overhead from list creation;\")\n",
    "print(\"very large chunks approach list-like memory usage.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eff250f",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Parallel Processing\n",
    "\n",
    "### Why Parallel?\n",
    "\n",
    "Modern CPUs have multiple cores. By splitting work across cores, we can process data faster. The key challenge: **we need to divide the work into independent units** that don't share state.\n",
    "\n",
    "### The Pattern\n",
    "1. **Split** data into chunks\n",
    "2. **Send** each chunk to a separate worker process\n",
    "3. **Collect** and combine results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c5aea3",
   "metadata": {},
   "source": [
    "### 3.1 Worker Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6fb0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_gc_in_chunk(chunk: list) -> Dict[str, Any]:\n",
    "    \"\"\"Worker function: compute GC statistics for a chunk of reads.\n",
    "\n",
    "    This function runs in a separate process, so it must be\n",
    "    self-contained (no references to shared state).\n",
    "    \"\"\"\n",
    "    gc_sum = 0.0\n",
    "    total_bases = 0\n",
    "    n_count = 0\n",
    "\n",
    "    for header, sequence, quality in chunk:\n",
    "        seq_upper = sequence.upper()\n",
    "        gc_sum += seq_upper.count('G') + seq_upper.count('C')\n",
    "        n_count += seq_upper.count('N')\n",
    "        total_bases += len(sequence)\n",
    "\n",
    "    return {\n",
    "        'reads': len(chunk),\n",
    "        'gc_sum': gc_sum,\n",
    "        'total_bases': total_bases,\n",
    "        'n_count': n_count\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f194b7",
   "metadata": {},
   "source": [
    "### 3.2 Sequential vs. Parallel Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa45fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sequential(filename: str, chunk_size: int = 2500) -> dict:\n",
    "    \"\"\"Process file sequentially.\"\"\"\n",
    "    combined = {'reads': 0, 'gc_sum': 0, 'total_bases': 0, 'n_count': 0}\n",
    "    for chunk in read_fastq_chunks(filename, chunk_size):\n",
    "        result = count_gc_in_chunk(chunk)\n",
    "        for key in combined:\n",
    "            combined[key] += result[key]\n",
    "    return combined\n",
    "\n",
    "def process_parallel(filename: str, chunk_size: int = 2500,\n",
    "                     num_workers: int = None) -> dict:\n",
    "    \"\"\"Process file in parallel using multiprocessing.Pool.\"\"\"\n",
    "    if num_workers is None:\n",
    "        num_workers = min(4, multiprocessing.cpu_count())\n",
    "\n",
    "    chunks = list(read_fastq_chunks(filename, chunk_size))\n",
    "\n",
    "    combined = {'reads': 0, 'gc_sum': 0, 'total_bases': 0, 'n_count': 0}\n",
    "\n",
    "    with _mp_context.Pool(processes=num_workers) as pool:\n",
    "        results = pool.map(count_gc_in_chunk, chunks)\n",
    "        for result in results:\n",
    "            for key in combined:\n",
    "                combined[key] += result[key]\n",
    "\n",
    "    return combined\n",
    "\n",
    "# Benchmark on the larger file\n",
    "large_file = os.path.join(DATA_DIR, 'large_sample.fastq')\n",
    "\n",
    "start = time.perf_counter()\n",
    "seq_result = process_sequential(large_file)\n",
    "seq_time = time.perf_counter() - start\n",
    "\n",
    "start = time.perf_counter()\n",
    "par_result = process_parallel(large_file)\n",
    "par_time = time.perf_counter() - start\n",
    "\n",
    "gc_pct = seq_result['gc_sum'] / seq_result['total_bases'] * 100\n",
    "print(f\"GC content: {gc_pct:.2f}%\")\n",
    "print(f\"N bases: {seq_result['n_count']:,}\")\n",
    "print(f\"Sequential: {seq_time:.3f}s\")\n",
    "print(f\"Parallel:   {par_time:.3f}s\")\n",
    "print(f\"Speedup:    {seq_time/par_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a3a3f1",
   "metadata": {},
   "source": [
    "### Exercise 3.1: Parallel Quality Statistics\n",
    "\n",
    "**Task:** Implement a parallel version of quality statistics calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679c7342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quality_stats_chunk(chunk: list) -> Dict[str, Any]:\n",
    "    \"\"\"Worker function: compute quality statistics for a chunk.\n",
    "\n",
    "    TODO: Implement this function\n",
    "    - Calculate: total quality sum, total bases, min quality, max quality\n",
    "    - Return a dict with these values\n",
    "    \"\"\"\n",
    "    # TODO: Your implementation here\n",
    "    pass\n",
    "\n",
    "def parallel_quality_stats(filename: str, num_workers: int = 4) -> dict:\n",
    "    \"\"\"Compute quality stats in parallel.\n",
    "\n",
    "    TODO: Implement this function\n",
    "    - Split file into chunks\n",
    "    - Process chunks in parallel using multiprocessing.Pool\n",
    "    - Combine results (sum totals, take min of mins, max of maxes)\n",
    "    - Return dict with 'avg_quality', 'min_quality', 'max_quality'\n",
    "    \"\"\"\n",
    "    # TODO: Your implementation here\n",
    "    pass\n",
    "\n",
    "# Test:\n",
    "# stats = parallel_quality_stats(large_file)\n",
    "# print(f\"Average quality: {stats['avg_quality']:.2f}\")\n",
    "# print(f\"Min quality: {stats['min_quality']}\")\n",
    "# print(f\"Max quality: {stats['max_quality']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b79036",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# SOLUTION - Exercise 3.1 (click to expand)\n",
    "# =============================================\n",
    "\n",
    "def quality_stats_chunk(chunk: list) -> Dict[str, Any]:\n",
    "    \"\"\"Worker function: compute quality statistics for a chunk.\"\"\"\n",
    "    total_qual = 0\n",
    "    total_bases = 0\n",
    "    min_qual = float('inf')\n",
    "    max_qual = float('-inf')\n",
    "\n",
    "    for header, sequence, quality in chunk:\n",
    "        for char in quality:\n",
    "            q = ord(char) - 33\n",
    "            total_qual += q\n",
    "            total_bases += 1\n",
    "            min_qual = min(min_qual, q)\n",
    "            max_qual = max(max_qual, q)\n",
    "\n",
    "    return {\n",
    "        'total_qual': total_qual,\n",
    "        'total_bases': total_bases,\n",
    "        'min_qual': min_qual,\n",
    "        'max_qual': max_qual\n",
    "    }\n",
    "\n",
    "def parallel_quality_stats(filename: str, num_workers: int = 4) -> dict:\n",
    "    \"\"\"Compute quality stats in parallel.\"\"\"\n",
    "    chunks = list(read_fastq_chunks(filename, chunk_size=2500))\n",
    "\n",
    "    combined_qual = 0\n",
    "    combined_bases = 0\n",
    "    combined_min = float('inf')\n",
    "    combined_max = float('-inf')\n",
    "\n",
    "    with _mp_context.Pool(processes=num_workers) as pool:\n",
    "        for result in pool.map(quality_stats_chunk, chunks):\n",
    "            combined_qual += result['total_qual']\n",
    "            combined_bases += result['total_bases']\n",
    "            combined_min = min(combined_min, result['min_qual'])\n",
    "            combined_max = max(combined_max, result['max_qual'])\n",
    "\n",
    "    return {\n",
    "        'avg_quality': combined_qual / combined_bases if combined_bases > 0 else 0,\n",
    "        'min_quality': combined_min,\n",
    "        'max_quality': combined_max\n",
    "    }\n",
    "\n",
    "stats = parallel_quality_stats(large_file)\n",
    "print(f\"Average quality: {stats['avg_quality']:.2f}\")\n",
    "print(f\"Min quality: {stats['min_quality']}\")\n",
    "print(f\"Max quality: {stats['max_quality']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b11d2d",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Indexing and Random Access\n",
    "\n",
    "### The Problem\n",
    "\n",
    "FASTQ files are sequential: to find read #50,000, you must scan past the first 49,999 reads. With an **index**, we can jump directly to any read in O(1) time.\n",
    "\n",
    "### The Approach\n",
    "\n",
    "Build an index that maps read IDs (or positions) to byte offsets in the file. Then use `file.seek()` to jump directly to any read."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3326f5e",
   "metadata": {},
   "source": [
    "### 4.1 Building a FASTQ Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff7eed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fastq_index(filename: str) -> Dict[str, int]:\n",
    "    \"\"\"Build an index mapping read IDs to byte offsets.\n",
    "\n",
    "    Returns a dict: {read_id: byte_offset}\n",
    "    The byte offset points to the start of the header line.\n",
    "    \"\"\"\n",
    "    index = {}\n",
    "    with open(filename, 'rb') as f:\n",
    "        while True:\n",
    "            offset = f.tell()\n",
    "            header = f.readline()\n",
    "            if not header:\n",
    "                break\n",
    "            # Extract read ID (everything after @ up to first space)\n",
    "            read_id = header.decode().strip().split()[0][1:]  # remove @\n",
    "            index[read_id] = offset\n",
    "            # Skip sequence, +, quality\n",
    "            f.readline()\n",
    "            f.readline()\n",
    "            f.readline()\n",
    "    return index\n",
    "\n",
    "# Build index\n",
    "start = time.perf_counter()\n",
    "index = build_fastq_index(sample_file)\n",
    "index_time = time.perf_counter() - start\n",
    "print(f\"Indexed {len(index):,} reads in {index_time:.3f}s\")\n",
    "print(f\"Index size: {sys.getsizeof(index) / 1024:.1f} KB\")\n",
    "print(f\"Sample entries: {list(index.items())[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba2c6a1",
   "metadata": {},
   "source": [
    "### 4.2 Random Access Lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfe5a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_read(filename: str, index: dict, read_id: str) -> Optional[Tuple[str, str, str]]:\n",
    "    \"\"\"Look up a specific read by ID using the index.\"\"\"\n",
    "    if read_id not in index:\n",
    "        return None\n",
    "\n",
    "    with open(filename, 'r') as f:\n",
    "        f.seek(index[read_id])\n",
    "        header = f.readline().strip()\n",
    "        sequence = f.readline().strip()\n",
    "        f.readline()  # skip +\n",
    "        quality = f.readline().strip()\n",
    "        return (header, sequence, quality)\n",
    "\n",
    "# Demonstrate random access\n",
    "target_id = 'READ_0005000'\n",
    "start = time.perf_counter()\n",
    "record = lookup_read(sample_file, index, target_id)\n",
    "lookup_time = time.perf_counter() - start\n",
    "\n",
    "if record:\n",
    "    print(f\"Found {target_id} in {lookup_time*1000:.3f}ms\")\n",
    "    print(f\"  Sequence: {record[1][:50]}...\")\n",
    "    print(f\"  Quality:  {record[2][:50]}...\")\n",
    "\n",
    "# Compare with sequential scan\n",
    "start = time.perf_counter()\n",
    "for header, seq, qual in read_fastq_generator(sample_file):\n",
    "    if 'READ_0005000' in header:\n",
    "        break\n",
    "scan_time = time.perf_counter() - start\n",
    "print(f\"\\nSequential scan: {scan_time*1000:.3f}ms\")\n",
    "print(f\"Index speedup: {scan_time/lookup_time:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9048f1",
   "metadata": {},
   "source": [
    "### Exercise 4.1: Batch Random Access\n",
    "\n",
    "**Task:** Implement a function that retrieves multiple reads by ID using the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cd71ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_lookup(filename: str, index: dict,\n",
    "                 read_ids: list) -> List[Tuple[str, str, str]]:\n",
    "    \"\"\"Look up multiple reads by ID using the index.\n",
    "\n",
    "    TODO: Implement this function\n",
    "    - For each read_id in read_ids, use the index to find and read the record\n",
    "    - Optimization: sort lookups by file offset to minimize seeking\n",
    "    - Return a list of (header, sequence, quality) tuples\n",
    "    - Skip any read_ids not found in the index\n",
    "    \"\"\"\n",
    "    # TODO: Your implementation here\n",
    "    pass\n",
    "\n",
    "# Test:\n",
    "# target_ids = ['READ_0000001', 'READ_0005000', 'READ_0009999', 'READ_0003333']\n",
    "# results = batch_lookup(sample_file, index, target_ids)\n",
    "# for header, seq, qual in results:\n",
    "#     print(f\"  {header.split()[0]}: {seq[:30]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28706f56",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# SOLUTION - Exercise 4.1 (click to expand)\n",
    "# =============================================\n",
    "\n",
    "def batch_lookup(filename: str, index: dict,\n",
    "                 read_ids: list) -> List[Tuple[str, str, str]]:\n",
    "    \"\"\"Look up multiple reads by ID, optimized with sorted offsets.\"\"\"\n",
    "    # Filter to valid IDs and sort by file offset for sequential access\n",
    "    valid_ids = [(rid, index[rid]) for rid in read_ids if rid in index]\n",
    "    valid_ids.sort(key=lambda x: x[1])\n",
    "\n",
    "    results = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for read_id, offset in valid_ids:\n",
    "            f.seek(offset)\n",
    "            header = f.readline().strip()\n",
    "            sequence = f.readline().strip()\n",
    "            f.readline()  # skip +\n",
    "            quality = f.readline().strip()\n",
    "            results.append((header, sequence, quality))\n",
    "\n",
    "    return results\n",
    "\n",
    "target_ids = ['READ_0000001', 'READ_0005000', 'READ_0009999', 'READ_0003333']\n",
    "start = time.perf_counter()\n",
    "results = batch_lookup(sample_file, index, target_ids)\n",
    "elapsed = time.perf_counter() - start\n",
    "print(f\"Retrieved {len(results)} reads in {elapsed*1000:.2f}ms\")\n",
    "for header, seq, qual in results:\n",
    "    print(f\"  {header.split()[0]}: {seq[:40]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4386873",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Streaming Pipelines\n",
    "\n",
    "### The Power of Composable Generators\n",
    "\n",
    "Generators can be **chained together** to form processing pipelines. Each stage transforms the data stream without buffering the entire dataset. This is similar to Unix pipes (`cat file | grep | sort`).\n",
    "\n",
    "```\n",
    "[Read FASTQ] -> [Filter Quality] -> [Trim Adapters] -> [Calculate Stats]\n",
    "     ^               ^                    ^                    ^\n",
    "  generator       generator           generator            consumer\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5c3a05",
   "metadata": {},
   "source": [
    "### 5.1 Pipeline Building Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcd8d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_length(records, min_length: int = 50):\n",
    "    \"\"\"Filter reads shorter than min_length.\"\"\"\n",
    "    for header, sequence, quality in records:\n",
    "        if len(sequence) >= min_length:\n",
    "            yield (header, sequence, quality)\n",
    "\n",
    "def filter_by_quality(records, min_avg_quality: float = 20.0):\n",
    "    \"\"\"Filter reads below minimum average quality.\"\"\"\n",
    "    for header, sequence, quality in records:\n",
    "        avg_q = sum(ord(c) - 33 for c in quality) / len(quality)\n",
    "        if avg_q >= min_avg_quality:\n",
    "            yield (header, sequence, quality)\n",
    "\n",
    "def filter_no_ns(records, max_n_fraction: float = 0.05):\n",
    "    \"\"\"Filter reads with too many N bases.\"\"\"\n",
    "    for header, sequence, quality in records:\n",
    "        n_fraction = sequence.upper().count('N') / len(sequence)\n",
    "        if n_fraction <= max_n_fraction:\n",
    "            yield (header, sequence, quality)\n",
    "\n",
    "def trim_low_quality_ends(records, min_quality: int = 15):\n",
    "    \"\"\"Trim low-quality bases from the 3' end of reads.\"\"\"\n",
    "    for header, sequence, quality in records:\n",
    "        # Trim from the right while quality is below threshold\n",
    "        end = len(quality)\n",
    "        while end > 0 and (ord(quality[end-1]) - 33) < min_quality:\n",
    "            end -= 1\n",
    "        if end > 0:\n",
    "            yield (header, sequence[:end], quality[:end])\n",
    "\n",
    "def compute_stats(records) -> dict:\n",
    "    \"\"\"Terminal stage: consume the pipeline and compute statistics.\"\"\"\n",
    "    total = 0\n",
    "    total_length = 0\n",
    "    gc_sum = 0\n",
    "    qual_sum = 0\n",
    "    qual_bases = 0\n",
    "\n",
    "    for header, sequence, quality in records:\n",
    "        total += 1\n",
    "        total_length += len(sequence)\n",
    "        seq_upper = sequence.upper()\n",
    "        gc_sum += seq_upper.count('G') + seq_upper.count('C')\n",
    "        for c in quality:\n",
    "            qual_sum += ord(c) - 33\n",
    "            qual_bases += 1\n",
    "\n",
    "    return {\n",
    "        'total_reads': total,\n",
    "        'avg_length': total_length / total if total > 0 else 0,\n",
    "        'avg_gc': gc_sum / total_length if total_length > 0 else 0,\n",
    "        'avg_quality': qual_sum / qual_bases if qual_bases > 0 else 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1e3f8c",
   "metadata": {},
   "source": [
    "### 5.2 Composing the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7514178b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain generators into a pipeline — no intermediate files or lists!\n",
    "mixed_file = os.path.join(DATA_DIR, 'mixed_quality.fastq')\n",
    "\n",
    "# Build the pipeline (nothing executes yet — lazy evaluation!)\n",
    "pipeline = read_fastq_generator(mixed_file)\n",
    "pipeline = filter_by_quality(pipeline, min_avg_quality=20.0)\n",
    "pipeline = filter_no_ns(pipeline, max_n_fraction=0.05)\n",
    "pipeline = trim_low_quality_ends(pipeline, min_quality=15)\n",
    "\n",
    "# Only now does processing begin:\n",
    "start = time.perf_counter()\n",
    "stats = compute_stats(pipeline)\n",
    "elapsed = time.perf_counter() - start\n",
    "\n",
    "print(f\"Pipeline results ({elapsed:.3f}s):\")\n",
    "for key, value in stats.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a11378",
   "metadata": {},
   "source": [
    "### Exercise 5.1: Custom Pipeline Stage\n",
    "\n",
    "**Task:** Add a `subsample` stage that randomly keeps a fraction of reads (useful for quick previews of large datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4bffe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample(records, fraction: float = 0.1, seed: int = RANDOM_SEED):\n",
    "    \"\"\"Randomly subsample a fraction of reads.\n",
    "\n",
    "    TODO: Implement this function\n",
    "    - Use random.Random(seed) for reproducibility\n",
    "    - For each record, keep it with probability 'fraction'\n",
    "    - Yield kept records\n",
    "    \"\"\"\n",
    "    # TODO: Your implementation here\n",
    "    pass\n",
    "\n",
    "# Test: subsample 10% of reads and compute stats\n",
    "# pipeline = read_fastq_generator(sample_file)\n",
    "# pipeline = subsample(pipeline, fraction=0.1)\n",
    "# stats = compute_stats(pipeline)\n",
    "# print(f\"Subsampled reads: {stats['total_reads']:,}\")\n",
    "# print(f\"Expected ~{10000 * 0.1:.0f} reads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ccfb84",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# SOLUTION - Exercise 5.1 (click to expand)\n",
    "# =============================================\n",
    "\n",
    "def subsample(records, fraction: float = 0.1, seed: int = RANDOM_SEED):\n",
    "    \"\"\"Randomly subsample a fraction of reads.\"\"\"\n",
    "    rng = random.Random(seed)\n",
    "    for record in records:\n",
    "        if rng.random() < fraction:\n",
    "            yield record\n",
    "\n",
    "# Test\n",
    "pipeline = read_fastq_generator(sample_file)\n",
    "pipeline = subsample(pipeline, fraction=0.1)\n",
    "stats = compute_stats(pipeline)\n",
    "print(f\"Subsampled reads: {stats['total_reads']:,}\")\n",
    "print(f\"Expected ~{10000 * 0.1:.0f} reads\")\n",
    "print(f\"Avg quality: {stats['avg_quality']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484cb2fd",
   "metadata": {},
   "source": [
    "### Exercise 5.2: Pipeline Comparison\n",
    "\n",
    "**Task:** Build two different pipelines and compare their output — one strict, one lenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac87f2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline_comparison(filename: str) -> None:\n",
    "    \"\"\"Compare strict vs lenient filtering pipelines.\n",
    "\n",
    "    TODO: Implement this function\n",
    "    Strict pipeline: min_quality=30, max_n=0.01, min_length=100\n",
    "    Lenient pipeline: min_quality=15, max_n=0.10, min_length=50\n",
    "\n",
    "    Print stats for: raw input, strict output, lenient output\n",
    "    \"\"\"\n",
    "    # TODO: Your implementation here\n",
    "    pass\n",
    "\n",
    "# run_pipeline_comparison(mixed_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de5a54d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# SOLUTION - Exercise 5.2 (click to expand)\n",
    "# =============================================\n",
    "\n",
    "def run_pipeline_comparison(filename: str) -> None:\n",
    "    \"\"\"Compare strict vs lenient filtering pipelines.\"\"\"\n",
    "    # Raw stats\n",
    "    raw = compute_stats(read_fastq_generator(filename))\n",
    "\n",
    "    # Strict pipeline\n",
    "    strict = read_fastq_generator(filename)\n",
    "    strict = filter_by_quality(strict, min_avg_quality=30.0)\n",
    "    strict = filter_no_ns(strict, max_n_fraction=0.01)\n",
    "    strict = filter_by_length(strict, min_length=100)\n",
    "    strict_stats = compute_stats(strict)\n",
    "\n",
    "    # Lenient pipeline\n",
    "    lenient = read_fastq_generator(filename)\n",
    "    lenient = filter_by_quality(lenient, min_avg_quality=15.0)\n",
    "    lenient = filter_no_ns(lenient, max_n_fraction=0.10)\n",
    "    lenient = filter_by_length(lenient, min_length=50)\n",
    "    lenient_stats = compute_stats(lenient)\n",
    "\n",
    "    print(f\"{'Metric':<20} {'Raw':>10} {'Strict':>10} {'Lenient':>10}\")\n",
    "    print(\"-\" * 52)\n",
    "    print(f\"{'Total reads':<20} {raw['total_reads']:>10,} {strict_stats['total_reads']:>10,} {lenient_stats['total_reads']:>10,}\")\n",
    "    print(f\"{'Avg length':<20} {raw['avg_length']:>10.1f} {strict_stats['avg_length']:>10.1f} {lenient_stats['avg_length']:>10.1f}\")\n",
    "    print(f\"{'Avg GC':<20} {raw['avg_gc']:>10.4f} {strict_stats['avg_gc']:>10.4f} {lenient_stats['avg_gc']:>10.4f}\")\n",
    "    print(f\"{'Avg quality':<20} {raw['avg_quality']:>10.2f} {strict_stats['avg_quality']:>10.2f} {lenient_stats['avg_quality']:>10.2f}\")\n",
    "\n",
    "run_pipeline_comparison(mixed_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719a45f6",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: The MapReduce Pattern\n",
    "\n",
    "### What is MapReduce?\n",
    "\n",
    "MapReduce is a programming model for processing large datasets in parallel. It was popularized by Google and is the foundation of Hadoop and Spark. The pattern has four distinct phases:\n",
    "\n",
    "```\n",
    "                     MAP PHASE                  SHUFFLE PHASE              REDUCE PHASE\n",
    "                  +-------------+            +-----------------+        +---------------+\n",
    "  Partition 1 --> | map_func(r) | --\\       | Group by key:   |    /-> | reduce_func() | --> Result A\n",
    "  Partition 2 --> | map_func(r) | ---+----> | key_A: [v1, v3] | --+--> | reduce_func() | --> Result B\n",
    "  Partition 3 --> | map_func(r) | --/       | key_B: [v2, v4] |    \\-> | reduce_func() | --> Result C\n",
    "                  +-------------+            +-----------------+        +---------------+\n",
    "```\n",
    "\n",
    "**Key distinction from simple parallel map:** The **shuffle phase** groups intermediate results by key across all partitions. This is what enables aggregation (counting, averaging, etc.) over the full dataset.\n",
    "\n",
    "### The Phases\n",
    "\n",
    "1. **SPLIT**: Divide input into partitions\n",
    "2. **MAP**: Apply `map_func` to each record, emitting `(key, value)` pairs\n",
    "3. **SHUFFLE**: Group all values by key across all partitions\n",
    "4. **REDUCE**: Apply `reduce_func` to each `(key, [values])` group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4bc60e",
   "metadata": {},
   "source": [
    "### 6.1 MapReduce Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d8da98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapreduce_fastq(filename: str,\n",
    "                   map_func,\n",
    "                   reduce_func,\n",
    "                   num_partitions: int = 4) -> dict:\n",
    "    \"\"\"\n",
    "    MapReduce implementation for FASTQ files.\n",
    "\n",
    "    Args:\n",
    "        filename: Input FASTQ file\n",
    "        map_func: Function(record) -> list of (key, value) pairs\n",
    "        reduce_func: Function(key, values_list) -> result\n",
    "        num_partitions: Number of partitions to split input into\n",
    "\n",
    "    Returns:\n",
    "        Dict mapping each key to its reduced result\n",
    "    \"\"\"\n",
    "    # PHASE 1: SPLIT - divide records into partitions\n",
    "    partitions = [[] for _ in range(num_partitions)]\n",
    "    for i, record in enumerate(read_fastq_generator(filename)):\n",
    "        partitions[i % num_partitions].append(record)\n",
    "\n",
    "    print(f\"SPLIT: {sum(len(p) for p in partitions)} records -> {num_partitions} partitions\")\n",
    "    for i, p in enumerate(partitions):\n",
    "        print(f\"  Partition {i}: {len(p)} records\")\n",
    "\n",
    "    # PHASE 2: MAP - apply map_func to each record, collect (key, value) pairs\n",
    "    all_pairs = []\n",
    "    for partition_idx, partition in enumerate(partitions):\n",
    "        partition_pairs = []\n",
    "        for record in partition:\n",
    "            pairs = map_func(record)\n",
    "            partition_pairs.extend(pairs)\n",
    "        all_pairs.extend(partition_pairs)\n",
    "        print(f\"MAP partition {partition_idx}: emitted {len(partition_pairs)} (key, value) pairs\")\n",
    "\n",
    "    # PHASE 3: SHUFFLE - group values by key\n",
    "    grouped = defaultdict(list)\n",
    "    for key, value in all_pairs:\n",
    "        grouped[key].append(value)\n",
    "\n",
    "    print(f\"SHUFFLE: {len(all_pairs)} pairs -> {len(grouped)} unique keys\")\n",
    "\n",
    "    # PHASE 4: REDUCE - apply reduce_func to each group\n",
    "    results = {}\n",
    "    for key, values in grouped.items():\n",
    "        results[key] = reduce_func(key, values)\n",
    "\n",
    "    print(f\"REDUCE: {len(grouped)} keys -> {len(results)} results\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2eb84ab",
   "metadata": {},
   "source": [
    "### 6.2 Example: Quality Score Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f80b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAP function: classify each read by quality tier\n",
    "def map_quality_tier(record):\n",
    "    \"\"\"Map a read to a quality tier.\"\"\"\n",
    "    header, sequence, quality = record\n",
    "    avg_q = sum(ord(c) - 33 for c in quality) / len(quality)\n",
    "\n",
    "    if avg_q >= 30:\n",
    "        tier = 'high'\n",
    "    elif avg_q >= 20:\n",
    "        tier = 'medium'\n",
    "    else:\n",
    "        tier = 'low'\n",
    "\n",
    "    return [(tier, avg_q)]  # emit (tier, actual_quality)\n",
    "\n",
    "# REDUCE function: compute statistics for each tier\n",
    "def reduce_quality_stats(key, values):\n",
    "    \"\"\"Reduce quality values to summary statistics.\"\"\"\n",
    "    return {\n",
    "        'count': len(values),\n",
    "        'avg_quality': sum(values) / len(values),\n",
    "        'min_quality': min(values),\n",
    "        'max_quality': max(values)\n",
    "    }\n",
    "\n",
    "# Run MapReduce\n",
    "print(\"MapReduce: Quality Tier Analysis\")\n",
    "print(\"=\" * 50)\n",
    "results = mapreduce_fastq(sample_file, map_quality_tier, reduce_quality_stats)\n",
    "print(\"\\nResults:\")\n",
    "for tier in ['high', 'medium', 'low']:\n",
    "    if tier in results:\n",
    "        r = results[tier]\n",
    "        print(f\"  {tier:>6}: {r['count']:,} reads, \"\n",
    "              f\"avg Q={r['avg_quality']:.1f}, \"\n",
    "              f\"range [{r['min_quality']:.1f}, {r['max_quality']:.1f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ca5a5b",
   "metadata": {},
   "source": [
    "### Exercise 6.1: MapReduce - Base Composition\n",
    "\n",
    "**Task:** Use MapReduce to count the occurrence of each base (A, C, G, T, N) across all reads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81fcd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_base_counts(record):\n",
    "    \"\"\"Map function: emit (base, 1) for each base in the read.\n",
    "\n",
    "    TODO: Implement this function\n",
    "    - For each character in the sequence, emit (base, 1)\n",
    "    - This is the classic word-count pattern\n",
    "    \"\"\"\n",
    "    # TODO: Your implementation here\n",
    "    pass\n",
    "\n",
    "def reduce_sum(key, values):\n",
    "    \"\"\"Reduce function: sum all values for a key.\n",
    "\n",
    "    TODO: Implement this function\n",
    "    \"\"\"\n",
    "    # TODO: Your implementation here\n",
    "    pass\n",
    "\n",
    "# Test:\n",
    "# results = mapreduce_fastq(sample_file, map_base_counts, reduce_sum)\n",
    "# total = sum(results.values())\n",
    "# print(\"\\nBase composition:\")\n",
    "# for base in sorted(results.keys()):\n",
    "#     pct = results[base] / total * 100\n",
    "#     print(f\"  {base}: {results[base]:>10,} ({pct:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938d6443",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# SOLUTION - Exercise 6.1 (click to expand)\n",
    "# =============================================\n",
    "\n",
    "def map_base_counts(record):\n",
    "    \"\"\"Map function: emit (base, 1) for each base in the read.\"\"\"\n",
    "    header, sequence, quality = record\n",
    "    return [(base, 1) for base in sequence.upper()]\n",
    "\n",
    "def reduce_sum(key, values):\n",
    "    \"\"\"Reduce function: sum all values.\"\"\"\n",
    "    return sum(values)\n",
    "\n",
    "results = mapreduce_fastq(sample_file, map_base_counts, reduce_sum)\n",
    "total = sum(results.values())\n",
    "print(\"\\nBase composition:\")\n",
    "for base in sorted(results.keys()):\n",
    "    pct = results[base] / total * 100\n",
    "    print(f\"  {base}: {results[base]:>10,} ({pct:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51226107",
   "metadata": {},
   "source": [
    "### Exercise 6.2: MapReduce - Quality by Position\n",
    "\n",
    "**Task:** Use MapReduce to compute average quality at each read position (reveals the Illumina quality decay curve)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed2e9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_position_quality(record):\n",
    "    \"\"\"Map function: emit (position, quality_score) for each position.\n",
    "\n",
    "    TODO: Implement this function\n",
    "    - For each position in the quality string, emit (position, phred_score)\n",
    "    \"\"\"\n",
    "    # TODO: Your implementation here\n",
    "    pass\n",
    "\n",
    "def reduce_average(key, values):\n",
    "    \"\"\"Reduce function: compute the average of values.\n",
    "\n",
    "    TODO: Implement this function\n",
    "    \"\"\"\n",
    "    # TODO: Your implementation here\n",
    "    pass\n",
    "\n",
    "# Test:\n",
    "# results = mapreduce_fastq(sample_file, map_position_quality, reduce_average)\n",
    "# print(\"\\nAvg quality by position (first 20, last 20):\")\n",
    "# positions = sorted(results.keys())\n",
    "# for pos in positions[:20]:\n",
    "#     print(f\"  Position {pos:3d}: {results[pos]:.1f}\")\n",
    "# print(\"  ...\")\n",
    "# for pos in positions[-20:]:\n",
    "#     print(f\"  Position {pos:3d}: {results[pos]:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414d3093",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# SOLUTION - Exercise 6.2 (click to expand)\n",
    "# =============================================\n",
    "\n",
    "def map_position_quality(record):\n",
    "    \"\"\"Map function: emit (position, quality_score) for each position.\"\"\"\n",
    "    header, sequence, quality = record\n",
    "    return [(pos, ord(c) - 33) for pos, c in enumerate(quality)]\n",
    "\n",
    "def reduce_average(key, values):\n",
    "    \"\"\"Reduce function: compute the average of values.\"\"\"\n",
    "    return sum(values) / len(values)\n",
    "\n",
    "results = mapreduce_fastq(sample_file, map_position_quality, reduce_average)\n",
    "\n",
    "# Display results\n",
    "positions = sorted(results.keys())\n",
    "print(\"Avg quality by position (first 10, last 10):\")\n",
    "for pos in positions[:10]:\n",
    "    bar = '#' * int(results[pos])\n",
    "    print(f\"  Position {pos:3d}: {results[pos]:5.1f} {bar}\")\n",
    "print(\"  ...\")\n",
    "for pos in positions[-10:]:\n",
    "    bar = '#' * int(results[pos])\n",
    "    print(f\"  Position {pos:3d}: {results[pos]:5.1f} {bar}\")\n",
    "\n",
    "print(\"\\nNotice how quality drops toward the end of the read —\")\n",
    "print(\"this is the characteristic Illumina quality decay curve!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3483e7b6",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 7: Bioinformatics Tool Integration\n",
    "\n",
    "### Why Use External Tools?\n",
    "\n",
    "While Python is great for custom analysis, specialized tools like **seqkit** and **fastp** are:\n",
    "- Written in C/Go for maximum performance (10-100x faster)\n",
    "- Battle-tested on billions of reads\n",
    "- Industry standard in bioinformatics pipelines\n",
    "\n",
    "This section teaches you to call these tools from Python and parse their output.\n",
    "\n",
    "### Installing seqkit and fastp\n",
    "\n",
    "These tools are **optional** — the notebook detects whether they are installed and skips gracefully if not. But installing them lets you complete all Part 7 exercises.\n",
    "\n",
    "**With Conda (easiest):**\n",
    "```bash\n",
    "conda install -c bioconda seqkit fastp\n",
    "```\n",
    "\n",
    "**With Homebrew (macOS):**\n",
    "```bash\n",
    "brew install seqkit\n",
    "brew install fastp\n",
    "```\n",
    "\n",
    "**From pre-built binaries (any platform):**\n",
    "- **seqkit:** Download from https://bioinf.shenwei.me/seqkit/download/ — extract and place on your `PATH`.\n",
    "- **fastp:** Download from https://github.com/OpenGene/fastp/releases — extract, `chmod +x fastp`, and move to your `PATH`.\n",
    "\n",
    "Verify with:\n",
    "```bash\n",
    "seqkit version\n",
    "fastp --version\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f595da",
   "metadata": {},
   "source": [
    "### 7.1 SeqKit Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d65b0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqKitHelper:\n",
    "    \"\"\"Wrapper for seqkit command-line tool.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_available():\n",
    "        if not shutil.which('seqkit'):\n",
    "            raise EnvironmentError(\n",
    "                \"seqkit not found. Install with: conda install -c bioconda seqkit\\n\"\n",
    "                \"Or skip this section — it's optional.\"\n",
    "            )\n",
    "\n",
    "    @staticmethod\n",
    "    def get_stats(fastq_file: str):\n",
    "        \"\"\"Get basic statistics using seqkit stats.\"\"\"\n",
    "        SeqKitHelper._check_available()\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                ['seqkit', 'stats', fastq_file, '-T'],\n",
    "                capture_output=True, text=True, check=True\n",
    "            )\n",
    "            import pandas as pd\n",
    "            return pd.read_csv(StringIO(result.stdout), sep='\\t')\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            raise RuntimeError(f\"seqkit stats failed: {e.stderr}\") from e\n",
    "\n",
    "    @staticmethod\n",
    "    def grep_by_pattern(fastq_file: str, pattern: str, output_file: str) -> int:\n",
    "        \"\"\"Extract reads matching a sequence pattern.\"\"\"\n",
    "        SeqKitHelper._check_available()\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                ['seqkit', 'grep', '-s', '-r', '-p', pattern,\n",
    "                 fastq_file, '-o', output_file],\n",
    "                capture_output=True, text=True, check=True\n",
    "            )\n",
    "            # Count output reads\n",
    "            if os.path.exists(output_file):\n",
    "                return file_line_count(output_file) // 4\n",
    "            return 0\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            raise RuntimeError(f\"seqkit grep failed: {e.stderr}\") from e\n",
    "\n",
    "# Try seqkit if available\n",
    "if check_tool('seqkit'):\n",
    "    print(\"seqkit is available! Running stats...\")\n",
    "    stats_df = SeqKitHelper.get_stats(sample_file)\n",
    "    print(stats_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"seqkit not found — skipping. Install with:\")\n",
    "    print(\"  conda install -c bioconda seqkit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e373b77d",
   "metadata": {},
   "source": [
    "### 7.2 Fastp Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521e39bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastpHelper:\n",
    "    \"\"\"Wrapper for fastp quality control tool.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_available():\n",
    "        if not shutil.which('fastp'):\n",
    "            raise EnvironmentError(\n",
    "                \"fastp not found. Install with: conda install -c bioconda fastp\\n\"\n",
    "                \"Or skip this section — it's optional.\"\n",
    "            )\n",
    "\n",
    "    @staticmethod\n",
    "    def run_qc(input_file: str,\n",
    "               output_file: str,\n",
    "               html_report: str = None,\n",
    "               json_report: str = None,\n",
    "               min_quality: int = 20,\n",
    "               min_length: int = 50) -> Optional[dict]:\n",
    "        \"\"\"Run fastp quality control.\n",
    "\n",
    "        Returns parsed JSON report if json_report path is provided.\n",
    "        \"\"\"\n",
    "        FastpHelper._check_available()\n",
    "\n",
    "        cmd = [\n",
    "            'fastp',\n",
    "            '-i', input_file,\n",
    "            '-o', output_file,\n",
    "            '-q', str(min_quality),\n",
    "            '-l', str(min_length),\n",
    "        ]\n",
    "\n",
    "        if html_report:\n",
    "            cmd.extend(['-h', html_report])\n",
    "        else:\n",
    "            cmd.extend(['-h', '/dev/null'])\n",
    "\n",
    "        if json_report:\n",
    "            cmd.extend(['-j', json_report])\n",
    "        else:\n",
    "            cmd.extend(['-j', '/dev/null'])\n",
    "\n",
    "        try:\n",
    "            subprocess.run(cmd, capture_output=True, text=True, check=True)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            raise RuntimeError(f\"fastp failed: {e.stderr}\") from e\n",
    "\n",
    "        if json_report and os.path.exists(json_report):\n",
    "            import json as json_module\n",
    "            with open(json_report) as f:\n",
    "                return json_module.load(f)\n",
    "        return None\n",
    "\n",
    "# Try fastp if available\n",
    "if check_tool('fastp'):\n",
    "    print(\"fastp is available! Running QC...\")\n",
    "    report = FastpHelper.run_qc(\n",
    "        sample_file,\n",
    "        os.path.join(DATA_DIR, 'fastp_filtered.fastq'),\n",
    "        json_report=os.path.join(DATA_DIR, 'fastp_report.json')\n",
    "    )\n",
    "    if report:\n",
    "        summary = report.get('summary', {})\n",
    "        before = summary.get('before_filtering', {})\n",
    "        after = summary.get('after_filtering', {})\n",
    "        print(f\"  Before: {before.get('total_reads', 'N/A'):,} reads\")\n",
    "        print(f\"  After:  {after.get('total_reads', 'N/A'):,} reads\")\n",
    "else:\n",
    "    print(\"fastp not found — skipping. Install with:\")\n",
    "    print(\"  conda install -c bioconda fastp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467096d2",
   "metadata": {},
   "source": [
    "### Exercise 7.1: Python vs. Tool Benchmark\n",
    "\n",
    "**Task:** Compare the speed of your Python quality filter (from Part 2) with fastp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafbdba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_python_vs_tools(filename: str) -> None:\n",
    "    \"\"\"Compare Python filtering speed with external tools.\n",
    "\n",
    "    TODO: Implement this function\n",
    "    - Time your Python filter_by_quality_chunked() on the large file\n",
    "    - If fastp is available, time fastp on the same file\n",
    "    - Print comparison results\n",
    "    - If fastp is not available, print a message and only show Python results\n",
    "    \"\"\"\n",
    "    # TODO: Your implementation here\n",
    "    pass\n",
    "\n",
    "# benchmark_python_vs_tools(large_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bf8d75",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# SOLUTION - Exercise 7.1 (click to expand)\n",
    "# =============================================\n",
    "\n",
    "def benchmark_python_vs_tools(filename: str) -> None:\n",
    "    \"\"\"Compare Python filtering speed with external tools.\"\"\"\n",
    "    # Python approach\n",
    "    start = time.perf_counter()\n",
    "    py_stats = filter_by_quality_chunked(\n",
    "        filename,\n",
    "        os.path.join(DATA_DIR, 'benchmark_py_filtered.fastq'),\n",
    "        min_avg_quality=20.0,\n",
    "        chunk_size=5000\n",
    "    )\n",
    "    py_time = time.perf_counter() - start\n",
    "\n",
    "    print(f\"\\nPython: {py_time:.3f}s ({py_stats['passed']:,} reads passed)\")\n",
    "\n",
    "    # fastp approach (if available)\n",
    "    if check_tool('fastp'):\n",
    "        start = time.perf_counter()\n",
    "        FastpHelper.run_qc(\n",
    "            filename,\n",
    "            os.path.join(DATA_DIR, 'benchmark_fastp_filtered.fastq'),\n",
    "            min_quality=20\n",
    "        )\n",
    "        fastp_time = time.perf_counter() - start\n",
    "        fastp_reads = file_line_count(os.path.join(DATA_DIR, 'benchmark_fastp_filtered.fastq')) // 4\n",
    "        print(f\"fastp:  {fastp_time:.3f}s ({fastp_reads:,} reads passed)\")\n",
    "        print(f\"fastp speedup: {py_time/fastp_time:.1f}x\")\n",
    "    else:\n",
    "        print(\"fastp not available — install to compare performance\")\n",
    "\n",
    "    # Cleanup\n",
    "    for f in ['benchmark_py_filtered.fastq', 'benchmark_fastp_filtered.fastq']:\n",
    "        fpath = os.path.join(DATA_DIR, f)\n",
    "        if os.path.exists(fpath):\n",
    "            os.remove(fpath)\n",
    "\n",
    "benchmark_python_vs_tools(large_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4600680b",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 8: Profiling and Optimization\n",
    "\n",
    "### The Golden Rule of Optimization\n",
    "\n",
    "> \"Premature optimization is the root of all evil.\" — Donald Knuth\n",
    "\n",
    "**Always profile before optimizing.** Measure where time is actually spent, then optimize the bottleneck. Optimizing the wrong thing wastes effort.\n",
    "\n",
    "### Profiling Tools\n",
    "- `time.perf_counter()` — wall-clock timing\n",
    "- `sys.getsizeof()` — object memory size\n",
    "- `cProfile` — function-level profiling (built into Python)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd72ee5b",
   "metadata": {},
   "source": [
    "### 8.1 Timing Decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85837407",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def timed(func):\n",
    "    \"\"\"Decorator that prints execution time.\"\"\"\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.perf_counter()\n",
    "        result = func(*args, **kwargs)\n",
    "        elapsed = time.perf_counter() - start\n",
    "        print(f\"  {func.__name__}: {elapsed:.4f}s\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "# Example usage\n",
    "@timed\n",
    "def process_reads_v1(filename):\n",
    "    \"\"\"Version 1: String concatenation.\"\"\"\n",
    "    total_gc = 0\n",
    "    count = 0\n",
    "    for header, seq, qual in read_fastq_generator(filename):\n",
    "        gc = seq.count('G') + seq.count('C')\n",
    "        total_gc += gc / len(seq)\n",
    "        count += 1\n",
    "    return total_gc / count\n",
    "\n",
    "@timed\n",
    "def process_reads_v2(filename):\n",
    "    \"\"\"Version 2: Using translate for counting.\"\"\"\n",
    "    total_gc = 0\n",
    "    count = 0\n",
    "    gc_set = set('GCgc')\n",
    "    for header, seq, qual in read_fastq_generator(filename):\n",
    "        gc = sum(1 for c in seq if c in gc_set)\n",
    "        total_gc += gc / len(seq)\n",
    "        count += 1\n",
    "    return total_gc / count\n",
    "\n",
    "print(\"Benchmarking GC calculation methods:\")\n",
    "r1 = process_reads_v1(large_file)\n",
    "r2 = process_reads_v2(large_file)\n",
    "print(f\"  Results match: {abs(r1 - r2) < 0.0001}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f930f0cb",
   "metadata": {},
   "source": [
    "### 8.2 Memory Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0230f98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_memory_approaches(filename: str) -> None:\n",
    "    \"\"\"Compare memory usage of different approaches.\"\"\"\n",
    "    import gc as gc_mod\n",
    "\n",
    "    # Approach 1: Load all into list\n",
    "    gc_mod.collect()\n",
    "    records = read_fastq_list(filename)\n",
    "    list_mem = sys.getsizeof(records)\n",
    "    # Deeper measurement\n",
    "    deep_mem = list_mem + sum(\n",
    "        sys.getsizeof(r) + sum(sys.getsizeof(s) for s in r)\n",
    "        for r in records\n",
    "    )\n",
    "    del records\n",
    "    gc_mod.collect()\n",
    "\n",
    "    # Approach 2: Generator (just the generator object)\n",
    "    gen = read_fastq_generator(filename)\n",
    "    gen_mem = sys.getsizeof(gen)\n",
    "\n",
    "    # Approach 3: Chunks of 1000\n",
    "    chunk_gen = read_fastq_chunks(filename, chunk_size=1000)\n",
    "    first_chunk = next(chunk_gen)\n",
    "    chunk_mem = sys.getsizeof(first_chunk) + sum(\n",
    "        sys.getsizeof(r) + sum(sys.getsizeof(s) for s in r)\n",
    "        for r in first_chunk\n",
    "    )\n",
    "\n",
    "    print(\"Memory comparison (sample.fastq):\")\n",
    "    print(f\"  Full list:    {deep_mem/1024:>10.1f} KB\")\n",
    "    print(f\"  Generator:    {gen_mem:>10d} bytes\")\n",
    "    print(f\"  Chunk (1000): {chunk_mem/1024:>10.1f} KB\")\n",
    "    print(f\"  List/Generator ratio: {deep_mem/gen_mem:,.0f}x\")\n",
    "    print(f\"  List/Chunk ratio:     {deep_mem/chunk_mem:,.1f}x\")\n",
    "\n",
    "measure_memory_approaches(sample_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34754ab",
   "metadata": {},
   "source": [
    "### Exercise 8.1: Profile and Optimize\n",
    "\n",
    "**Task:** Profile the quality filtering pipeline, identify the bottleneck, and optimize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ca5a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_pipeline(filename: str) -> None:\n",
    "    \"\"\"Profile each stage of a filtering pipeline.\n",
    "\n",
    "    TODO: Implement this function\n",
    "    - Create a pipeline with: read -> filter_quality -> filter_ns -> trim -> stats\n",
    "    - Time each stage individually to find the bottleneck\n",
    "    - Hint: Run each stage separately on the same data to measure its cost\n",
    "\n",
    "    Expected output format:\n",
    "      Stage timings:\n",
    "        read_fastq_generator: X.XXXXs\n",
    "        filter_by_quality:    X.XXXXs\n",
    "        filter_no_ns:         X.XXXXs\n",
    "        trim_low_quality:     X.XXXXs\n",
    "      Bottleneck: <stage_name>\n",
    "    \"\"\"\n",
    "    # TODO: Your implementation here\n",
    "    pass\n",
    "\n",
    "# profile_pipeline(large_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2f970c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# SOLUTION - Exercise 8.1 (click to expand)\n",
    "# =============================================\n",
    "\n",
    "def profile_pipeline(filename: str) -> None:\n",
    "    \"\"\"Profile each stage of a filtering pipeline.\"\"\"\n",
    "    # First, load all records so we can test each stage independently\n",
    "    records = read_fastq_list(filename)\n",
    "\n",
    "    timings = {}\n",
    "\n",
    "    # Time: reading\n",
    "    start = time.perf_counter()\n",
    "    _ = read_fastq_list(filename)\n",
    "    timings['read_fastq'] = time.perf_counter() - start\n",
    "\n",
    "    # Time: quality filter\n",
    "    start = time.perf_counter()\n",
    "    _ = list(filter_by_quality(iter(records), min_avg_quality=20.0))\n",
    "    timings['filter_quality'] = time.perf_counter() - start\n",
    "\n",
    "    # Time: N filter\n",
    "    start = time.perf_counter()\n",
    "    _ = list(filter_no_ns(iter(records), max_n_fraction=0.05))\n",
    "    timings['filter_no_ns'] = time.perf_counter() - start\n",
    "\n",
    "    # Time: trimming\n",
    "    start = time.perf_counter()\n",
    "    _ = list(trim_low_quality_ends(iter(records), min_quality=15))\n",
    "    timings['trim_quality'] = time.perf_counter() - start\n",
    "\n",
    "    # Time: stats computation\n",
    "    start = time.perf_counter()\n",
    "    _ = compute_stats(iter(records))\n",
    "    timings['compute_stats'] = time.perf_counter() - start\n",
    "\n",
    "    # Report\n",
    "    print(\"Stage timings:\")\n",
    "    bottleneck = max(timings, key=timings.get)\n",
    "    for stage, t in timings.items():\n",
    "        marker = \" <-- bottleneck\" if stage == bottleneck else \"\"\n",
    "        print(f\"  {stage:<20} {t:.4f}s{marker}\")\n",
    "\n",
    "    total = sum(timings.values())\n",
    "    print(f\"\\n  Total: {total:.4f}s\")\n",
    "    print(f\"  Bottleneck '{bottleneck}' is {timings[bottleneck]/total*100:.1f}% of total time\")\n",
    "\n",
    "profile_pipeline(large_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c825b024",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 9: Paired-End Read Processing\n",
    "\n",
    "### What are Paired-End Reads?\n",
    "\n",
    "In Illumina sequencing, each DNA fragment is read from both ends:\n",
    "- **R1** (Read 1): Forward read from the 5' end\n",
    "- **R2** (Read 2): Reverse complement from the 3' end\n",
    "\n",
    "```\n",
    "Fragment:  5'---[====R1====>............<====R2====]---3'\n",
    "                |<------------ insert size ----------->|\n",
    "```\n",
    "\n",
    "Paired-end reads must be processed **in sync** — if you filter out a read from R1, you must also remove its mate from R2. Losing sync corrupts downstream analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1d092b",
   "metadata": {},
   "source": [
    "### 9.1 Synchronized Paired-End Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15139a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_paired_fastq(r1_file: str,\n",
    "                     r2_file: str) -> Generator[Tuple[Tuple, Tuple], None, None]:\n",
    "    \"\"\"Read paired-end FASTQ files in sync.\n",
    "\n",
    "    Yields tuples of ((r1_header, r1_seq, r1_qual), (r2_header, r2_seq, r2_qual)).\n",
    "    Validates that read IDs match between R1 and R2.\n",
    "    \"\"\"\n",
    "    r1_gen = read_fastq_generator(r1_file)\n",
    "    r2_gen = read_fastq_generator(r2_file)\n",
    "\n",
    "    for r1_record, r2_record in zip(r1_gen, r2_gen):\n",
    "        # Verify paired reads match (compare base read ID)\n",
    "        r1_id = r1_record[0].split()[0]\n",
    "        r2_id = r2_record[0].split()[0]\n",
    "        # Strip /1 and /2 suffixes if present\n",
    "        if r1_id.endswith('/1'):\n",
    "            r1_id = r1_id[:-2]\n",
    "        if r2_id.endswith('/2'):\n",
    "            r2_id = r2_id[:-2]\n",
    "        if r1_id != r2_id:\n",
    "            raise ValueError(f\"Paired read mismatch: {r1_id} vs {r2_id}\")\n",
    "        yield (r1_record, r2_record)\n",
    "\n",
    "# Test synchronized reading\n",
    "r1_file = os.path.join(DATA_DIR, 'sample_R1.fastq')\n",
    "r2_file = os.path.join(DATA_DIR, 'sample_R2.fastq')\n",
    "\n",
    "count = 0\n",
    "for r1, r2 in read_paired_fastq(r1_file, r2_file):\n",
    "    count += 1\n",
    "    if count <= 3:\n",
    "        print(f\"Pair {count}:\")\n",
    "        print(f\"  R1: {r1[0].split()[0]} -> {r1[1][:30]}...\")\n",
    "        print(f\"  R2: {r2[0].split()[0]} -> {r2[1][:30]}...\")\n",
    "print(f\"\\nTotal read pairs: {count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318d122d",
   "metadata": {},
   "source": [
    "### 9.2 Paired-End Quality Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f375d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_paired_reads(r1_file: str, r2_file: str,\n",
    "                      out_r1: str, out_r2: str,\n",
    "                      min_avg_quality: float = 20.0) -> dict:\n",
    "    \"\"\"Filter paired-end reads — both mates must pass.\n",
    "\n",
    "    If either R1 or R2 fails the quality check, BOTH are discarded.\n",
    "    This maintains synchronization between the files.\n",
    "    \"\"\"\n",
    "    stats = {'total_pairs': 0, 'passed_pairs': 0, 'failed_pairs': 0}\n",
    "\n",
    "    with open(out_r1, 'w') as f1, open(out_r2, 'w') as f2:\n",
    "        for r1, r2 in read_paired_fastq(r1_file, r2_file):\n",
    "            stats['total_pairs'] += 1\n",
    "\n",
    "            # Both reads must pass\n",
    "            r1_qual = sum(ord(c) - 33 for c in r1[2]) / len(r1[2])\n",
    "            r2_qual = sum(ord(c) - 33 for c in r2[2]) / len(r2[2])\n",
    "\n",
    "            if r1_qual >= min_avg_quality and r2_qual >= min_avg_quality:\n",
    "                stats['passed_pairs'] += 1\n",
    "                for rec, fh in [(r1, f1), (r2, f2)]:\n",
    "                    fh.write(f\"{rec[0]}\\n{rec[1]}\\n+\\n{rec[2]}\\n\")\n",
    "            else:\n",
    "                stats['failed_pairs'] += 1\n",
    "\n",
    "    return stats\n",
    "\n",
    "# Test\n",
    "pe_stats = filter_paired_reads(\n",
    "    r1_file, r2_file,\n",
    "    os.path.join(DATA_DIR, 'filtered_R1.fastq'),\n",
    "    os.path.join(DATA_DIR, 'filtered_R2.fastq'),\n",
    "    min_avg_quality=25.0\n",
    ")\n",
    "print(f\"Paired-end filtering:\")\n",
    "print(f\"  Total pairs:  {pe_stats['total_pairs']:,}\")\n",
    "print(f\"  Passed pairs: {pe_stats['passed_pairs']:,}\")\n",
    "print(f\"  Failed pairs: {pe_stats['failed_pairs']:,}\")\n",
    "print(f\"  Pass rate:    {100*pe_stats['passed_pairs']/pe_stats['total_pairs']:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadf93aa",
   "metadata": {},
   "source": [
    "### Exercise 9.1: Paired-End Insert Size Estimation\n",
    "\n",
    "**Task:** Estimate insert sizes by finding overlapping regions between R1 and R2 reads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0147c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_insert_sizes(r1_file: str, r2_file: str,\n",
    "                         num_samples: int = 1000) -> List[int]:\n",
    "    \"\"\"Estimate insert sizes from paired-end reads.\n",
    "\n",
    "    TODO: Implement this function\n",
    "    - For the first num_samples read pairs:\n",
    "    - Reverse complement the R2 read\n",
    "    - Find the overlap between R1 and reverse-complemented R2\n",
    "    - Insert size = len(R1) + len(R2) - overlap_length\n",
    "    - Return list of estimated insert sizes\n",
    "\n",
    "    Hint: Try matching the last k bases of R1 with the first k bases of RC(R2),\n",
    "    starting from k=20 up to k=read_length, and take the first good match.\n",
    "    Or simply use the known read_length and the data generation parameters.\n",
    "    \"\"\"\n",
    "    # TODO: Your implementation here\n",
    "    pass\n",
    "\n",
    "# Test:\n",
    "# insert_sizes = estimate_insert_sizes(r1_file, r2_file)\n",
    "# if insert_sizes:\n",
    "#     avg_insert = sum(insert_sizes) / len(insert_sizes)\n",
    "#     print(f\"Estimated mean insert size: {avg_insert:.0f}\")\n",
    "#     print(f\"Insert size range: {min(insert_sizes)}-{max(insert_sizes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4decae",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# SOLUTION - Exercise 9.1 (click to expand)\n",
    "# =============================================\n",
    "\n",
    "def reverse_complement(seq: str) -> str:\n",
    "    \"\"\"Return the reverse complement of a DNA sequence.\"\"\"\n",
    "    comp = {'A': 'T', 'T': 'A', 'C': 'G', 'G': 'C', 'N': 'N'}\n",
    "    return ''.join(comp.get(b, 'N') for b in reversed(seq))\n",
    "\n",
    "def estimate_insert_sizes(r1_file: str, r2_file: str,\n",
    "                         num_samples: int = 1000) -> List[int]:\n",
    "    \"\"\"Estimate insert sizes from paired-end reads.\n",
    "\n",
    "    Since our simulated data has a known structure (R2 is reverse complement\n",
    "    of the fragment end), we can search for overlaps to estimate insert size.\n",
    "    \"\"\"\n",
    "    insert_sizes = []\n",
    "    min_overlap = 10\n",
    "\n",
    "    for i, (r1, r2) in enumerate(read_paired_fastq(r1_file, r2_file)):\n",
    "        if i >= num_samples:\n",
    "            break\n",
    "\n",
    "        r1_seq = r1[1]\n",
    "        r2_rc = reverse_complement(r2[1])\n",
    "        read_len = len(r1_seq)\n",
    "\n",
    "        # Try to find overlap between end of R1 and start of R2_rc\n",
    "        best_overlap = 0\n",
    "        for overlap_len in range(min_overlap, read_len):\n",
    "            r1_end = r1_seq[read_len - overlap_len:]\n",
    "            r2_start = r2_rc[:overlap_len]\n",
    "            # Count matches\n",
    "            matches = sum(1 for a, b in zip(r1_end, r2_start) if a == b)\n",
    "            if matches / overlap_len > 0.85:  # 85% match threshold\n",
    "                best_overlap = overlap_len\n",
    "\n",
    "        if best_overlap > 0:\n",
    "            insert_size = 2 * read_len - best_overlap\n",
    "            insert_sizes.append(insert_size)\n",
    "        else:\n",
    "            # No overlap found — insert size > 2 * read_length\n",
    "            # Estimate based on known generation parameters\n",
    "            insert_sizes.append(300)  # default estimate\n",
    "\n",
    "    return insert_sizes\n",
    "\n",
    "insert_sizes = estimate_insert_sizes(r1_file, r2_file)\n",
    "if insert_sizes:\n",
    "    avg_insert = sum(insert_sizes) / len(insert_sizes)\n",
    "    print(f\"Sampled {len(insert_sizes)} read pairs\")\n",
    "    print(f\"Estimated mean insert size: {avg_insert:.0f}\")\n",
    "    print(f\"Insert size range: {min(insert_sizes)}-{max(insert_sizes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1a7bd9",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 10: Final Project — Complete FASTQ Processing Pipeline\n",
    "\n",
    "### The Challenge\n",
    "\n",
    "Build a complete, production-quality FASTQ processing pipeline that combines everything you've learned. Your pipeline should:\n",
    "\n",
    "1. **Accept** single-end or paired-end input\n",
    "2. **Generate** a comprehensive quality report\n",
    "3. **Filter** reads based on configurable criteria\n",
    "4. **Process** data efficiently using streaming and/or parallel approaches\n",
    "5. **Output** filtered reads and a summary report\n",
    "\n",
    "This is an open-ended exercise. Use whatever combination of techniques from Parts 0-9 makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f0146f",
   "metadata": {},
   "source": [
    "### 10.1 Pipeline Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebdf4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FASTQPipeline:\n",
    "    \"\"\"Complete FASTQ processing pipeline.\n",
    "\n",
    "    TODO: Implement this class with the following methods:\n",
    "\n",
    "    __init__(self, config: dict)\n",
    "        - Store configuration (quality thresholds, chunk size, etc.)\n",
    "        - Set defaults for any missing config values\n",
    "\n",
    "    analyze(self, filename: str) -> dict\n",
    "        - Generate pre-filtering statistics\n",
    "        - Return: read count, avg quality, GC content, N content,\n",
    "                  quality distribution, length distribution\n",
    "\n",
    "    filter_reads(self, input_file: str, output_file: str) -> dict\n",
    "        - Apply quality filtering pipeline\n",
    "        - Return filtering statistics\n",
    "\n",
    "    process_paired(self, r1_file: str, r2_file: str,\n",
    "                   out_r1: str, out_r2: str) -> dict\n",
    "        - Process paired-end files in sync\n",
    "        - Return paired filtering statistics\n",
    "\n",
    "    generate_report(self, stats: dict) -> str\n",
    "        - Generate a text summary report\n",
    "        - Return report as a string\n",
    "\n",
    "    run(self, input_files: list, output_dir: str) -> dict\n",
    "        - Full pipeline: analyze -> filter -> report\n",
    "        - Handle both single-end and paired-end inputs\n",
    "        - Return complete results dict\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: dict = None):\n",
    "        # TODO: Your implementation here\n",
    "        pass\n",
    "\n",
    "    def analyze(self, filename: str) -> dict:\n",
    "        # TODO: Your implementation here\n",
    "        pass\n",
    "\n",
    "    def filter_reads(self, input_file: str, output_file: str) -> dict:\n",
    "        # TODO: Your implementation here\n",
    "        pass\n",
    "\n",
    "    def process_paired(self, r1_file: str, r2_file: str,\n",
    "                       out_r1: str, out_r2: str) -> dict:\n",
    "        # TODO: Your implementation here\n",
    "        pass\n",
    "\n",
    "    def generate_report(self, stats: dict) -> str:\n",
    "        # TODO: Your implementation here\n",
    "        pass\n",
    "\n",
    "    def run(self, input_files: list, output_dir: str) -> dict:\n",
    "        # TODO: Your implementation here\n",
    "        pass\n",
    "\n",
    "# Test:\n",
    "# pipeline = FASTQPipeline({\n",
    "#     'min_quality': 20,\n",
    "#     'min_length': 50,\n",
    "#     'max_n_fraction': 0.05,\n",
    "#     'chunk_size': 2500\n",
    "# })\n",
    "# results = pipeline.run(\n",
    "#     [sample_file],\n",
    "#     os.path.join(DATA_DIR, 'pipeline_output')\n",
    "# )\n",
    "# print(pipeline.generate_report(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1b28a7",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# SOLUTION - Exercise 10 (click to expand)\n",
    "# =============================================\n",
    "\n",
    "class FASTQPipeline:\n",
    "    \"\"\"Complete FASTQ processing pipeline.\"\"\"\n",
    "\n",
    "    def __init__(self, config: dict = None):\n",
    "        defaults = {\n",
    "            'min_quality': 20.0,\n",
    "            'min_length': 50,\n",
    "            'max_n_fraction': 0.05,\n",
    "            'trim_quality': 15,\n",
    "            'chunk_size': 2500,\n",
    "            'num_workers': min(4, multiprocessing.cpu_count()),\n",
    "        }\n",
    "        self.config = {**defaults, **(config or {})}\n",
    "\n",
    "    def analyze(self, filename: str) -> dict:\n",
    "        \"\"\"Generate pre-filtering statistics.\"\"\"\n",
    "        total_reads = 0\n",
    "        total_bases = 0\n",
    "        gc_bases = 0\n",
    "        n_bases = 0\n",
    "        qual_sum = 0\n",
    "        qual_count = 0\n",
    "        lengths = []\n",
    "        quality_scores = []\n",
    "\n",
    "        for header, seq, qual in read_fastq_generator(filename):\n",
    "            total_reads += 1\n",
    "            total_bases += len(seq)\n",
    "            seq_upper = seq.upper()\n",
    "            gc_bases += seq_upper.count('G') + seq_upper.count('C')\n",
    "            n_bases += seq_upper.count('N')\n",
    "            lengths.append(len(seq))\n",
    "\n",
    "            for c in qual:\n",
    "                q = ord(c) - 33\n",
    "                qual_sum += q\n",
    "                qual_count += 1\n",
    "                quality_scores.append(q)\n",
    "\n",
    "        return {\n",
    "            'filename': os.path.basename(filename),\n",
    "            'total_reads': total_reads,\n",
    "            'total_bases': total_bases,\n",
    "            'avg_length': total_bases / total_reads if total_reads else 0,\n",
    "            'gc_content': gc_bases / total_bases if total_bases else 0,\n",
    "            'n_content': n_bases / total_bases if total_bases else 0,\n",
    "            'avg_quality': qual_sum / qual_count if qual_count else 0,\n",
    "        }\n",
    "\n",
    "    def filter_reads(self, input_file: str, output_file: str) -> dict:\n",
    "        \"\"\"Apply quality filtering pipeline.\"\"\"\n",
    "        stats = {'total': 0, 'passed': 0, 'failed': 0}\n",
    "\n",
    "        pipeline = read_fastq_generator(input_file)\n",
    "        pipeline = filter_by_quality(pipeline, self.config['min_quality'])\n",
    "        pipeline = filter_no_ns(pipeline, self.config['max_n_fraction'])\n",
    "        pipeline = trim_low_quality_ends(pipeline, self.config['trim_quality'])\n",
    "        pipeline = filter_by_length(pipeline, self.config['min_length'])\n",
    "\n",
    "        # We need to count total separately since pipeline may skip reads\n",
    "        total = 0\n",
    "        passed = 0\n",
    "        with open(output_file, 'w') as out:\n",
    "            # Count total from a separate pass\n",
    "            for header, seq, qual in read_fastq_generator(input_file):\n",
    "                total += 1\n",
    "\n",
    "        stats['total'] = total\n",
    "\n",
    "        with open(output_file, 'w') as out:\n",
    "            pipeline = read_fastq_generator(input_file)\n",
    "            pipeline = filter_by_quality(pipeline, self.config['min_quality'])\n",
    "            pipeline = filter_no_ns(pipeline, self.config['max_n_fraction'])\n",
    "            pipeline = trim_low_quality_ends(pipeline, self.config['trim_quality'])\n",
    "            pipeline = filter_by_length(pipeline, self.config['min_length'])\n",
    "\n",
    "            for header, seq, qual in pipeline:\n",
    "                passed += 1\n",
    "                out.write(f\"{header}\\n{seq}\\n+\\n{qual}\\n\")\n",
    "\n",
    "        stats['passed'] = passed\n",
    "        stats['failed'] = total - passed\n",
    "        return stats\n",
    "\n",
    "    def process_paired(self, r1_file: str, r2_file: str,\n",
    "                       out_r1: str, out_r2: str) -> dict:\n",
    "        \"\"\"Process paired-end files in sync.\"\"\"\n",
    "        return filter_paired_reads(\n",
    "            r1_file, r2_file, out_r1, out_r2,\n",
    "            min_avg_quality=self.config['min_quality']\n",
    "        )\n",
    "\n",
    "    def generate_report(self, stats: dict) -> str:\n",
    "        \"\"\"Generate a text summary report.\"\"\"\n",
    "        lines = []\n",
    "        lines.append(\"=\" * 60)\n",
    "        lines.append(\"FASTQ Processing Pipeline Report\")\n",
    "        lines.append(\"=\" * 60)\n",
    "\n",
    "        if 'pre_filter' in stats:\n",
    "            pf = stats['pre_filter']\n",
    "            lines.append(f\"\\nInput: {pf.get('filename', 'N/A')}\")\n",
    "            lines.append(f\"  Total reads:  {pf.get('total_reads', 0):,}\")\n",
    "            lines.append(f\"  Total bases:  {pf.get('total_bases', 0):,}\")\n",
    "            lines.append(f\"  Avg length:   {pf.get('avg_length', 0):.1f}\")\n",
    "            lines.append(f\"  GC content:   {pf.get('gc_content', 0)*100:.2f}%\")\n",
    "            lines.append(f\"  N content:    {pf.get('n_content', 0)*100:.4f}%\")\n",
    "            lines.append(f\"  Avg quality:  {pf.get('avg_quality', 0):.2f}\")\n",
    "\n",
    "        if 'filtering' in stats:\n",
    "            fs = stats['filtering']\n",
    "            lines.append(f\"\\nFiltering:\")\n",
    "            lines.append(f\"  Input reads:  {fs.get('total', 0):,}\")\n",
    "            lines.append(f\"  Passed:       {fs.get('passed', 0):,}\")\n",
    "            lines.append(f\"  Failed:       {fs.get('failed', 0):,}\")\n",
    "            if fs.get('total', 0) > 0:\n",
    "                rate = 100 * fs['passed'] / fs['total']\n",
    "                lines.append(f\"  Pass rate:    {rate:.1f}%\")\n",
    "\n",
    "        if 'config' in stats:\n",
    "            lines.append(f\"\\nConfiguration:\")\n",
    "            for k, v in stats['config'].items():\n",
    "                lines.append(f\"  {k}: {v}\")\n",
    "\n",
    "        lines.append(\"\\n\" + \"=\" * 60)\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "    def run(self, input_files: list, output_dir: str) -> dict:\n",
    "        \"\"\"Full pipeline: analyze -> filter -> report.\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        results = {'config': self.config}\n",
    "\n",
    "        if len(input_files) == 1:\n",
    "            # Single-end mode\n",
    "            input_file = input_files[0]\n",
    "            results['pre_filter'] = self.analyze(input_file)\n",
    "\n",
    "            output_file = os.path.join(\n",
    "                output_dir,\n",
    "                'filtered_' + os.path.basename(input_file)\n",
    "            )\n",
    "            results['filtering'] = self.filter_reads(input_file, output_file)\n",
    "            results['output_file'] = output_file\n",
    "\n",
    "        elif len(input_files) == 2:\n",
    "            # Paired-end mode\n",
    "            results['pre_filter'] = self.analyze(input_files[0])\n",
    "            out_r1 = os.path.join(output_dir, 'filtered_' + os.path.basename(input_files[0]))\n",
    "            out_r2 = os.path.join(output_dir, 'filtered_' + os.path.basename(input_files[1]))\n",
    "            pe_stats = self.process_paired(\n",
    "                input_files[0], input_files[1], out_r1, out_r2\n",
    "            )\n",
    "            results['filtering'] = {\n",
    "                'total': pe_stats['total_pairs'],\n",
    "                'passed': pe_stats['passed_pairs'],\n",
    "                'failed': pe_stats['failed_pairs']\n",
    "            }\n",
    "\n",
    "        return results\n",
    "\n",
    "# Run the pipeline\n",
    "pipeline = FASTQPipeline({\n",
    "    'min_quality': 20,\n",
    "    'min_length': 50,\n",
    "    'max_n_fraction': 0.05,\n",
    "})\n",
    "\n",
    "print(\"Running pipeline on single-end data...\")\n",
    "results = pipeline.run(\n",
    "    [sample_file],\n",
    "    os.path.join(DATA_DIR, 'pipeline_output')\n",
    ")\n",
    "print(pipeline.generate_report(results))\n",
    "\n",
    "print(\"\\n\\nRunning pipeline on paired-end data...\")\n",
    "pe_results = pipeline.run(\n",
    "    [r1_file, r2_file],\n",
    "    os.path.join(DATA_DIR, 'pipeline_output')\n",
    ")\n",
    "print(pipeline.generate_report(pe_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b973799a",
   "metadata": {},
   "source": [
    "---\n",
    "## Congratulations!\n",
    "\n",
    "You've completed the Big Data Bioinformatics Exercises! Here's a summary of what you've learned:\n",
    "\n",
    "| Part | Technique | Big Data Principle |\n",
    "|------|-----------|-------------------|\n",
    "| 1 | Generators | Process data larger than RAM |\n",
    "| 2 | Chunking | Batch processing with progress |\n",
    "| 3 | Parallel processing | Use all CPU cores |\n",
    "| 4 | Indexing | O(1) random access |\n",
    "| 5 | Streaming pipelines | Composable, memory-efficient transforms |\n",
    "| 6 | MapReduce | Distributed aggregation pattern |\n",
    "| 7 | Tool integration | Leverage optimized C/Go tools |\n",
    "| 8 | Profiling | Measure before optimizing |\n",
    "| 9 | Paired-end | Synchronized multi-file processing |\n",
    "| 10 | Pipeline | Combine all techniques |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Try these techniques on real FASTQ data from [SRA](https://www.ncbi.nlm.nih.gov/sra)\n",
    "- Explore [Snakemake](https://snakemake.readthedocs.io/) for workflow management\n",
    "- Learn [Dask](https://dask.org/) for out-of-core DataFrames\n",
    "- Study [Apache Spark](https://spark.apache.org/) for true distributed computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ac6583-f995-4f5f-8bfc-aea9c6ad3867",
   "metadata": {},
   "outputs": [],
   "source": [
    "## End of Notebook ##"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
