{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS S3 Interaction with Boto3\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Setting up AWS credentials and Boto3 client\n",
    "2. Creating an S3 bucket\n",
    "3. Creating and uploading a test file\n",
    "4. Listing bucket contents\n",
    "5. Reading file contents from S3\n",
    "6. Cleaning up resources (deleting files and buckets)\n",
    "\n",
    "## Prerequisites\n",
    "- AWS account with appropriate permissions\n",
    "- AWS credentials configured (access key and secret key)\n",
    "- Boto3 library installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install boto3 if needed (uncomment to install)\n",
    "# !pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from botocore.exceptions import ClientError, NoCredentialsError\n",
    "import uuid\n",
    "\n",
    "print(f\"Boto3 version: {boto3.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Configure AWS Credentials\n",
    "\n",
    "**Option 1: Use AWS credentials file** (Recommended)\n",
    "- Configure via AWS CLI: `aws configure`\n",
    "- Or manually create `~/.aws/credentials`\n",
    "\n",
    "**Option 2: Set environment variables**\n",
    "```bash\n",
    "export AWS_ACCESS_KEY_ID='your-access-key'\n",
    "export AWS_SECRET_ACCESS_KEY='your-secret-key'\n",
    "export AWS_DEFAULT_REGION='us-east-1'\n",
    "```\n",
    "\n",
    "**Option 3: Specify credentials directly** (Not recommended for production)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "AWS_REGION = 'us-east-1'  # Change to your preferred region\n",
    "\n",
    "# Option 1: Use default credentials (from ~/.aws/credentials or environment)\n",
    "s3_client = boto3.client('s3', region_name=AWS_REGION)\n",
    "\n",
    "# Option 2: Specify credentials directly (ONLY for testing - not recommended)\n",
    "# s3_client = boto3.client(\n",
    "#     's3',\n",
    "#     region_name=AWS_REGION,\n",
    "#     aws_access_key_id='YOUR_ACCESS_KEY',\n",
    "#     aws_secret_access_key='YOUR_SECRET_KEY'\n",
    "# )\n",
    "\n",
    "# Test connection by listing existing buckets\n",
    "try:\n",
    "    response = s3_client.list_buckets()\n",
    "    print(\"Successfully connected to AWS S3!\")\n",
    "    print(f\"\\nExisting buckets in your account:\")\n",
    "    if response['Buckets']:\n",
    "        for bucket in response['Buckets']:\n",
    "            print(f\"  - {bucket['Name']} (Created: {bucket['CreationDate']})\")\n",
    "    else:\n",
    "        print(\"  No existing buckets found.\")\n",
    "except NoCredentialsError:\n",
    "    print(\"ERROR: AWS credentials not found!\")\n",
    "    print(\"Please configure your credentials using 'aws configure' or set environment variables.\")\n",
    "except ClientError as e:\n",
    "    print(f\"ERROR: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create S3 Bucket\n",
    "\n",
    "S3 bucket names must be globally unique across all AWS accounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_s3_bucket(bucket_name, region=None):\n",
    "    \"\"\"\n",
    "    Create an S3 bucket in a specified region.\n",
    "    \n",
    "    Args:\n",
    "        bucket_name: Name of the bucket to create\n",
    "        region: AWS region (if None, uses default region)\n",
    "    \n",
    "    Returns:\n",
    "        True if bucket created successfully, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if region is None or region == 'us-east-1':\n",
    "            # us-east-1 doesn't need LocationConstraint\n",
    "            s3_client.create_bucket(Bucket=bucket_name)\n",
    "        else:\n",
    "            location = {'LocationConstraint': region}\n",
    "            s3_client.create_bucket(\n",
    "                Bucket=bucket_name,\n",
    "                CreateBucketConfiguration=location\n",
    "            )\n",
    "        print(f\"Successfully created bucket: {bucket_name}\")\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        if error_code == 'BucketAlreadyOwnedByYou':\n",
    "            print(f\"Bucket {bucket_name} already exists and is owned by you.\")\n",
    "            return True\n",
    "        elif error_code == 'BucketAlreadyExists':\n",
    "            print(f\"ERROR: Bucket {bucket_name} already exists globally. Choose a different name.\")\n",
    "            return False\n",
    "        else:\n",
    "            print(f\"ERROR creating bucket: {e}\")\n",
    "            return False\n",
    "\n",
    "# Generate a unique bucket name\n",
    "timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "unique_id = str(uuid.uuid4())[:8]\n",
    "BUCKET_NAME = f'test-bucket-{timestamp}-{unique_id}'.lower()\n",
    "\n",
    "print(f\"Creating bucket: {BUCKET_NAME}\")\n",
    "print(f\"Region: {AWS_REGION}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "bucket_created = create_s3_bucket(BUCKET_NAME, AWS_REGION)\n",
    "\n",
    "if bucket_created:\n",
    "    print(f\"\\nBucket '{BUCKET_NAME}' is ready to use!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create a Test File and Upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_file(filename='test_file.txt', content=None):\n",
    "    \"\"\"\n",
    "    Create a test file with sample content.\n",
    "    \n",
    "    Args:\n",
    "        filename: Name of the file to create\n",
    "        content: Content to write (if None, creates default content)\n",
    "    \n",
    "    Returns:\n",
    "        Path to the created file\n",
    "    \"\"\"\n",
    "    if content is None:\n",
    "        content = f\"\"\"This is a test file for AWS S3 upload.\n",
    "Created on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "Sample Data:\n",
    "------------\n",
    "Line 1: Hello from S3!\n",
    "Line 2: This file demonstrates S3 upload functionality.\n",
    "Line 3: Boto3 makes AWS integration easy.\n",
    "Line 4: S3 is a scalable object storage service.\n",
    "Line 5: End of test file.\n",
    "\"\"\"\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(content)\n",
    "    \n",
    "    file_size = os.path.getsize(filename)\n",
    "    print(f\"Created test file: {filename} ({file_size} bytes)\")\n",
    "    return filename\n",
    "\n",
    "def upload_file_to_s3(file_path, bucket, object_name=None):\n",
    "    \"\"\"\n",
    "    Upload a file to an S3 bucket.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to file to upload\n",
    "        bucket: Bucket to upload to\n",
    "        object_name: S3 object name (if None, uses file_path)\n",
    "    \n",
    "    Returns:\n",
    "        True if file was uploaded, False otherwise\n",
    "    \"\"\"\n",
    "    if object_name is None:\n",
    "        object_name = os.path.basename(file_path)\n",
    "    \n",
    "    try:\n",
    "        s3_client.upload_file(file_path, bucket, object_name)\n",
    "        print(f\"Successfully uploaded {file_path} to {bucket}/{object_name}\")\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR uploading file: {e}\")\n",
    "        return False\n",
    "\n",
    "# Create and upload test file\n",
    "TEST_FILE = 'test_file.txt'\n",
    "print(\"Creating test file...\")\n",
    "create_test_file(TEST_FILE)\n",
    "\n",
    "print(\"\\nUploading to S3...\")\n",
    "upload_success = upload_file_to_s3(TEST_FILE, BUCKET_NAME)\n",
    "\n",
    "if upload_success:\n",
    "    print(f\"\\nFile is now available at: s3://{BUCKET_NAME}/{TEST_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Upload Additional Files (JSON Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and upload a JSON file\n",
    "json_data = {\n",
    "    'experiment': 'S3_test',\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'data': {\n",
    "        'samples': ['sample1', 'sample2', 'sample3'],\n",
    "        'results': [42, 37, 51],\n",
    "        'passed': True\n",
    "    },\n",
    "    'metadata': {\n",
    "        'version': '1.0',\n",
    "        'author': 'Boto3 Script'\n",
    "    }\n",
    "}\n",
    "\n",
    "JSON_FILE = 'test_data.json'\n",
    "with open(JSON_FILE, 'w') as f:\n",
    "    json.dump(json_data, f, indent=2)\n",
    "\n",
    "print(f\"Created JSON file: {JSON_FILE}\")\n",
    "print(\"\\nJSON content:\")\n",
    "print(json.dumps(json_data, indent=2))\n",
    "\n",
    "print(\"\\nUploading JSON file to S3...\")\n",
    "upload_file_to_s3(JSON_FILE, BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: List Bucket Contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_bucket_contents(bucket_name):\n",
    "    \"\"\"\n",
    "    List all objects in an S3 bucket.\n",
    "    \n",
    "    Args:\n",
    "        bucket_name: Name of the bucket to list\n",
    "    \n",
    "    Returns:\n",
    "        List of object keys\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = s3_client.list_objects_v2(Bucket=bucket_name)\n",
    "        \n",
    "        if 'Contents' not in response:\n",
    "            print(f\"Bucket '{bucket_name}' is empty.\")\n",
    "            return []\n",
    "        \n",
    "        objects = response['Contents']\n",
    "        print(f\"Contents of bucket '{bucket_name}':\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"{'Object Key':<40} {'Size (bytes)':<15} {'Last Modified'}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        object_keys = []\n",
    "        for obj in objects:\n",
    "            key = obj['Key']\n",
    "            size = obj['Size']\n",
    "            modified = obj['LastModified'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print(f\"{key:<40} {size:<15} {modified}\")\n",
    "            object_keys.append(key)\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Total objects: {len(objects)}\")\n",
    "        print(f\"Total size: {sum(obj['Size'] for obj in objects):,} bytes\")\n",
    "        \n",
    "        return object_keys\n",
    "        \n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR listing bucket contents: {e}\")\n",
    "        return []\n",
    "\n",
    "# List all objects in the bucket\n",
    "object_list = list_bucket_contents(BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Read and Display File Contents from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_s3_object(bucket_name, object_key):\n",
    "    \"\"\"\n",
    "    Read and return the contents of an S3 object.\n",
    "    \n",
    "    Args:\n",
    "        bucket_name: Name of the bucket\n",
    "        object_key: Key of the object to read\n",
    "    \n",
    "    Returns:\n",
    "        Content of the object as string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = s3_client.get_object(Bucket=bucket_name, Key=object_key)\n",
    "        content = response['Body'].read().decode('utf-8')\n",
    "        return content\n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR reading object: {e}\")\n",
    "        return None\n",
    "\n",
    "# Read and display the text file\n",
    "print(f\"Reading file: {TEST_FILE}\")\n",
    "print(\"=\" * 60)\n",
    "text_content = read_s3_object(BUCKET_NAME, TEST_FILE)\n",
    "if text_content:\n",
    "    print(text_content)\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "# Read and display the JSON file\n",
    "print(f\"\\nReading file: {JSON_FILE}\")\n",
    "print(\"=\" * 60)\n",
    "json_content = read_s3_object(BUCKET_NAME, JSON_FILE)\n",
    "if json_content:\n",
    "    json_parsed = json.loads(json_content)\n",
    "    print(json.dumps(json_parsed, indent=2))\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Get Object Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_object_metadata(bucket_name, object_key):\n",
    "    \"\"\"\n",
    "    Get metadata for an S3 object.\n",
    "    \n",
    "    Args:\n",
    "        bucket_name: Name of the bucket\n",
    "        object_key: Key of the object\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = s3_client.head_object(Bucket=bucket_name, Key=object_key)\n",
    "        \n",
    "        print(f\"Metadata for: s3://{bucket_name}/{object_key}\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Content-Type: {response.get('ContentType', 'N/A')}\")\n",
    "        print(f\"Content-Length: {response.get('ContentLength', 0):,} bytes\")\n",
    "        print(f\"Last-Modified: {response.get('LastModified', 'N/A')}\")\n",
    "        print(f\"ETag: {response.get('ETag', 'N/A')}\")\n",
    "        print(f\"Storage-Class: {response.get('StorageClass', 'STANDARD')}\")\n",
    "        \n",
    "        if 'Metadata' in response and response['Metadata']:\n",
    "            print(\"\\nCustom Metadata:\")\n",
    "            for key, value in response['Metadata'].items():\n",
    "                print(f\"  {key}: {value}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR getting metadata: {e}\")\n",
    "\n",
    "# Get metadata for uploaded files\n",
    "for obj_key in object_list:\n",
    "    get_object_metadata(BUCKET_NAME, obj_key)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Download Files from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file_from_s3(bucket_name, object_key, local_filename=None):\n",
    "    \"\"\"\n",
    "    Download a file from S3 to local filesystem.\n",
    "    \n",
    "    Args:\n",
    "        bucket_name: Name of the bucket\n",
    "        object_key: Key of the object to download\n",
    "        local_filename: Local file path (if None, uses object_key)\n",
    "    \n",
    "    Returns:\n",
    "        True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    if local_filename is None:\n",
    "        local_filename = f\"downloaded_{object_key}\"\n",
    "    \n",
    "    try:\n",
    "        s3_client.download_file(bucket_name, object_key, local_filename)\n",
    "        file_size = os.path.getsize(local_filename)\n",
    "        print(f\"Successfully downloaded {object_key} to {local_filename} ({file_size} bytes)\")\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR downloading file: {e}\")\n",
    "        return False\n",
    "\n",
    "# Example: Download the JSON file with a different name\n",
    "print(\"Downloading file from S3...\")\n",
    "download_file_from_s3(BUCKET_NAME, JSON_FILE, 'downloaded_test_data.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Cleanup - Delete Files and Bucket\n",
    "\n",
    "**WARNING:** The following cells will permanently delete files and the bucket.\n",
    "\n",
    "Run these cells only when you're ready to clean up the resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_object_from_s3(bucket_name, object_key):\n",
    "    \"\"\"\n",
    "    Delete an object from S3.\n",
    "    \n",
    "    Args:\n",
    "        bucket_name: Name of the bucket\n",
    "        object_key: Key of the object to delete\n",
    "    \n",
    "    Returns:\n",
    "        True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        s3_client.delete_object(Bucket=bucket_name, Key=object_key)\n",
    "        print(f\"Deleted: s3://{bucket_name}/{object_key}\")\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR deleting object: {e}\")\n",
    "        return False\n",
    "\n",
    "def delete_all_objects_in_bucket(bucket_name):\n",
    "    \"\"\"\n",
    "    Delete all objects in a bucket.\n",
    "    \n",
    "    Args:\n",
    "        bucket_name: Name of the bucket\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = s3_client.list_objects_v2(Bucket=bucket_name)\n",
    "        \n",
    "        if 'Contents' not in response:\n",
    "            print(f\"Bucket '{bucket_name}' is already empty.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Deleting all objects from bucket '{bucket_name}'...\")\n",
    "        for obj in response['Contents']:\n",
    "            delete_object_from_s3(bucket_name, obj['Key'])\n",
    "        \n",
    "        print(f\"\\nAll objects deleted from '{bucket_name}'.\")\n",
    "        \n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR deleting objects: {e}\")\n",
    "\n",
    "# Option 1: Delete specific file\n",
    "print(\"Option 1: Delete specific files\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nTo delete a specific file, uncomment and run:\")\n",
    "print(f\"# delete_object_from_s3('{BUCKET_NAME}', '{TEST_FILE}')\")\n",
    "print(f\"# delete_object_from_s3('{BUCKET_NAME}', '{JSON_FILE}')\")\n",
    "\n",
    "# Uncomment to delete specific files:\n",
    "# delete_object_from_s3(BUCKET_NAME, TEST_FILE)\n",
    "# delete_object_from_s3(BUCKET_NAME, JSON_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Delete all objects in the bucket\n",
    "print(\"Option 2: Delete ALL objects in bucket\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Bucket: {BUCKET_NAME}\")\n",
    "print(\"\\nWARNING: This will delete ALL files in the bucket!\")\n",
    "print(\"\\nTo delete all objects, uncomment and run:\")\n",
    "print(f\"# delete_all_objects_in_bucket('{BUCKET_NAME}')\")\n",
    "\n",
    "# Uncomment to delete all objects:\n",
    "# delete_all_objects_in_bucket(BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_bucket(bucket_name):\n",
    "    \"\"\"\n",
    "    Delete an S3 bucket (must be empty).\n",
    "    \n",
    "    Args:\n",
    "        bucket_name: Name of the bucket to delete\n",
    "    \n",
    "    Returns:\n",
    "        True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        s3_client.delete_bucket(Bucket=bucket_name)\n",
    "        print(f\"Successfully deleted bucket: {bucket_name}\")\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        if error_code == 'BucketNotEmpty':\n",
    "            print(f\"ERROR: Bucket '{bucket_name}' is not empty.\")\n",
    "            print(\"Delete all objects first before deleting the bucket.\")\n",
    "        else:\n",
    "            print(f\"ERROR deleting bucket: {e}\")\n",
    "        return False\n",
    "\n",
    "# Delete the bucket\n",
    "print(\"Option 3: Delete the S3 bucket\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Bucket: {BUCKET_NAME}\")\n",
    "print(\"\\nWARNING: The bucket must be empty before deletion!\")\n",
    "print(\"\\nTo delete the bucket, uncomment and run:\")\n",
    "print(f\"# delete_bucket('{BUCKET_NAME}')\")\n",
    "\n",
    "# Uncomment to delete the bucket:\n",
    "# delete_bucket(BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete cleanup function - deletes everything\n",
    "def complete_cleanup(bucket_name):\n",
    "    \"\"\"\n",
    "    Complete cleanup: delete all objects and the bucket.\n",
    "    \n",
    "    Args:\n",
    "        bucket_name: Name of the bucket to clean up\n",
    "    \"\"\"\n",
    "    print(f\"Starting complete cleanup for bucket: {bucket_name}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Step 1: Delete all objects\n",
    "    print(\"\\nStep 1: Deleting all objects...\")\n",
    "    delete_all_objects_in_bucket(bucket_name)\n",
    "    \n",
    "    # Step 2: Delete the bucket\n",
    "    print(\"\\nStep 2: Deleting bucket...\")\n",
    "    if delete_bucket(bucket_name):\n",
    "        print(\"\\nCleanup complete! All resources have been removed.\")\n",
    "    else:\n",
    "        print(\"\\nCleanup incomplete. Please check errors above.\")\n",
    "\n",
    "print(\"Complete Cleanup Option\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Bucket: {BUCKET_NAME}\")\n",
    "print(\"\\nWARNING: This will delete ALL files and the bucket!\")\n",
    "print(\"\\nTo perform complete cleanup, uncomment and run:\")\n",
    "print(f\"# complete_cleanup('{BUCKET_NAME}')\")\n",
    "\n",
    "# Uncomment to perform complete cleanup:\n",
    "# complete_cleanup(BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Clean Up Local Files (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up local test files\n",
    "def cleanup_local_files():\n",
    "    \"\"\"\n",
    "    Remove local test files created during this session.\n",
    "    \"\"\"\n",
    "    local_files = [TEST_FILE, JSON_FILE, 'downloaded_test_data.json']\n",
    "    \n",
    "    print(\"Cleaning up local files...\")\n",
    "    for filename in local_files:\n",
    "        if os.path.exists(filename):\n",
    "            os.remove(filename)\n",
    "            print(f\"Removed: {filename}\")\n",
    "        else:\n",
    "            print(f\"Not found (skipping): {filename}\")\n",
    "    \n",
    "    print(\"\\nLocal cleanup complete!\")\n",
    "\n",
    "print(\"Local File Cleanup\")\n",
    "print(\"=\" * 60)\n",
    "print(\"To remove local test files, uncomment and run:\")\n",
    "print(\"# cleanup_local_files()\")\n",
    "\n",
    "# Uncomment to clean up local files:\n",
    "# cleanup_local_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **AWS S3 Connection** - Connected to AWS using Boto3 with proper credential handling\n",
    "2. **Bucket Creation** - Created a uniquely named S3 bucket\n",
    "3. **File Upload** - Created and uploaded text and JSON files to S3\n",
    "4. **Bucket Listing** - Listed all objects in the bucket with metadata\n",
    "5. **File Reading** - Read and displayed contents of files stored in S3\n",
    "6. **File Download** - Downloaded files from S3 to local filesystem\n",
    "7. **Cleanup Options** - Provided multiple options for resource cleanup:\n",
    "   - Delete specific files\n",
    "   - Delete all files in bucket\n",
    "   - Delete the bucket\n",
    "   - Complete cleanup (all of the above)\n",
    "   - Clean up local test files\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- S3 bucket names must be globally unique\n",
    "- Always handle AWS credentials securely\n",
    "- Buckets must be empty before deletion\n",
    "- Use proper error handling for AWS operations\n",
    "- Consider costs when storing data in S3\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Explore S3 versioning and lifecycle policies\n",
    "- Implement server-side encryption\n",
    "- Set up bucket policies and access controls\n",
    "- Use S3 Select for querying data\n",
    "- Integrate with other AWS services (Lambda, EC2, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
