{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRAM File Processing and Analysis\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Generation of simulated sequencing data\n",
    "2. Creation of CRAM files\n",
    "3. Decompression and parsing of CRAM files\n",
    "4. Conversion to BAM and SAM formats\n",
    "5. Visualization of alignment characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "!pip install pysam numpy matplotlib seaborn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Generate Simulated Reference Genome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_reference_genome(length=10000, output_file='reference.fasta'):\n",
    "    \"\"\"\n",
    "    Generate a simulated reference genome.\n",
    "    \n",
    "    Args:\n",
    "        length: Length of the reference sequence\n",
    "        output_file: Output FASTA file path\n",
    "    \"\"\"\n",
    "    bases = ['A', 'C', 'G', 'T']\n",
    "    sequence = ''.join(random.choices(bases, k=length))\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write('>chr1\\n')\n",
    "        # Write sequence in lines of 80 characters\n",
    "        for i in range(0, len(sequence), 80):\n",
    "            f.write(sequence[i:i+80] + '\\n')\n",
    "    \n",
    "    print(f\"Generated reference genome: {output_file} ({length} bp)\")\n",
    "    return output_file\n",
    "\n",
    "# Generate reference\n",
    "ref_file = generate_reference_genome(length=10000)\n",
    "\n",
    "# Index the reference (required for CRAM)\n",
    "!samtools faidx {ref_file}\n",
    "print(f\"Indexed reference genome\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Generate Simulated Sequencing Reads and Create CRAM File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_simulated_reads(ref_file, num_reads=1000, read_length=100):\n",
    "    \"\"\"\n",
    "    Generate simulated sequencing reads aligned to a reference genome.\n",
    "    \n",
    "    Args:\n",
    "        ref_file: Reference FASTA file\n",
    "        num_reads: Number of reads to generate\n",
    "        read_length: Length of each read\n",
    "    \n",
    "    Returns:\n",
    "        Path to the generated BAM file\n",
    "    \"\"\"\n",
    "    # Read reference sequence\n",
    "    ref_fasta = pysam.FastaFile(ref_file)\n",
    "    ref_seq = ref_fasta.fetch('chr1')\n",
    "    ref_length = len(ref_seq)\n",
    "    \n",
    "    # Create BAM file header\n",
    "    header = {\n",
    "        'HD': {'VN': '1.6', 'SO': 'coordinate'},\n",
    "        'SQ': [{'LN': ref_length, 'SN': 'chr1'}],\n",
    "        'PG': [{'ID': 'simulate', 'PN': 'simulator', 'VN': '1.0'}]\n",
    "    }\n",
    "    \n",
    "    # Create temporary unsorted BAM file\n",
    "    temp_bam = 'temp_unsorted.bam'\n",
    "    bamfile = pysam.AlignmentFile(temp_bam, 'wb', header=header)\n",
    "    \n",
    "    # Generate reads\n",
    "    for i in range(num_reads):\n",
    "        # Random position on reference\n",
    "        pos = random.randint(0, ref_length - read_length - 1)\n",
    "        \n",
    "        # Extract sequence from reference\n",
    "        seq = ref_seq[pos:pos + read_length]\n",
    "        \n",
    "        # Introduce some mutations (SNPs) - 1% error rate\n",
    "        seq_list = list(seq)\n",
    "        for j in range(len(seq_list)):\n",
    "            if random.random() < 0.01:\n",
    "                bases = ['A', 'C', 'G', 'T']\n",
    "                bases.remove(seq_list[j])\n",
    "                seq_list[j] = random.choice(bases)\n",
    "        seq = ''.join(seq_list)\n",
    "        \n",
    "        # Generate quality scores (Phred scale)\n",
    "        qual = ''.join([chr(random.randint(30, 40) + 33) for _ in range(read_length)])\n",
    "        \n",
    "        # Create alignment\n",
    "        a = pysam.AlignedSegment()\n",
    "        a.query_name = f'read_{i:06d}'\n",
    "        a.query_sequence = seq\n",
    "        a.flag = 0 if random.random() > 0.5 else 16  # Random forward/reverse\n",
    "        a.reference_id = 0  # chr1\n",
    "        a.reference_start = pos\n",
    "        a.mapping_quality = random.randint(20, 60)\n",
    "        a.cigar = [(0, read_length)]  # Match\n",
    "        a.query_qualities = pysam.qualitystring_to_array(qual)\n",
    "        \n",
    "        # Add some tags\n",
    "        a.set_tag('NM', random.randint(0, 3))  # Edit distance\n",
    "        a.set_tag('AS', random.randint(80, 100))  # Alignment score\n",
    "        \n",
    "        bamfile.write(a)\n",
    "    \n",
    "    bamfile.close()\n",
    "    ref_fasta.close()\n",
    "    \n",
    "    # Sort BAM file\n",
    "    sorted_bam = 'simulated_sorted.bam'\n",
    "    pysam.sort('-o', sorted_bam, temp_bam)\n",
    "    pysam.index(sorted_bam)\n",
    "    \n",
    "    # Clean up temporary file\n",
    "    os.remove(temp_bam)\n",
    "    \n",
    "    print(f\"Generated {num_reads} simulated reads in {sorted_bam}\")\n",
    "    return sorted_bam\n",
    "\n",
    "# Generate simulated reads\n",
    "bam_file = generate_simulated_reads(ref_file, num_reads=1000, read_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert BAM to CRAM\n",
    "cram_file = 'simulated.cram'\n",
    "\n",
    "# Use samtools to convert BAM to CRAM (requires reference)\n",
    "!samtools view -C -T {ref_file} -o {cram_file} {bam_file}\n",
    "!samtools index {cram_file}\n",
    "\n",
    "print(f\"\\nCreated CRAM file: {cram_file}\")\n",
    "\n",
    "# Compare file sizes\n",
    "bam_size = os.path.getsize(bam_file)\n",
    "cram_size = os.path.getsize(cram_file)\n",
    "print(f\"\\nFile size comparison:\")\n",
    "print(f\"BAM: {bam_size:,} bytes\")\n",
    "print(f\"CRAM: {cram_size:,} bytes\")\n",
    "print(f\"Compression ratio: {bam_size/cram_size:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Parse and Uncompress CRAM File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_cram_file(cram_file, ref_file, num_reads=10):\n",
    "    \"\"\"\n",
    "    Parse and display information from a CRAM file.\n",
    "    \n",
    "    Args:\n",
    "        cram_file: Path to CRAM file\n",
    "        ref_file: Path to reference FASTA file\n",
    "        num_reads: Number of example reads to display\n",
    "    \"\"\"\n",
    "    cramfile = pysam.AlignmentFile(cram_file, 'rc', reference_filename=ref_file)\n",
    "    \n",
    "    print(\"CRAM File Header:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Version: {cramfile.header.get('HD', {}).get('VN', 'N/A')}\")\n",
    "    print(f\"Sort order: {cramfile.header.get('HD', {}).get('SO', 'N/A')}\")\n",
    "    print(f\"\\nReference sequences:\")\n",
    "    for sq in cramfile.header.get('SQ', []):\n",
    "        print(f\"  {sq['SN']}: {sq['LN']:,} bp\")\n",
    "    \n",
    "    print(f\"\\nFirst {num_reads} reads:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, read in enumerate(cramfile.fetch()):\n",
    "        if i >= num_reads:\n",
    "            break\n",
    "        \n",
    "        strand = '-' if read.is_reverse else '+'\n",
    "        print(f\"\\nRead {i+1}: {read.query_name}\")\n",
    "        print(f\"  Position: {read.reference_name}:{read.reference_start}-{read.reference_end} ({strand})\")\n",
    "        print(f\"  Sequence: {read.query_sequence[:50]}...\" if len(read.query_sequence) > 50 else f\"  Sequence: {read.query_sequence}\")\n",
    "        print(f\"  MAPQ: {read.mapping_quality}\")\n",
    "        print(f\"  CIGAR: {read.cigarstring}\")\n",
    "        tags_str = ', '.join([f'{tag}={val}' for tag, val in read.get_tags()])\n",
    "        print(f\"  Flags: {read.flag} (Tags: {tags_str})\")\n",
    "    \n",
    "    # Count total reads\n",
    "    total_reads = cramfile.count()\n",
    "    print(f\"\\n\\nTotal reads in CRAM file: {total_reads:,}\")\n",
    "    \n",
    "    cramfile.close()\n",
    "    return total_reads\n",
    "\n",
    "# Parse CRAM file\n",
    "total_reads = parse_cram_file(cram_file, ref_file, num_reads=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Convert CRAM to BAM and SAM Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert CRAM to BAM\n",
    "output_bam = 'output_from_cram.bam'\n",
    "!samtools view -b -T {ref_file} -o {output_bam} {cram_file}\n",
    "!samtools index {output_bam}\n",
    "print(f\"Created BAM file from CRAM: {output_bam}\")\n",
    "\n",
    "# Convert CRAM to SAM\n",
    "output_sam = 'output_from_cram.sam'\n",
    "!samtools view -h -T {ref_file} -o {output_sam} {cram_file}\n",
    "print(f\"Created SAM file from CRAM: {output_sam}\")\n",
    "\n",
    "# Verify files\n",
    "print(f\"\\nOutput file sizes:\")\n",
    "print(f\"BAM: {os.path.getsize(output_bam):,} bytes\")\n",
    "print(f\"SAM: {os.path.getsize(output_sam):,} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Analyze SAM File and Create Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sam_file(sam_file):\n",
    "    \"\"\"\n",
    "    Analyze SAM file and extract statistics for visualization.\n",
    "    \n",
    "    Args:\n",
    "        sam_file: Path to SAM file\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing various statistics\n",
    "    \"\"\"\n",
    "    samfile = pysam.AlignmentFile(sam_file, 'r')\n",
    "    \n",
    "    stats = {\n",
    "        'mapping_qualities': [],\n",
    "        'read_lengths': [],\n",
    "        'positions': [],\n",
    "        'strands': {'forward': 0, 'reverse': 0},\n",
    "        'flags': Counter(),\n",
    "        'edit_distances': [],\n",
    "        'alignment_scores': [],\n",
    "        'base_qualities': [],\n",
    "        'gc_content': []\n",
    "    }\n",
    "    \n",
    "    for read in samfile.fetch():\n",
    "        # Mapping quality\n",
    "        stats['mapping_qualities'].append(read.mapping_quality)\n",
    "        \n",
    "        # Read length\n",
    "        stats['read_lengths'].append(read.query_length)\n",
    "        \n",
    "        # Position\n",
    "        stats['positions'].append(read.reference_start)\n",
    "        \n",
    "        # Strand\n",
    "        if read.is_reverse:\n",
    "            stats['strands']['reverse'] += 1\n",
    "        else:\n",
    "            stats['strands']['forward'] += 1\n",
    "        \n",
    "        # Flags\n",
    "        stats['flags'][read.flag] += 1\n",
    "        \n",
    "        # Edit distance\n",
    "        nm_tag = read.get_tag('NM') if read.has_tag('NM') else 0\n",
    "        stats['edit_distances'].append(nm_tag)\n",
    "        \n",
    "        # Alignment score\n",
    "        as_tag = read.get_tag('AS') if read.has_tag('AS') else 0\n",
    "        stats['alignment_scores'].append(as_tag)\n",
    "        \n",
    "        # Base qualities\n",
    "        if read.query_qualities is not None:\n",
    "            stats['base_qualities'].extend(read.query_qualities)\n",
    "        \n",
    "        # GC content\n",
    "        if read.query_sequence:\n",
    "            gc = (read.query_sequence.count('G') + read.query_sequence.count('C')) / len(read.query_sequence) * 100\n",
    "            stats['gc_content'].append(gc)\n",
    "    \n",
    "    samfile.close()\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Analyze SAM file\n",
    "print(\"Analyzing SAM file...\")\n",
    "stats = analyze_sam_file(output_sam)\n",
    "print(\"Analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 1: Mapping Quality Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(stats['mapping_qualities'], bins=30, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "plt.xlabel('Mapping Quality (MAPQ)', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Distribution of Mapping Quality Scores', fontsize=14, fontweight='bold')\n",
    "plt.axvline(np.mean(stats['mapping_qualities']), color='red', linestyle='--', \n",
    "            label=f'Mean: {np.mean(stats[\"mapping_qualities\"]):.2f}')\n",
    "plt.axvline(np.median(stats['mapping_qualities']), color='green', linestyle='--', \n",
    "            label=f'Median: {np.median(stats[\"mapping_qualities\"]):.2f}')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mapping Quality Statistics:\")\n",
    "print(f\"  Mean: {np.mean(stats['mapping_qualities']):.2f}\")\n",
    "print(f\"  Median: {np.median(stats['mapping_qualities']):.2f}\")\n",
    "print(f\"  Std Dev: {np.std(stats['mapping_qualities']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 2: Read Coverage Along Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate coverage in bins\n",
    "num_bins = 50\n",
    "ref_length = 10000\n",
    "bin_size = ref_length // num_bins\n",
    "coverage = np.zeros(num_bins)\n",
    "\n",
    "for pos in stats['positions']:\n",
    "    bin_idx = min(pos // bin_size, num_bins - 1)\n",
    "    coverage[bin_idx] += 1\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "x_positions = np.arange(num_bins) * bin_size\n",
    "plt.bar(x_positions, coverage, width=bin_size*0.9, edgecolor='black', alpha=0.7, color='coral')\n",
    "plt.xlabel('Position on Reference (bp)', fontsize=12)\n",
    "plt.ylabel('Number of Reads', fontsize=12)\n",
    "plt.title('Read Coverage Distribution Along Reference Genome', fontsize=14, fontweight='bold')\n",
    "plt.axhline(np.mean(coverage), color='red', linestyle='--', \n",
    "            label=f'Mean Coverage: {np.mean(coverage):.2f}')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Coverage Statistics:\")\n",
    "print(f\"  Mean: {np.mean(coverage):.2f} reads/bin\")\n",
    "print(f\"  Max: {np.max(coverage):.0f} reads/bin\")\n",
    "print(f\"  Min: {np.min(coverage):.0f} reads/bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 3: Strand Bias and Base Quality Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Strand distribution pie chart\n",
    "strands = list(stats['strands'].keys())\n",
    "counts = list(stats['strands'].values())\n",
    "colors = ['lightblue', 'lightcoral']\n",
    "explode = (0.05, 0.05)\n",
    "\n",
    "axes[0].pie(counts, labels=strands, autopct='%1.1f%%', startangle=90, \n",
    "            colors=colors, explode=explode, shadow=True)\n",
    "axes[0].set_title('Strand Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Base quality distribution\n",
    "axes[1].hist(stats['base_qualities'], bins=40, edgecolor='black', alpha=0.7, color='lightgreen')\n",
    "axes[1].set_xlabel('Base Quality (Phred Score)', fontsize=12)\n",
    "axes[1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1].set_title('Base Quality Score Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1].axvline(np.mean(stats['base_qualities']), color='red', linestyle='--', \n",
    "                label=f'Mean: {np.mean(stats[\"base_qualities\"]):.2f}')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nStrand Bias:\")\n",
    "print(f\"  Forward: {stats['strands']['forward']} ({stats['strands']['forward']/sum(counts)*100:.1f}%)\")\n",
    "print(f\"  Reverse: {stats['strands']['reverse']} ({stats['strands']['reverse']/sum(counts)*100:.1f}%)\")\n",
    "print(f\"\\nBase Quality Statistics:\")\n",
    "print(f\"  Mean: {np.mean(stats['base_qualities']):.2f}\")\n",
    "print(f\"  Median: {np.median(stats['base_qualities']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 4: Edit Distance and Alignment Score Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Edit distance distribution\n",
    "ed_counts = Counter(stats['edit_distances'])\n",
    "ed_sorted = sorted(ed_counts.items())\n",
    "axes[0].bar([x[0] for x in ed_sorted], [x[1] for x in ed_sorted], \n",
    "            edgecolor='black', alpha=0.7, color='mediumpurple')\n",
    "axes[0].set_xlabel('Edit Distance (NM tag)', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title('Edit Distance Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Alignment score distribution\n",
    "axes[1].hist(stats['alignment_scores'], bins=30, edgecolor='black', alpha=0.7, color='gold')\n",
    "axes[1].set_xlabel('Alignment Score (AS tag)', fontsize=12)\n",
    "axes[1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1].set_title('Alignment Score Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1].axvline(np.mean(stats['alignment_scores']), color='red', linestyle='--', \n",
    "                label=f'Mean: {np.mean(stats[\"alignment_scores\"]):.2f}')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nEdit Distance Statistics:\")\n",
    "print(f\"  Mean: {np.mean(stats['edit_distances']):.2f}\")\n",
    "print(f\"  Median: {np.median(stats['edit_distances']):.2f}\")\n",
    "print(f\"\\nAlignment Score Statistics:\")\n",
    "print(f\"  Mean: {np.mean(stats['alignment_scores']):.2f}\")\n",
    "print(f\"  Median: {np.median(stats['alignment_scores']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization 5: GC Content and Read Length Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# GC content distribution\n",
    "axes[0].hist(stats['gc_content'], bins=30, edgecolor='black', alpha=0.7, color='teal')\n",
    "axes[0].set_xlabel('GC Content (%)', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title('GC Content Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].axvline(np.mean(stats['gc_content']), color='red', linestyle='--', \n",
    "                label=f'Mean: {np.mean(stats[\"gc_content\"]):.2f}%')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Read length distribution\n",
    "rl_counts = Counter(stats['read_lengths'])\n",
    "rl_sorted = sorted(rl_counts.items())\n",
    "axes[1].bar([x[0] for x in rl_sorted], [x[1] for x in rl_sorted], \n",
    "            edgecolor='black', alpha=0.7, color='salmon')\n",
    "axes[1].set_xlabel('Read Length (bp)', fontsize=12)\n",
    "axes[1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1].set_title('Read Length Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nGC Content Statistics:\")\n",
    "print(f\"  Mean: {np.mean(stats['gc_content']):.2f}%\")\n",
    "print(f\"  Median: {np.median(stats['gc_content']):.2f}%\")\n",
    "print(f\"  Std Dev: {np.std(stats['gc_content']):.2f}%\")\n",
    "print(f\"\\nRead Length Statistics:\")\n",
    "print(f\"  Mean: {np.mean(stats['read_lengths']):.2f} bp\")\n",
    "print(f\"  Median: {np.median(stats['read_lengths']):.2f} bp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary DataFrame\n",
    "summary_data = {\n",
    "    'Metric': [\n",
    "        'Total Reads',\n",
    "        'Mean Mapping Quality',\n",
    "        'Mean Read Length',\n",
    "        'Mean Base Quality',\n",
    "        'Mean Edit Distance',\n",
    "        'Mean Alignment Score',\n",
    "        'Mean GC Content',\n",
    "        'Forward Strand %',\n",
    "        'Reverse Strand %'\n",
    "    ],\n",
    "    'Value': [\n",
    "        len(stats['mapping_qualities']),\n",
    "        f\"{np.mean(stats['mapping_qualities']):.2f}\",\n",
    "        f\"{np.mean(stats['read_lengths']):.2f} bp\",\n",
    "        f\"{np.mean(stats['base_qualities']):.2f}\",\n",
    "        f\"{np.mean(stats['edit_distances']):.2f}\",\n",
    "        f\"{np.mean(stats['alignment_scores']):.2f}\",\n",
    "        f\"{np.mean(stats['gc_content']):.2f}%\",\n",
    "        f\"{stats['strands']['forward']/len(stats['mapping_qualities'])*100:.1f}%\",\n",
    "        f\"{stats['strands']['reverse']/len(stats['mapping_qualities'])*100:.1f}%\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to remove generated files\n",
    "# import os\n",
    "# files_to_remove = [\n",
    "#     'reference.fasta', 'reference.fasta.fai',\n",
    "#     'simulated_sorted.bam', 'simulated_sorted.bam.bai',\n",
    "#     'simulated.cram', 'simulated.cram.crai',\n",
    "#     'output_from_cram.bam', 'output_from_cram.bam.bai',\n",
    "#     'output_from_cram.sam'\n",
    "# ]\n",
    "# for f in files_to_remove:\n",
    "#     if os.path.exists(f):\n",
    "#         os.remove(f)\n",
    "#         print(f\"Removed: {f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
