{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Data Quality Monitoring\n",
    "\n",
    "This notebook demonstrates:\n",
    "- Generating data quality reports\n",
    "- Detecting distribution drift\n",
    "- Monitoring feature correlations\n",
    "- Tracking data quality over time\n",
    "- Best practices for ML monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from feature_store import FeatureStore\n",
    "from monitoring import DataQualityMonitor\n",
    "from data_generator import ClinicalDataGenerator\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data from Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to feature store\n",
    "fs = FeatureStore(\n",
    "    db_path='../data/feature_store.duckdb',\n",
    "    config_dir='../config'\n",
    ")\n",
    "\n",
    "# Get raw data and features\n",
    "raw_data = fs.get_raw_data()\n",
    "features = fs.get_features(feature_version=1)\n",
    "\n",
    "print(f\"Loaded {len(raw_data)} patient records\")\n",
    "print(f\"Loaded {len(features)} feature records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Monitor and Generate Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize monitor\n",
    "monitor = DataQualityMonitor(feature_store=fs)\n",
    "\n",
    "# Generate comprehensive quality report\n",
    "report_path = monitor.generate_quality_report(\n",
    "    df=raw_data,\n",
    "    report_name=\"raw_data_quality\",\n",
    "    output_dir=\"../reports\"\n",
    ")\n",
    "\n",
    "print(f\"\\nReport generated: {report_path}\")\n",
    "print(\"Open the HTML file in a browser to view the full report.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Missing Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute missing data metrics\n",
    "missing_metrics = monitor.compute_missing_data_metrics(raw_data)\n",
    "\n",
    "# Convert to DataFrame for visualization\n",
    "missing_df = pd.DataFrame(missing_metrics).T\n",
    "missing_df = missing_df[missing_df['missing_rate'] > 0].sort_values('missing_rate', ascending=False)\n",
    "\n",
    "print(\"Missing Data Summary:\")\n",
    "print(missing_df[['missing_count', 'missing_rate']])\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.barh(missing_df.index, missing_df['missing_rate'] * 100)\n",
    "ax.set_xlabel('Missing Data (%)')\n",
    "ax.set_title('Missing Data by Column')\n",
    "ax.axvline(15, color='red', linestyle='--', label='15% threshold')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute distribution metrics\n",
    "dist_metrics = monitor.compute_distribution_metrics(raw_data)\n",
    "\n",
    "# Display key statistics\n",
    "dist_df = pd.DataFrame(dist_metrics).T\n",
    "print(\"Distribution Statistics:\")\n",
    "print(dist_df[['count', 'mean', 'median', 'std']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distributions for key features\n",
    "key_features = ['age', 'tmb_score', 'wbc_count', 'survival_months']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(key_features):\n",
    "    values = raw_data[feature].dropna()\n",
    "    \n",
    "    axes[idx].hist(values, bins=30, edgecolor='black', alpha=0.7)\n",
    "    axes[idx].axvline(values.mean(), color='red', linestyle='--', label=f'Mean: {values.mean():.2f}')\n",
    "    axes[idx].axvline(values.median(), color='green', linestyle='--', label=f'Median: {values.median():.2f}')\n",
    "    axes[idx].set_xlabel(feature)\n",
    "    axes[idx].set_ylabel('Count')\n",
    "    axes[idx].set_title(f'{feature} Distribution')\n",
    "    axes[idx].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Categorical Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute categorical metrics\n",
    "cat_metrics = monitor.compute_categorical_metrics(raw_data)\n",
    "\n",
    "print(\"Categorical Feature Summary:\\n\")\n",
    "for feature, metrics in cat_metrics.items():\n",
    "    print(f\"{feature}:\")\n",
    "    print(f\"  Unique values: {metrics['unique_values']}\")\n",
    "    print(f\"  Most common: {metrics['most_common']} ({metrics['most_common_pct']:.1f}%)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize key categorical distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Diagnosis\n",
    "diagnosis_counts = raw_data['diagnosis'].value_counts()\n",
    "axes[0, 0].barh(diagnosis_counts.index, diagnosis_counts.values)\n",
    "axes[0, 0].set_xlabel('Count')\n",
    "axes[0, 0].set_title('Diagnosis Distribution')\n",
    "\n",
    "# Treatment Response\n",
    "response_counts = raw_data['treatment_response'].value_counts()\n",
    "axes[0, 1].bar(range(len(response_counts)), response_counts.values)\n",
    "axes[0, 1].set_xticks(range(len(response_counts)))\n",
    "axes[0, 1].set_xticklabels(response_counts.index, rotation=45, ha='right')\n",
    "axes[0, 1].set_ylabel('Count')\n",
    "axes[0, 1].set_title('Treatment Response Distribution')\n",
    "\n",
    "# MSI Status\n",
    "msi_counts = raw_data['msi_status'].value_counts()\n",
    "axes[1, 0].pie(msi_counts.values, labels=msi_counts.index, autopct='%1.1f%%')\n",
    "axes[1, 0].set_title('MSI Status Distribution')\n",
    "\n",
    "# Sex (showing inconsistent formatting)\n",
    "sex_counts = raw_data['sex'].value_counts()\n",
    "axes[1, 1].bar(range(len(sex_counts)), sex_counts.values)\n",
    "axes[1, 1].set_xticks(range(len(sex_counts)))\n",
    "axes[1, 1].set_xticklabels(sex_counts.index, rotation=45)\n",
    "axes[1, 1].set_ylabel('Count')\n",
    "axes[1, 1].set_title('Sex Distribution (Note: Inconsistent Format)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlations\n",
    "corr_metrics = monitor.compute_correlation_metrics(raw_data, threshold=0.5)\n",
    "\n",
    "print(\"High Correlations Detected:\\n\")\n",
    "if corr_metrics.get('high_correlations'):\n",
    "    for corr in corr_metrics['high_correlations']:\n",
    "        print(f\"  {corr['feature1']} <-> {corr['feature2']}: {corr['correlation']:.3f}\")\n",
    "else:\n",
    "    print(\"  No high correlations above threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize correlation matrix\n",
    "numeric_cols = ['age', 'comorbidity_count', 'tmb_score', 'wbc_count', 'hemoglobin', \n",
    "                'platelet_count', 'survival_months']\n",
    "corr_matrix = raw_data[numeric_cols].corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "ax.set_title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Simulate Distribution Drift\n",
    "\n",
    "Generate a new batch of data and compare to detect drift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new batch with slightly different distribution\n",
    "# (Simulating data drift that might occur in production)\n",
    "generator = ClinicalDataGenerator(seed=100)  # Different seed\n",
    "new_data = generator.generate_dataset(n_patients=500, introduce_errors=False)\n",
    "\n",
    "# Remove any validation errors\n",
    "new_data_clean = new_data[\n",
    "    (new_data['age'] >= 0) & (new_data['age'] <= 120) &\n",
    "    (new_data['tmb_score'] <= 100) &\n",
    "    (new_data['comorbidity_count'] <= 10)\n",
    "]\n",
    "\n",
    "print(f\"Original data: {len(raw_data)} patients\")\n",
    "print(f\"New data batch: {len(new_data_clean)} patients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare distributions\n",
    "comparison = monitor.compare_distributions(\n",
    "    df1=raw_data,\n",
    "    df2=new_data_clean,\n",
    "    label1=\"Original\",\n",
    "    label2=\"New Batch\"\n",
    ")\n",
    "\n",
    "# Display features with drift\n",
    "print(\"Distribution Drift Analysis:\\n\")\n",
    "for feature, metrics in comparison['differences'].items():\n",
    "    if metrics['drift_detected']:\n",
    "        print(f\"{feature}:\")\n",
    "        print(f\"  Original mean: {metrics['Original_mean']:.2f}\")\n",
    "        print(f\"  New mean: {metrics['New Batch_mean']:.2f}\")\n",
    "        print(f\"  Drift: {metrics['mean_drift_pct']:.1f}%\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize drift for key features\n",
    "drift_features = ['age', 'tmb_score', 'comorbidity_count']\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "for idx, feature in enumerate(drift_features):\n",
    "    axes[idx].hist(raw_data[feature].dropna(), bins=30, alpha=0.5, label='Original', edgecolor='black')\n",
    "    axes[idx].hist(new_data_clean[feature].dropna(), bins=30, alpha=0.5, label='New Batch', edgecolor='black')\n",
    "    axes[idx].set_xlabel(feature)\n",
    "    axes[idx].set_ylabel('Count')\n",
    "    axes[idx].set_title(f'{feature} Distribution Comparison')\n",
    "    axes[idx].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Quality History\n",
    "\n",
    "Track quality checks over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get quality check history from feature store\n",
    "quality_history = fs.get_data_quality_history(limit=20)\n",
    "\n",
    "print(\"Recent Data Quality Checks:\")\n",
    "print(quality_history[['check_timestamp', 'check_type', 'passed', 'error_count', 'warning_count']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Best Practices for ML Monitoring\n",
    "\n",
    "### Key Monitoring Metrics:\n",
    "\n",
    "1. **Data Quality Metrics**\n",
    "   - Missing data rates\n",
    "   - Invalid values\n",
    "   - Data type errors\n",
    "\n",
    "2. **Distribution Metrics**\n",
    "   - Mean/median shifts\n",
    "   - Standard deviation changes\n",
    "   - Outlier detection\n",
    "\n",
    "3. **Feature Metrics**\n",
    "   - Feature correlations\n",
    "   - Feature importance drift\n",
    "   - New categorical values\n",
    "\n",
    "4. **Model Performance**\n",
    "   - Prediction distribution\n",
    "   - Confidence scores\n",
    "   - Error rates by segment\n",
    "\n",
    "### Alerting Thresholds:\n",
    "\n",
    "- Missing data > 20%: **Alert**\n",
    "- Distribution shift > 20%: **Warning**\n",
    "- New categorical values: **Warning**\n",
    "- Model performance drop > 5%: **Alert**\n",
    "\n",
    "### Monitoring Frequency:\n",
    "\n",
    "- **Real-time**: Critical features and model predictions\n",
    "- **Daily**: Data quality and distribution checks\n",
    "- **Weekly**: Feature importance and correlation analysis\n",
    "- **Monthly**: Comprehensive reports and trend analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook we:\n",
    "1. Generated comprehensive data quality reports\n",
    "2. Analyzed missing data patterns\n",
    "3. Monitored feature distributions\n",
    "4. Detected distribution drift between batches\n",
    "5. Tracked feature correlations\n",
    "6. Reviewed quality check history\n",
    "7. Learned monitoring best practices\n",
    "\n",
    "### Why Monitoring Matters:\n",
    "\n",
    "- **Prevent Silent Failures**: Catch data issues before they affect models\n",
    "- **Detect Drift**: Identify when data patterns change over time\n",
    "- **Maintain Quality**: Ensure consistent feature engineering\n",
    "- **Enable Debugging**: Track down root causes of issues\n",
    "- **Build Trust**: Demonstrate data quality to stakeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close feature store\n",
    "fs.close()\n",
    "\n",
    "print(\"\\nMonitoring demo complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## End of Noteboook ##"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
