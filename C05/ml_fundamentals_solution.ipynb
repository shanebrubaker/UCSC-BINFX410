{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Fundamentals: Data Cleaning, Labeling, and Normalization\n",
    "# SOLUTION NOTEBOOK\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This is the complete solution notebook for the ML fundamentals exercise. It demonstrates best practices for data preprocessing in machine learning.\n",
    "\n",
    "### Learning Goals\n",
    "1. Handle missing values, duplicates, and outliers\n",
    "2. Standardize inconsistent labels\n",
    "3. Apply and compare normalization techniques\n",
    "4. Understand the impact of preprocessing on model performance\n",
    "5. Avoid common pitfalls like data leakage\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 20)\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "expression_df = pd.read_csv('gene_expression_data.csv')\n",
    "metadata_df = pd.read_csv('sample_metadata.csv')\n",
    "\n",
    "print(\"Expression data shape:\", expression_df.shape)\n",
    "print(\"Metadata shape:\", metadata_df.shape)\n",
    "print(\"\\nFirst few rows of expression data:\")\n",
    "display(expression_df.head())\n",
    "print(\"\\nFirst few rows of metadata:\")\n",
    "display(metadata_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.1: Visualize Missing Data\n",
    "\n",
    "**Key Insight**: Visualizing missing data patterns helps identify if missingness is random or systematic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_summary = expression_df.isnull().sum()\n",
    "print(f\"Total missing values: {expression_df.isnull().sum().sum()}\")\n",
    "print(f\"Percentage missing: {100 * expression_df.isnull().sum().sum() / expression_df.size:.2f}%\")\n",
    "\n",
    "# Create heatmap of missing values\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(expression_df.isnull(), cbar=False, cmap='YlOrRd', yticklabels=False)\n",
    "plt.title('Missing Data Heatmap (Yellow = Missing)', fontsize=14)\n",
    "plt.xlabel('Genes')\n",
    "plt.ylabel('Samples')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.2: Handle Missing Values\n",
    "\n",
    "We'll use **median imputation** as it's robust to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy to preserve original data\n",
    "expr_cleaned = expression_df.copy()\n",
    "\n",
    "# Get gene columns (all except sample_id)\n",
    "gene_columns = [col for col in expr_cleaned.columns if col != 'sample_id']\n",
    "\n",
    "print(f\"Before imputation:\")\n",
    "print(f\"  Total NaN values: {expr_cleaned[gene_columns].isnull().sum().sum()}\")\n",
    "print(f\"  Shape: {expr_cleaned.shape}\")\n",
    "\n",
    "# Use sklearn's SimpleImputer for robust median imputation\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "expr_cleaned[gene_columns] = imputer.fit_transform(expr_cleaned[gene_columns])\n",
    "\n",
    "# Verify no missing values remain\n",
    "nan_count = expr_cleaned.isnull().sum().sum()\n",
    "print(f\"\\nAfter imputation:\")\n",
    "print(f\"  Total NaN values: {nan_count}\")\n",
    "print(f\"  Shape: {expr_cleaned.shape}\")\n",
    "\n",
    "# Additional check: verify columns are numeric\n",
    "print(f\"\\nData types check:\")\n",
    "print(f\"  All gene columns numeric: {all(expr_cleaned[col].dtype in ['float64', 'int64'] for col in gene_columns)}\")\n",
    "\n",
    "# Final assertion\n",
    "assert nan_count == 0, f\"ERROR: Still have {nan_count} missing values after imputation!\"\n",
    "print(\"\\n✓ Success! All missing values imputed.\")\n",
    "print(f\"✓ Imputed {len(gene_columns)} genes using median strategy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.3: Detect and Remove Duplicate Samples\n",
    "\n",
    "**Key Insight**: Duplicates violate independence assumptions and can bias results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "print(f\"Shape before removing duplicates: {expr_cleaned.shape}\")\n",
    "print(f\"Number of duplicate rows: {expr_cleaned.duplicated(subset=gene_columns).sum()}\")\n",
    "\n",
    "# Show duplicate rows if any\n",
    "if expr_cleaned.duplicated(subset=gene_columns).sum() > 0:\n",
    "    print(\"\\nDuplicate sample IDs:\")\n",
    "    duplicate_mask = expr_cleaned.duplicated(subset=gene_columns, keep=False)\n",
    "    print(expr_cleaned[duplicate_mask]['sample_id'].tolist())\n",
    "\n",
    "# Remove duplicates (keep first occurrence)\n",
    "expr_cleaned = expr_cleaned.drop_duplicates(subset=gene_columns, keep='first')\n",
    "\n",
    "print(f\"\\nShape after removing duplicates: {expr_cleaned.shape}\")\n",
    "assert expr_cleaned.duplicated(subset=gene_columns).sum() == 0, \"Still have duplicates!\"\n",
    "print(\"Success! All duplicates removed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.4: Identify and Handle Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distribution of first 10 genes\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, gene in enumerate(gene_columns[:10]):\n",
    "    axes[i].boxplot(expr_cleaned[gene].dropna())\n",
    "    axes[i].set_title(gene, fontsize=10)\n",
    "    axes[i].set_ylabel('Expression')\n",
    "\n",
    "plt.suptitle('Gene Expression Distributions (First 10 Genes)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservation: Some genes have extreme outliers and very different scales.\")\n",
    "print(\"We'll handle these through normalization rather than removal.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.5: Standardize Sample IDs\n",
    "\n",
    "**Common Mistake**: Ignoring ID mismatches leads to data loss during merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current ID formats\n",
    "print(\"Expression data sample IDs (first 10):\")\n",
    "print(expr_cleaned['sample_id'].head(10).tolist())\n",
    "print(\"\\nMetadata sample IDs (first 10):\")\n",
    "print(metadata_df['sample_id'].head(10).tolist())\n",
    "\n",
    "# Check how many IDs match\n",
    "matching_ids = set(expr_cleaned['sample_id']) & set(metadata_df['sample_id'])\n",
    "print(f\"\\nCurrently matching IDs: {len(matching_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_sample_id(sample_id):\n",
    "    \"\"\"\n",
    "    Standardize sample ID to 'SAMPLE_XXX' format.\n",
    "    \n",
    "    Examples:\n",
    "    'patient-1' -> 'SAMPLE_001'\n",
    "    'Patient_01' -> 'SAMPLE_001'\n",
    "    'SAMPLE-002' -> 'SAMPLE_002'\n",
    "    \"\"\"\n",
    "    # Convert to string and uppercase\n",
    "    sample_id = str(sample_id).upper()\n",
    "    \n",
    "    # Extract numeric part using regex\n",
    "    match = re.search(r'(\\d+)', sample_id)\n",
    "    if match:\n",
    "        number = int(match.group(1))\n",
    "        # Return formatted ID with zero-padding\n",
    "        return f\"SAMPLE_{number:03d}\"\n",
    "    else:\n",
    "        # If no number found, return original\n",
    "        return sample_id\n",
    "\n",
    "# Apply standardization\n",
    "expr_cleaned['sample_id'] = expr_cleaned['sample_id'].apply(standardize_sample_id)\n",
    "metadata_df['sample_id'] = metadata_df['sample_id'].apply(standardize_sample_id)\n",
    "\n",
    "# Check matches now\n",
    "matching_ids = set(expr_cleaned['sample_id']) & set(metadata_df['sample_id'])\n",
    "print(f\"Matching IDs after standardization: {len(matching_ids)}\")\n",
    "print(\"\\nSample of standardized IDs:\")\n",
    "print(expr_cleaned['sample_id'].head(10).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Data Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.1: Inspect Label Inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine unique diagnosis values\n",
    "print(\"Unique diagnosis labels:\")\n",
    "print(metadata_df['diagnosis'].unique())\n",
    "print(\"\\nDiagnosis value counts:\")\n",
    "print(metadata_df['diagnosis'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.2: Map Labels to Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_diagnosis_to_binary(diagnosis):\n",
    "    \"\"\"\n",
    "    Map diagnosis string to binary classification.\n",
    "    \n",
    "    Returns:\n",
    "    - 0 for normal/healthy/control\n",
    "    - 1 for cancer/tumor/malignant\n",
    "    - None for ambiguous cases\n",
    "    \"\"\"\n",
    "    diagnosis_lower = diagnosis.lower()\n",
    "    \n",
    "    # Normal class\n",
    "    if diagnosis_lower in ['normal', 'healthy', 'control']:\n",
    "        return 0\n",
    "    \n",
    "    # Cancer class\n",
    "    elif diagnosis_lower in ['cancer', 'tumor', 'malignant']:\n",
    "        return 1\n",
    "    \n",
    "    # Ambiguous cases\n",
    "    elif diagnosis_lower in ['borderline', 'unclear', 'suspicious']:\n",
    "        return None\n",
    "    \n",
    "    else:\n",
    "        # Unknown label\n",
    "        print(f\"Warning: Unknown diagnosis '{diagnosis}'\")\n",
    "        return None\n",
    "\n",
    "# Apply mapping\n",
    "metadata_df['label'] = metadata_df['diagnosis'].apply(map_diagnosis_to_binary)\n",
    "\n",
    "print(\"Label distribution:\")\n",
    "print(metadata_df['label'].value_counts())\n",
    "print(f\"\\nAmbiguous cases: {metadata_df['label'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.3: Handle Ambiguous Labels\n",
    "\n",
    "We'll drop ambiguous cases for this binary classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show ambiguous cases before removal\n",
    "print(\"Ambiguous cases:\")\n",
    "print(metadata_df[metadata_df['label'].isnull()][['sample_id', 'diagnosis']])\n",
    "\n",
    "# Remove rows with ambiguous labels\n",
    "metadata_df = metadata_df[metadata_df['label'].notna()].copy()\n",
    "\n",
    "print(f\"\\nSamples after removing ambiguous cases: {len(metadata_df)}\")\n",
    "assert metadata_df['label'].isnull().sum() == 0, \"Still have ambiguous labels!\"\n",
    "print(\"Success! All ambiguous labels removed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.4: Merge Expression Data with Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge on sample_id (inner join)\n",
    "full_data = expr_cleaned.merge(metadata_df, on='sample_id', how='inner')\n",
    "\n",
    "print(f\"Final dataset shape: {full_data.shape}\")\n",
    "print(f\"\\nColumns: {full_data.columns.tolist()[:10]}...\")  # Show first 10 columns\n",
    "\n",
    "# Verify merge didn't introduce NaN values\n",
    "gene_nan_count = full_data[gene_columns].isnull().sum().sum()\n",
    "print(f\"\\nNaN check after merge:\")\n",
    "print(f\"  Gene expression NaN: {gene_nan_count}\")\n",
    "print(f\"  Label NaN: {full_data['label'].isnull().sum()}\")\n",
    "\n",
    "if gene_nan_count > 0:\n",
    "    print(f\"\\n⚠️  WARNING: Merge introduced {gene_nan_count} NaN values in gene data!\")\n",
    "    print(\"This shouldn't happen with an inner join. Investigating...\")\n",
    "    # Show which columns have NaN\n",
    "    nan_cols = full_data[gene_columns].isnull().sum()\n",
    "    print(f\"Columns with NaN: {nan_cols[nan_cols > 0]}\")\n",
    "else:\n",
    "    print(\"✓ No NaN values introduced by merge\")\n",
    "\n",
    "print(f\"\\nSample of merged data:\")\n",
    "display(full_data[['sample_id', 'GENE_001', 'GENE_002', 'diagnosis', 'label', 'age', 'gender']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.5: Visualize Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bar plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "class_counts = full_data['label'].value_counts().sort_index()\n",
    "plt.bar(class_counts.index, class_counts.values, color=['blue', 'red'], alpha=0.7)\n",
    "plt.xlabel('Class (0=Normal, 1=Cancer)', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.title('Class Distribution', fontsize=14)\n",
    "plt.xticks([0, 1])\n",
    "for i, v in enumerate(class_counts.values):\n",
    "    plt.text(i, v + 1, str(v), ha='center', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate class balance\n",
    "print(f\"\\nClass balance:\")\n",
    "print(f\"Normal (0): {class_counts[0]} ({100*class_counts[0]/len(full_data):.1f}%)\")\n",
    "print(f\"Cancer (1): {class_counts[1]} ({100*class_counts[1]/len(full_data):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.6: Create Train/Validation/Test Splits\n",
    "\n",
    "**Key Insight**: Use stratification to maintain class balance across splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features (X) and target (y)\n",
    "X = full_data[gene_columns].values\n",
    "y = full_data['label'].values.astype(int)  # Convert to int for proper bincount\n",
    "\n",
    "# Verify no NaN in features before splitting\n",
    "print(f\"NaN values in X: {np.isnan(X).sum()}\")\n",
    "if np.isnan(X).sum() > 0:\n",
    "    print(\"ERROR: Found NaN values in features! Check that Task 1.2 (imputation) completed successfully.\")\n",
    "    print(\"Attempting to fix by applying median imputation...\")\n",
    "    imputer_fix = SimpleImputer(strategy='median')\n",
    "    X = imputer_fix.fit_transform(X)\n",
    "    print(f\"NaN values after fix: {np.isnan(X).sum()}\")\n",
    "\n",
    "# First split: 60% train, 40% temp\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.4, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Second split: Split temp into 50/50 (which is 20/20 of total)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\nSplit sizes:\")\n",
    "print(f\"Training: {len(X_train)} samples ({100*len(X_train)/len(X):.1f}%)\")\n",
    "print(f\"Validation: {len(X_val)} samples ({100*len(X_val)/len(X):.1f}%)\")\n",
    "print(f\"Test: {len(X_test)} samples ({100*len(X_test)/len(X):.1f}%)\")\n",
    "\n",
    "# Verify stratification\n",
    "print(\"\\nClass distribution in each split:\")\n",
    "print(f\"Training: {np.bincount(y_train)} -> [{100*np.bincount(y_train)[0]/len(y_train):.1f}%, {100*np.bincount(y_train)[1]/len(y_train):.1f}%]\")\n",
    "print(f\"Validation: {np.bincount(y_val)} -> [{100*np.bincount(y_val)[0]/len(y_val):.1f}%, {100*np.bincount(y_val)[1]/len(y_val):.1f}%]\")\n",
    "print(f\"Test: {np.bincount(y_test)} -> [{100*np.bincount(y_test)[0]/len(y_test):.1f}%, {100*np.bincount(y_test)[1]/len(y_test):.1f}%]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Normalization and Its Impact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.1: Baseline Model (No Normalization)\n",
    "\n",
    "**Key Insight**: Features with larger scales dominate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train logistic regression without normalization\n",
    "baseline_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "baseline_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on validation set\n",
    "y_val_pred_baseline = baseline_model.predict(X_val)\n",
    "\n",
    "# Evaluate\n",
    "baseline_acc = accuracy_score(y_val, y_val_pred_baseline)\n",
    "print(\"=\"*60)\n",
    "print(\"BASELINE MODEL (No Normalization)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Validation Accuracy: {baseline_acc:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm_baseline = confusion_matrix(y_val, y_val_pred_baseline)\n",
    "print(cm_baseline)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_val_pred_baseline, target_names=['Normal', 'Cancer']))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance (model coefficients)\n",
    "coef_baseline = baseline_model.coef_[0]\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.bar(range(len(coef_baseline)), coef_baseline, alpha=0.7, color='red')\n",
    "plt.xlabel('Gene Index', fontsize=12)\n",
    "plt.ylabel('Coefficient', fontsize=12)\n",
    "plt.title('Baseline Model Coefficients (No Normalization)', fontsize=14)\n",
    "plt.axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Coefficient range: [{coef_baseline.min():.6f}, {coef_baseline.max():.6f}]\")\n",
    "print(f\"Coefficient std: {coef_baseline.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.2: Min-Max Scaling\n",
    "\n",
    "**Critical**: Fit scaler on training data ONLY!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit scaler on training data\n",
    "minmax_scaler = MinMaxScaler()\n",
    "X_train_minmax = minmax_scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform validation and test sets using the fitted scaler\n",
    "X_val_minmax = minmax_scaler.transform(X_val)\n",
    "X_test_minmax = minmax_scaler.transform(X_test)\n",
    "\n",
    "# Train model on scaled data\n",
    "model_minmax = LogisticRegression(random_state=42, max_iter=1000)\n",
    "model_minmax.fit(X_train_minmax, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_val_pred_minmax = model_minmax.predict(X_val_minmax)\n",
    "minmax_acc = accuracy_score(y_val, y_val_pred_minmax)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MIN-MAX SCALING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Validation Accuracy: {minmax_acc:.4f}\")\n",
    "print(f\"Improvement over baseline: {minmax_acc - baseline_acc:+.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_val_pred_minmax))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize coefficients after Min-Max scaling\n",
    "coef_minmax = model_minmax.coef_[0]\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.bar(range(len(coef_minmax)), coef_minmax, alpha=0.7, color='blue')\n",
    "plt.xlabel('Gene Index', fontsize=12)\n",
    "plt.ylabel('Coefficient', fontsize=12)\n",
    "plt.title('Model Coefficients After Min-Max Scaling', fontsize=14)\n",
    "plt.axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Coefficient range: [{coef_minmax.min():.4f}, {coef_minmax.max():.4f}]\")\n",
    "print(f\"Coefficient std: {coef_minmax.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.3: Standardization (Z-Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit scaler\n",
    "standard_scaler = StandardScaler()\n",
    "X_train_standard = standard_scaler.fit_transform(X_train)\n",
    "X_val_standard = standard_scaler.transform(X_val)\n",
    "X_test_standard = standard_scaler.transform(X_test)\n",
    "\n",
    "# Train model\n",
    "model_standard = LogisticRegression(random_state=42, max_iter=1000)\n",
    "model_standard.fit(X_train_standard, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_val_pred_standard = model_standard.predict(X_val_standard)\n",
    "standard_acc = accuracy_score(y_val, y_val_pred_standard)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STANDARDIZATION (Z-Score)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Validation Accuracy: {standard_acc:.4f}\")\n",
    "print(f\"Improvement over baseline: {standard_acc - baseline_acc:+.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_val_pred_standard))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize coefficients\n",
    "coef_standard = model_standard.coef_[0]\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.bar(range(len(coef_standard)), coef_standard, alpha=0.7, color='green')\n",
    "plt.xlabel('Gene Index', fontsize=12)\n",
    "plt.ylabel('Coefficient', fontsize=12)\n",
    "plt.title('Model Coefficients After Standardization', fontsize=14)\n",
    "plt.axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Coefficient range: [{coef_standard.min():.4f}, {coef_standard.max():.4f}]\")\n",
    "print(f\"Coefficient std: {coef_standard.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.4: Log Transformation + Standardization\n",
    "\n",
    "**Key Insight**: Gene expression data is often log-normally distributed.\n",
    "\n",
    "**Important Note**: This cell requires that missing values were properly imputed in Part 1. If you skipped that step, you'll get NaN errors here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log transformation (log1p handles zeros)\n",
    "# Note: Ensure data is clean before log transform\n",
    "\n",
    "# Safety check: Handle any remaining NaN values BEFORE log transform\n",
    "if np.isnan(X_train).any() or np.isnan(X_val).any() or np.isnan(X_test).any():\n",
    "    print(\"WARNING: NaN values detected before log transform!\")\n",
    "    print(f\"  Train NaN: {np.isnan(X_train).sum()}\")\n",
    "    print(f\"  Val NaN: {np.isnan(X_val).sum()}\")\n",
    "    print(f\"  Test NaN: {np.isnan(X_test).sum()}\")\n",
    "    print(\"Applying emergency imputation...\")\n",
    "    \n",
    "    # Emergency fix: impute NaN with median\n",
    "    emergency_imputer = SimpleImputer(strategy='median')\n",
    "    X_train = emergency_imputer.fit_transform(X_train)\n",
    "    X_val = emergency_imputer.transform(X_val)\n",
    "    X_test = emergency_imputer.transform(X_test)\n",
    "    \n",
    "    print(f\"After fix - Train NaN: {np.isnan(X_train).sum()}\")\n",
    "    print(f\"After fix - Val NaN: {np.isnan(X_val).sum()}\")\n",
    "    print(f\"After fix - Test NaN: {np.isnan(X_test).sum()}\")\n",
    "\n",
    "# Check for negative values (would cause NaN in log)\n",
    "neg_train = (X_train < 0).sum()\n",
    "neg_val = (X_val < 0).sum()\n",
    "neg_test = (X_test < 0).sum()\n",
    "\n",
    "if neg_train > 0 or neg_val > 0 or neg_test > 0:\n",
    "    print(f\"\\nWARNING: Negative values detected!\")\n",
    "    print(f\"  Train negatives: {neg_train}\")\n",
    "    print(f\"  Val negatives: {neg_val}\")\n",
    "    print(f\"  Test negatives: {neg_test}\")\n",
    "    print(\"Converting to absolute values for log transform...\")\n",
    "    X_train = np.abs(X_train)\n",
    "    X_val = np.abs(X_val)\n",
    "    X_test = np.abs(X_test)\n",
    "\n",
    "# Apply log transformation\n",
    "X_train_log = np.log1p(X_train)\n",
    "X_val_log = np.log1p(X_val)\n",
    "X_test_log = np.log1p(X_test)\n",
    "\n",
    "# CRITICAL: Replace any NaN values created by log transform\n",
    "# This handles edge cases where log1p might produce NaN\n",
    "X_train_log = np.nan_to_num(X_train_log, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "X_val_log = np.nan_to_num(X_val_log, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "X_test_log = np.nan_to_num(X_test_log, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "# Verify no NaN after log transform\n",
    "train_nan = np.isnan(X_train_log).sum()\n",
    "val_nan = np.isnan(X_val_log).sum()\n",
    "test_nan = np.isnan(X_test_log).sum()\n",
    "\n",
    "print(f\"\\nAfter log transform:\")\n",
    "print(f\"  Train NaN: {train_nan}, Inf: {np.isinf(X_train_log).sum()}\")\n",
    "print(f\"  Val NaN: {val_nan}, Inf: {np.isinf(X_val_log).sum()}\")\n",
    "print(f\"  Test NaN: {test_nan}, Inf: {np.isinf(X_test_log).sum()}\")\n",
    "\n",
    "if train_nan > 0 or val_nan > 0 or test_nan > 0:\n",
    "    print(\"\\nERROR: NaN still present after nan_to_num! This should not happen.\")\n",
    "    # Last resort: use imputer\n",
    "    final_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "    X_train_log = final_imputer.fit_transform(X_train_log)\n",
    "    X_val_log = final_imputer.transform(X_val_log)\n",
    "    X_test_log = final_imputer.transform(X_test_log)\n",
    "    print(f\"Applied final imputation. NaN count: {np.isnan(X_val_log).sum()}\")\n",
    "\n",
    "# Then standardize\n",
    "log_scaler = StandardScaler()\n",
    "X_train_log_std = log_scaler.fit_transform(X_train_log)\n",
    "X_val_log_std = log_scaler.transform(X_val_log)\n",
    "X_test_log_std = log_scaler.transform(X_test_log)\n",
    "\n",
    "print(f\"\\nAfter standardization:\")\n",
    "print(f\"  Train NaN: {np.isnan(X_train_log_std).sum()}\")\n",
    "print(f\"  Val NaN: {np.isnan(X_val_log_std).sum()}\")\n",
    "print(f\"  Test NaN: {np.isnan(X_test_log_std).sum()}\")\n",
    "\n",
    "# Train model\n",
    "model_log = LogisticRegression(random_state=42, max_iter=1000)\n",
    "model_log.fit(X_train_log_std, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_val_pred_log = model_log.predict(X_val_log_std)\n",
    "log_acc = accuracy_score(y_val, y_val_pred_log)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LOG TRANSFORMATION + STANDARDIZATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Validation Accuracy: {log_acc:.4f}\")\n",
    "print(f\"Improvement over baseline: {log_acc - baseline_acc:+.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_val_pred_log))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize coefficients\n",
    "coef_log = model_log.coef_[0]\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.bar(range(len(coef_log)), coef_log, alpha=0.7, color='purple')\n",
    "plt.xlabel('Gene Index', fontsize=12)\n",
    "plt.ylabel('Coefficient', fontsize=12)\n",
    "plt.title('Model Coefficients After Log Transformation + Standardization', fontsize=14)\n",
    "plt.axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Coefficient range: [{coef_log.min():.4f}, {coef_log.max():.4f}]\")\n",
    "print(f\"Coefficient std: {coef_log.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.5: Compare All Normalization Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "results_df = pd.DataFrame({\n",
    "    'Method': ['Baseline (No Norm)', 'Min-Max Scaling', 'Standardization', 'Log + Standardization'],\n",
    "    'Validation Accuracy': [baseline_acc, minmax_acc, standard_acc, log_acc],\n",
    "    'Improvement': [0, minmax_acc - baseline_acc, standard_acc - baseline_acc, log_acc - baseline_acc]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"NORMALIZATION COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Find best method\n",
    "best_idx = results_df['Validation Accuracy'].idxmax()\n",
    "best_method = results_df.loc[best_idx, 'Method']\n",
    "print(f\"\\nBest method: {best_method} ({results_df.loc[best_idx, 'Validation Accuracy']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['red', 'blue', 'green', 'purple']\n",
    "bars = plt.bar(results_df['Method'], results_df['Validation Accuracy'], color=colors, alpha=0.7)\n",
    "plt.ylabel('Validation Accuracy', fontsize=12)\n",
    "plt.title('Model Performance: Impact of Normalization Methods', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylim([0.5, 1.0])\n",
    "plt.axhline(y=baseline_acc, color='red', linestyle='--', alpha=0.5, label='Baseline')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars, results_df['Validation Accuracy']):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{acc:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Pipeline Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.1: Build a Proper Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline with StandardScaler and LogisticRegression\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression(random_state=42, max_iter=1000))\n",
    "])\n",
    "\n",
    "# Fit pipeline on training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_val_pred_pipeline = pipeline.predict(X_val)\n",
    "pipeline_acc = accuracy_score(y_val, y_val_pred_pipeline)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SKLEARN PIPELINE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Validation Accuracy: {pipeline_acc:.4f}\")\n",
    "print(f\"\\nPipeline steps: {list(pipeline.named_steps.keys())}\")\n",
    "print(\"\\nAdvantages of pipelines:\")\n",
    "print(\"  - Prevents data leakage automatically\")\n",
    "print(\"  - Cleaner, more maintainable code\")\n",
    "print(\"  - Easy to save and deploy\")\n",
    "print(\"  - Works seamlessly with cross-validation\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.2: Demonstrate Data Leakage Bug\n",
    "\n",
    "**Warning**: This shows INCORRECT data handling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"⚠️  WARNING: This demonstrates INCORRECT data handling! ⚠️\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# WRONG: Normalize ALL data before split (data leakage!)\n",
    "leaky_scaler = StandardScaler()\n",
    "X_all_normalized = leaky_scaler.fit_transform(X)  # Fit on ALL data - BAD!\n",
    "\n",
    "# Then split\n",
    "X_train_leaky, X_temp_leaky, y_train_leaky, y_temp_leaky = train_test_split(\n",
    "    X_all_normalized, y, test_size=0.4, stratify=y, random_state=42\n",
    ")\n",
    "X_val_leaky, X_test_leaky, y_val_leaky, y_test_leaky = train_test_split(\n",
    "    X_temp_leaky, y_temp_leaky, test_size=0.5, stratify=y_temp_leaky, random_state=42\n",
    ")\n",
    "\n",
    "# Train model\n",
    "leaky_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "leaky_model.fit(X_train_leaky, y_train_leaky)\n",
    "leaky_acc = accuracy_score(y_val_leaky, leaky_model.predict(X_val_leaky))\n",
    "\n",
    "print(\"Results comparison:\")\n",
    "print(f\"  Leaky approach (WRONG):  {leaky_acc:.4f}\")\n",
    "print(f\"  Proper approach (RIGHT): {standard_acc:.4f}\")\n",
    "print(f\"  Difference: {leaky_acc - standard_acc:+.4f}\")\n",
    "print(\"\\nWhy this is wrong:\")\n",
    "print(\"  - Test set statistics leaked into training\")\n",
    "print(\"  - Performance estimate is overly optimistic\")\n",
    "print(\"  - Model won't generalize to new data\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.3: Final Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use best model (based on validation performance)\n",
    "# Let's use the log transformation + standardization model\n",
    "\n",
    "y_test_pred = model_log.predict(X_test_log_std)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL TEST SET EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model: Log Transformation + Standardization\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Validation Accuracy: {log_acc:.4f}\")\n",
    "print(f\"Difference: {abs(test_acc - log_acc):.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm_test = confusion_matrix(y_test, y_test_pred)\n",
    "print(cm_test)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=['Normal', 'Cancer']))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Normal', 'Cancer'],\n",
    "            yticklabels=['Normal', 'Cancer'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.title('Confusion Matrix - Test Set', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary and Key Insights\n",
    "\n",
    "## What We Learned\n",
    "\n",
    "### 1. Data Cleaning is Critical\n",
    "- **Missing values** (7% of data) were handled with median imputation\n",
    "- **Duplicates** were identified and removed to avoid bias\n",
    "- **Inconsistent IDs** required standardization to merge datasets\n",
    "- **Outliers** were handled through normalization rather than removal\n",
    "\n",
    "### 2. Label Quality Matters\n",
    "- Started with 11 different label terms!\n",
    "- Mapped to binary classification (0=Normal, 1=Cancer)\n",
    "- Removed ambiguous cases (borderline, unclear, suspicious)\n",
    "- Used stratified splitting to maintain class balance\n",
    "\n",
    "### 3. Normalization Significantly Improves Performance\n",
    "- Baseline (no normalization): Struggled with different feature scales\n",
    "- Min-Max scaling: Improved by bringing features to [0,1]\n",
    "- Standardization: Even better by centering at mean=0, std=1\n",
    "- Log transformation: Best for log-normally distributed gene expression\n",
    "\n",
    "### 4. Pipelines Prevent Mistakes\n",
    "- Encapsulate preprocessing + model together\n",
    "- Automatically prevent data leakage\n",
    "- Make code cleaner and more reproducible\n",
    "- Essential for production deployments\n",
    "\n",
    "### 5. Data Leakage is a Serious Problem\n",
    "- Normalizing before splitting gives overoptimistic results\n",
    "- Always fit preprocessing on training data only\n",
    "- Use pipelines or be very careful with manual preprocessing\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Data quality often matters more than model choice**\n",
    "   - A simple model on clean data beats a complex model on messy data\n",
    "   \n",
    "2. **Preprocessing is not optional for real-world data**\n",
    "   - Real datasets always have quality issues\n",
    "   - Document your cleaning decisions\n",
    "   \n",
    "3. **Always validate on held-out data**\n",
    "   - Training accuracy is meaningless\n",
    "   - Test set gives realistic performance estimate\n",
    "   \n",
    "4. **Use appropriate metrics**\n",
    "   - Confusion matrix shows types of errors\n",
    "   - Precision/recall matter for imbalanced classes\n",
    "   \n",
    "5. **Reproducibility requires discipline**\n",
    "   - Set random seeds\n",
    "   - Document preprocessing steps\n",
    "   - Use version control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impact Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary\n",
    "summary_df = pd.DataFrame({\n",
    "    'Stage': [\n",
    "        'Raw messy data',\n",
    "        'After cleaning',\n",
    "        'No normalization',\n",
    "        'Min-Max scaling',\n",
    "        'Standardization',\n",
    "        'Log + Standardization'\n",
    "    ],\n",
    "    'Data Quality': [\n",
    "        'Missing, duplicates, inconsistent',\n",
    "        'Clean, no missing/duplicates',\n",
    "        'Clean',\n",
    "        'Clean + normalized [0,1]',\n",
    "        'Clean + normalized (μ=0, σ=1)',\n",
    "        'Clean + log-normalized'\n",
    "    ],\n",
    "    'Val Accuracy': [\n",
    "        'N/A',\n",
    "        'N/A',\n",
    "        f'{baseline_acc:.4f}',\n",
    "        f'{minmax_acc:.4f}',\n",
    "        f'{standard_acc:.4f}',\n",
    "        f'{log_acc:.4f}'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPLETE PIPELINE IMPACT SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal improvement from baseline: {log_acc - baseline_acc:+.4f}\")\n",
    "print(f\"Relative improvement: {100*(log_acc - baseline_acc)/baseline_acc:+.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension Ideas\n",
    "\n",
    "Try these challenges to deepen your understanding:\n",
    "\n",
    "1. **Different ML Models**\n",
    "   ```python\n",
    "   from sklearn.ensemble import RandomForestClassifier\n",
    "   from sklearn.svm import SVC\n",
    "   # Compare performance across models\n",
    "   ```\n",
    "\n",
    "2. **Cross-Validation**\n",
    "   ```python\n",
    "   from sklearn.model_selection import cross_val_score\n",
    "   scores = cross_val_score(pipeline, X_train, y_train, cv=5)\n",
    "   ```\n",
    "\n",
    "3. **Feature Selection**\n",
    "   ```python\n",
    "   # Find top 10 most important genes\n",
    "   coef_importance = np.abs(model_log.coef_[0])\n",
    "   top_genes_idx = np.argsort(coef_importance)[-10:]\n",
    "   ```\n",
    "\n",
    "4. **Dimensionality Reduction**\n",
    "   ```python\n",
    "   from sklearn.decomposition import PCA\n",
    "   pca = PCA(n_components=10)\n",
    "   X_train_pca = pca.fit_transform(X_train_standard)\n",
    "   ```\n",
    "\n",
    "5. **Handle Class Imbalance**\n",
    "   ```python\n",
    "   from imblearn.over_sampling import SMOTE\n",
    "   smote = SMOTE(random_state=42)\n",
    "   X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "   ```\n",
    "\n",
    "6. **Error Analysis**\n",
    "   ```python\n",
    "   # Find misclassified samples\n",
    "   misclassified = X_test[y_test != y_test_pred]\n",
    "   # Analyze what makes them difficult\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Congratulations!\n",
    "\n",
    "You've completed a comprehensive machine learning preprocessing pipeline!\n",
    "\n",
    "### Skills Mastered:\n",
    "- Data cleaning and quality assessment\n",
    "- Label standardization and handling ambiguity\n",
    "- Multiple normalization techniques\n",
    "- Proper train/validation/test splitting\n",
    "- Avoiding data leakage\n",
    "- Building sklearn pipelines\n",
    "- Model evaluation and comparison\n",
    "\n",
    "### Next Steps:\n",
    "1. Apply these techniques to your own datasets\n",
    "2. Experiment with different models and parameters\n",
    "3. Learn about feature engineering\n",
    "4. Explore automated ML (AutoML) tools\n",
    "5. Study deep learning for complex patterns\n",
    "\n",
    "Remember: **Good data beats fancy algorithms!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
